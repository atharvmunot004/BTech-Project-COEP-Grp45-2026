{
    "component": "Data Layer",
    "purpose": "Persistent raw data storage and serving; canonical source of truth for OHLCV and metadata",
    "implementation_steps": [
        {
            "step": 1,
            "title": "Setup Project Structure",
            "description": "Create the basic folder structure for the Data Layer",
            "tasks": [
                "Create `dataset/` folder if it doesn't exist",
                "Create `src/implementation_01/data/` folder for data layer modules",
                "Create `src/implementation_01/data/__init__.py` to make it a package",
                "Set up logging configuration for data layer operations"
            ],
            "files_to_create": [
                "src/implementation_01/data/__init__.py",
                "src/implementation_01/data/logger_config.py"
            ],
            "dependencies": [
                "os",
                "logging",
                "pathlib"
            ]
        },
        {
            "step": 2,
            "title": "Create Data Schema and Validation",
            "description": "Define the expected data schema and validation functions for OHLCV records",
            "tasks": [
                "Define expected column names: ['Date', 'Symbol', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adjusted Close', 'corporate_action_flags']",
                "Create schema validation function to check data types (Date as datetime, prices as float64, Volume as int64)",
                "Validate ISO 8601 date format (YYYY-MM-DD)",
                "Check for required columns presence",
                "Validate price relationships (Low <= Open/Close/High <= High)",
                "Handle optional columns gracefully (Adjusted Close, corporate_action_flags)"
            ],
            "files_to_create": [
                "src/implementation_01/data/schema.py"
            ],
            "code_example": {
                "schema_definition": {
                    "required_columns": ["Date", "Symbol", "Open", "High", "Low", "Close", "Volume"],
                    "optional_columns": ["Adjusted Close", "corporate_action_flags"],
                    "dtypes": {
                        "Date": "datetime64[ns]",
                        "Symbol": "object",
                        "Open": "float64",
                        "High": "float64",
                        "Low": "float64",
                        "Close": "float64",
                        "Volume": "int64",
                        "Adjusted Close": "float64",
                        "corporate_action_flags": "object"
                    }
                }
            },
            "dependencies": [
                "pandas",
                "numpy",
                "datetime"
            ]
        },
        {
            "step": 3,
            "title": "Create CSV Reader Module",
            "description": "Implement functions to read and parse CSV files from the dataset folder",
            "tasks": [
                "Create function `read_csv_file(filepath)` that reads CSV with proper date parsing",
                "Handle different CSV formats (date as index vs date column)",
                "Set Date as DataFrame index",
                "Extract Symbol from filename or data if present",
                "Normalize column names (handle case variations, spaces, underscores)",
                "Apply schema validation after reading",
                "Handle encoding issues (UTF-8, fallback to latin-1)",
                "Add metadata (file_path, load_timestamp, file_size)"
            ],
            "files_to_create": [
                "src/implementation_01/data/csv_reader.py"
            ],
            "code_structure": {
                "functions": [
                    "read_csv_file(filepath: str) -> pd.DataFrame",
                    "normalize_column_names(df: pd.DataFrame) -> pd.DataFrame",
                    "extract_symbol_from_filename(filepath: str) -> str",
                    "validate_ohlcv_data(df: pd.DataFrame) -> bool"
                ]
            },
            "dependencies": [
                "pandas",
                "os",
                "pathlib",
                "datetime"
            ]
        },
        {
            "step": 4,
            "title": "Create Data Loader Class",
            "description": "Implement the main DataLoader class that serves as the interface to the Data Layer",
            "tasks": [
                "Create `DataLoader` class with dataset folder path as parameter",
                "Implement `list_available_symbols()` method to scan dataset folder and return available symbols",
                "Implement `load_symbol(symbol: str, start_date: str = None, end_date: str = None) -> pd.DataFrame`",
                "Implement `load_multiple_symbols(symbols: list, start_date: str = None, end_date: str = None) -> dict`",
                "Implement date filtering using ISO 8601 format",
                "Cache loaded data in memory (optional, with TTL)",
                "Handle file not found errors gracefully",
                "Return data in standardized format (Date as index, OHLCV columns)"
            ],
            "files_to_create": [
                "src/implementation_01/data/data_loader.py"
            ],
            "code_structure": {
                "class": "DataLoader",
                "methods": [
                    "__init__(dataset_folder: str)",
                    "list_available_symbols() -> list[str]",
                    "load_symbol(symbol: str, start_date: str = None, end_date: str = None) -> pd.DataFrame",
                    "load_multiple_symbols(symbols: list, start_date: str = None, end_date: str = None) -> dict[str, pd.DataFrame]",
                    "_find_symbol_file(symbol: str) -> str",
                    "_normalize_date_range(df: pd.DataFrame, start_date: str, end_date: str) -> pd.DataFrame"
                ]
            },
            "dependencies": [
                "pandas",
                "os",
                "glob",
                "pathlib",
                "functools (for caching)"
            ]
        },
        {
            "step": 5,
            "title": "Create File Path Resolver",
            "description": "Implement utility to map symbol names to CSV file paths",
            "tasks": [
                "Create function to match symbol to filename patterns (e.g., 'ASIANPAINT' -> 'ASIANPAINT_10yr_daily.csv')",
                "Handle case-insensitive matching",
                "Support multiple naming conventions",
                "Return absolute file path",
                "Handle ambiguous matches (multiple files for same symbol)"
            ],
            "files_to_create": [
                "src/implementation_01/data/path_resolver.py"
            ],
            "code_structure": {
                "functions": [
                    "resolve_symbol_to_filepath(symbol: str, dataset_folder: str) -> str",
                    "_find_files_matching_symbol(symbol: str, dataset_folder: str) -> list[str]",
                    "_normalize_symbol_name(symbol: str) -> str"
                ]
            },
            "dependencies": [
                "os",
                "glob",
                "pathlib",
                "re"
            ]
        },
        {
            "step": 6,
            "title": "Create Data Exporter Module",
            "description": "Implement functions to export data in various formats (Parquet, CSV, in-memory DataFrame)",
            "tasks": [
                "Create `export_to_parquet(df: pd.DataFrame, output_path: str)` function",
                "Create `export_to_csv(df: pd.DataFrame, output_path: str)` function",
                "Create `to_dataframe_format(df: pd.DataFrame) -> pd.DataFrame` for standardized output",
                "Handle multi-index DataFrames (date x symbol)",
                "Preserve metadata in output files",
                "Support compression for Parquet (snappy, gzip)"
            ],
            "files_to_create": [
                "src/implementation_01/data/data_exporter.py"
            ],
            "code_structure": {
                "functions": [
                    "export_to_parquet(df: pd.DataFrame, output_path: str, compression: str = 'snappy')",
                    "export_to_csv(df: pd.DataFrame, output_path: str, index: bool = True)",
                    "to_dataframe_format(df: pd.DataFrame, pivot: bool = False) -> pd.DataFrame",
                    "_prepare_multi_symbol_df(symbols_dict: dict) -> pd.DataFrame"
                ]
            },
            "dependencies": [
                "pandas",
                "pyarrow (for Parquet)",
                "os"
            ]
        },
        {
            "step": 7,
            "title": "Create Metadata Handler",
            "description": "Track and manage metadata about datasets (version, source, checksums, last modified)",
            "tasks": [
                "Create function to compute file checksums (MD5 or SHA256)",
                "Track file metadata (size, modification time, row count)",
                "Create `get_dataset_info(symbol: str) -> dict` function",
                "Store metadata cache (JSON file) for faster access",
                "Validate data integrity using checksums",
                "Track data source information (exchange, provider, download date)"
            ],
            "files_to_create": [
                "src/implementation_01/data/metadata_handler.py"
            ],
            "code_structure": {
                "functions": [
                    "compute_file_checksum(filepath: str) -> str",
                    "get_dataset_info(symbol: str, dataset_folder: str) -> dict",
                    "update_metadata_cache(dataset_folder: str)",
                    "get_data_range(symbol: str) -> tuple[str, str]",
                    "get_row_count(symbol: str) -> int"
                ],
                "metadata_schema": {
                    "symbol": "str",
                    "filepath": "str",
                    "checksum": "str",
                    "file_size_bytes": "int",
                    "last_modified": "str (ISO 8601)",
                    "date_range": {"start": "str", "end": "str"},
                    "row_count": "int",
                    "source": "str",
                    "download_date": "str"
                }
            },
            "dependencies": [
                "hashlib",
                "json",
                "os",
                "pathlib",
                "pandas"
            ]
        },
        {
            "step": 8,
            "title": "Create Data Validation and Quality Checks",
            "description": "Implement comprehensive data quality validation",
            "tasks": [
                "Check for missing values in critical columns",
                "Detect outliers (prices/volumes outside reasonable ranges)",
                "Validate time series continuity (no unexpected gaps in dates)",
                "Check for duplicate dates",
                "Validate trading calendar (exclude weekends/holidays if needed)",
                "Generate data quality report",
                "Handle corporate actions flags if present",
                "Validate adjusted vs unadjusted prices"
            ],
            "files_to_create": [
                "src/implementation_01/data/quality_checks.py"
            ],
            "code_structure": {
                "functions": [
                    "validate_data_quality(df: pd.DataFrame) -> dict",
                    "check_missing_values(df: pd.DataFrame) -> dict",
                    "detect_outliers(df: pd.DataFrame, method: str = 'iqr') -> pd.DataFrame",
                    "check_date_continuity(df: pd.DataFrame) -> dict",
                    "detect_duplicate_dates(df: pd.DataFrame) -> list",
                    "generate_quality_report(symbol: str, df: pd.DataFrame) -> dict"
                ]
            },
            "dependencies": [
                "pandas",
                "numpy",
                "datetime"
            ]
        },
        {
            "step": 9,
            "title": "Create API/Interface Module",
            "description": "Create a unified API interface for accessing the Data Layer",
            "tasks": [
                "Create main `DataLayer` class that encapsulates all functionality",
                "Implement REST-like methods: `get(symbol)`, `get_multiple(symbols)`, `list()`, `info(symbol)`",
                "Add configuration management (dataset folder path, caching settings)",
                "Implement error handling and custom exceptions",
                "Add logging for all operations",
                "Create factory function for easy instantiation"
            ],
            "files_to_create": [
                "src/implementation_01/data/data_layer.py"
            ],
            "code_structure": {
                "class": "DataLayer",
                "methods": [
                    "__init__(dataset_folder: str, enable_cache: bool = True)",
                    "get(symbol: str, start_date: str = None, end_date: str = None) -> pd.DataFrame",
                    "get_multiple(symbols: list, start_date: str = None, end_date: str = None) -> dict",
                    "list_symbols() -> list[str]",
                    "info(symbol: str) -> dict",
                    "validate(symbol: str) -> dict",
                    "export(symbol: str, format: str = 'dataframe', output_path: str = None)"
                ],
                "custom_exceptions": [
                    "SymbolNotFoundError",
                    "InvalidDataFormatError",
                    "DataLoadError"
                ]
            },
            "dependencies": [
                "All previous modules",
                "logging"
            ]
        },
        {
            "step": 10,
            "title": "Create Unit Tests",
            "description": "Write comprehensive unit tests for all Data Layer components",
            "tasks": [
                "Create test fixtures with sample OHLCV data",
                "Test CSV reader with various formats",
                "Test DataLoader with single and multiple symbols",
                "Test date filtering functionality",
                "Test error handling (file not found, invalid data)",
                "Test schema validation",
                "Test metadata handler",
                "Test data quality checks",
                "Test export functions",
                "Use pytest framework"
            ],
            "files_to_create": [
                "tests/test_data_layer.py",
                "tests/fixtures/sample_data.csv",
                "tests/test_csv_reader.py",
                "tests/test_data_loader.py",
                "tests/test_quality_checks.py"
            ],
            "dependencies": [
                "pytest",
                "pytest-cov (for coverage)",
                "pandas",
                "numpy"
            ]
        },
        {
            "step": 11,
            "title": "Create Configuration File",
            "description": "Create configuration management for Data Layer settings",
            "tasks": [
                "Create `config.py` or `config.yaml` for Data Layer settings",
                "Define default dataset folder path",
                "Configure caching settings (TTL, max size)",
                "Set default date format (ISO 8601)",
                "Configure logging levels",
                "Set file naming conventions",
                "Allow environment variable overrides"
            ],
            "files_to_create": [
                "src/implementation_01/data/config.py",
                "src/implementation_01/data/config.yaml (optional)"
            ],
            "dependencies": [
                "os",
                "yaml (optional)",
                "pathlib"
            ]
        },
        {
            "step": 12,
            "title": "Create Documentation and Examples",
            "description": "Write documentation and usage examples",
            "tasks": [
                "Create README.md for Data Layer with usage examples",
                "Add docstrings to all public functions and classes",
                "Create example notebook showing Data Layer usage",
                "Document data format requirements",
                "Add troubleshooting guide",
                "Include performance benchmarks"
            ],
            "files_to_create": [
                "src/implementation_01/data/README.md",
                "docs/data_layer_examples.ipynb"
            ],
            "dependencies": []
        },
        {
            "step": 13,
            "title": "Integration with Data Preprocessing Engine",
            "description": "Ensure Data Layer output format matches Data Preprocessing Engine input requirements",
            "tasks": [
                "Verify output format: pandas DataFrame with Date index and symbol columns",
                "Test data handoff to Data Preprocessing Engine",
                "Implement format conversion if needed (pivot from long to wide format)",
                "Ensure compatibility with expected input format: 'index=date, columns=symbols, values=close (and OHLCV if needed)'",
                "Add helper function to convert to preprocessing engine format"
            ],
            "files_to_create": [
                "src/implementation_01/data/preprocessing_adapter.py"
            ],
            "code_structure": {
                "functions": [
                    "to_preprocessing_format(symbols_dict: dict, price_type: str = 'Close') -> pd.DataFrame",
                    "_pivot_ohlcv_data(df: pd.DataFrame, price_column: str = 'Close') -> pd.DataFrame"
                ]
            },
            "dependencies": [
                "pandas"
            ]
        }
    ],
    "output_format_specification": {
        "format": "pandas DataFrame",
        "index": "Date (datetime64[ns], ISO 8601 format)",
        "columns": [
            "For single symbol: ['Open', 'High', 'Low', 'Close', 'Volume', 'Adjusted Close' (optional)]",
            "For multiple symbols (pivoted): MultiIndex or columns named like 'SYMBOL_Close', 'SYMBOL_Volume', etc."
        ],
        "access_methods": [
            "Direct DataFrame return",
            "File export (Parquet/CSV)",
            "In-memory cache"
        ]
    },
    "dependencies_required": [
        "pandas >= 1.5.0",
        "numpy >= 1.23.0",
        "pyarrow >= 10.0.0 (for Parquet support)",
        "python-dateutil >= 2.8.0",
        "pytest >= 7.0.0 (for testing)"
    ],
    "expected_file_structure": {
        "src/implementation_01/data/": [
            "__init__.py",
            "config.py",
            "schema.py",
            "csv_reader.py",
            "data_loader.py",
            "path_resolver.py",
            "data_exporter.py",
            "metadata_handler.py",
            "quality_checks.py",
            "data_layer.py",
            "preprocessing_adapter.py",
            "logger_config.py",
            "README.md"
        ],
        "dataset/": [
            "*.csv (symbol data files)"
        ],
        "tests/": [
            "test_data_layer.py",
            "test_csv_reader.py",
            "test_data_loader.py",
            "fixtures/sample_data.csv"
        ]
    },
    "testing_strategy": {
        "unit_tests": "Test each module independently",
        "integration_tests": "Test full data loading pipeline",
        "performance_tests": "Measure load times for 10 symbols x 10 years",
        "data_quality_tests": "Validate against known good datasets"
    },
    "notes": [
        "All dates must be in ISO 8601 format (YYYY-MM-DD)",
        "Data should be adjusted for corporate actions",
        "Handle missing data gracefully (flag and document)",
        "Support both single symbol and multi-symbol queries",
        "Maintain backward compatibility with existing CSV formats",
        "Implement caching to improve performance for repeated queries",
        "Log all data access operations for debugging and audit",
        "Ensure thread-safety if used in concurrent environments"
    ]
}
