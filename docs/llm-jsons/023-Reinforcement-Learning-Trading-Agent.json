{
  "tool_name": "rl_trading_agent",
  "display_name": "Reinforcement Learning (RL) Trading Agent",
  "category": "strategy_generation",
  "subcategory": "reinforcement_learning",
  "description": "Sequential decision-making agent that learns trading or allocation policies by maximizing cumulative reward (e.g., risk-adjusted PnL) through interaction with a market environment. Provides adaptive strategy module complementing classical methods and enabling comparison with quantum RL counterparts.",
  "use_cases": [
    "Autonomous portfolio rebalancing",
    "Intraday signal generation",
    "Execution policy optimization (timing, sizing, slippage-aware)",
    "Research sandbox for new factor mixes or markets"
  ],
  "inputs": {
    "observations": {
      "type": "dict",
      "description": "Features forming environment state: price history, indicators, regime probabilities, portfolio state, risk metrics",
      "source": "Feature engineering layer, portfolio service, risk module"
    },
    "action_space": {
      "type": "list | continuous",
      "description": "Allowed trading actions (target weights, discrete buy/sell/hold, execution decisions)",
      "configuration": "Defined per strategy policy"
    },
    "reward_function": {
      "type": "callable",
      "description": "Maps state/action outcome to scalar reward (PnL, Sharpe, CVaR-penalized return)",
      "source": "Strategy configuration"
    },
    "episode_config": {
      "type": "dict",
      "description": "Trading horizon, reset rules, transaction cost model",
      "required": true
    }
  },
  "outputs": {
    "policy_model": {
      "type": "artifact",
      "description": "Trained RL policy (neural network weights)"
    },
    "signals": {
      "type": "list",
      "description": "Actions generated by policy when deployed (target weights, trade instructions)"
    },
    "metrics": {
      "type": "dict",
      "description": "Training/backtest metrics (return, Sharpe, drawdown, CVaR, constraint violations)"
    }
  },
  "algorithm_options": [
    "Value-based (DQN, Rainbow)",
    "Policy gradient / Actor-Critic (PPO, A2C, SAC)",
    "Model-based RL",
    "Offline RL / Imitation Learning"
  ],
  "pipeline_steps": [
    "Environment design (state, action, reward, constraints)",
    "Data handling (train/val/test, scenario augmentation)",
    "Training loop with chosen RL algorithm",
    "Evaluation & walk-forward backtesting",
    "Deployment with guardrails and monitoring"
  ],
  "pseudocode": {
    "language": "python",
    "snippet": "model = PPO('MlpPolicy', env, n_steps=2048, batch_size=256, learning_rate=3e-4); model.learn(total_timesteps=5_000_000)"
  },
  "dependencies": {
    "required": [
      "stable-baselines3 or RLlib",
      "gymnasium",
      "numpy",
      "pandas",
      "torch"
    ],
    "optional": [
      "mlflow",
      "prefect",
      "optuna"
    ]
  },
  "advantages": [
    "Learns adaptive policies from data",
    "Supports complex objectives (risk-aware rewards)",
    "Integrates with regime detectors and quantum modules",
    "Provides benchmark against quantum RL"
  ],
  "limitations": [
    "Sample inefficiency and training instability",
    "Risk of overfitting to historical regimes",
    "Requires robust evaluation and guardrails"
  ],
  "references": [
    {
      "title": "Reinforcement Learning for Trading",
      "authors": "Moody & Saffell",
      "year": 2001,
      "source": "Neural Networks",
      "url": "https://doi.org/10.1016/S0893-6080(01)00040-1"
    },
    {
      "title": "Deep Reinforcement Learning in Portfolio Management",
      "authors": "Li et al.",
      "year": 2019,
      "source": "AAAI",
      "url": "https://aaai.org/ojs/index.php/AAAI/article/view/3809"
    }
  ],
  "logging_metadata": {
    "inputs_to_log": [
      "env_config",
      "reward_fn",
      "train_period",
      "random_seed"
    ],
    "outputs_to_log": [
      "policy_checkpoint",
      "training_metrics",
      "backtest_results"
    ],
    "monitoring": [
      "online_return",
      "drawdown",
      "risk_limit_flags"
    ]
  },
  "integration_points": {
    "feature_layer": "Provides state features",
    "risk_module": "Supplies risk metrics and guardrails",
    "execution_service": "Consumes RL actions",
    "quantum_stack": "Enable hybrid classical-quantum RL experiments"
  }
}

