{
  "total_papers": 105,
  "papers": [
    {
      "id": 1,
      "title": "Self-Adaptive Quantum Kernel Principal Component Analysis",
      "paper-link": "\\docs\\reference-literature\\Advanced Science - 2025 - Wang - Self-Adaptive Quantum Kernel Principal Component Analysis for Compact Readout of.pdf",
      "paper-explaination-link": "\\docs\\reference-literature\\Advanced Science - 2025 - Wang - Self-Adaptive Quantum Kernel Principal Component Analysis for Compact Readout of.json",
      "authors": "Wang et al.",
      "year": 2025,
      "source": "Advanced Science",
      "url": "https://advanced.onlinelibrary.wiley.com/doi/10.1002/advs.202411573",
      "performance-metrics": [
        "Accuracy",
        "F1 Score",
        "Cohen's Kappa (CK Score)",
        "Collapse Rate (score reduction rate)",
        "Mean evaluation score (averaged over 10 runs)",
        "Dimensionality-level performance (7D, 6D, 5D, 4D, 3D, 2D)",
        "Kernel similarity patterns",
        "Kernel redundancy",
        "Cluster separability (t-SNE)",
        "Model-category performance (linear, nonlinear, ensemble)"
      ],
      "tools-used-in": [
        "Quantum-PCA-(qPCA)-for-Factor-Risk-Analysis"
      ],
      "function": ["risk assessment", "market regime detection"],
      "paper-details": {
        "algorithms": {
          "SAQK_PCA": {
            "name": "Self-Adaptive Quantum Kernel PCA",
            "purpose": "Enhance dimensionality reduction by mapping classical data into a quantum covariant space using adaptive variational circuits.",
            "components": {
              "feature_map": {
                "type": "Pauli-Z feature map",
                "operation": "Encodes each classical feature x_i into rotation Rz(x_i) on each qubit.",
                "effect": "Maps classical input into a quantum state that preserves structured relationships."
              },
              "variational_layer": {
                "type": "Trainable Rx rotations",
                "operation": "Applies Uθ_V(x) = Π Rx(x_i * θ_i) to adapt feature mapping.",
                "goal": "Align encoded data with group structures and optimize kernel expressivity."
              },
              "kernel_estimation": {
                "method": "Quantum fidelity estimation",
                "definition": "K(x_i, x_j) = Fidelity(ρ_i, ρ_j)",
                "effect": "Measures similarity between quantum-encoded data.",
                "implementation": "Qiskit sampling circuits estimate state fidelity."
              }
            },
            "procedure": [
              "Preprocess classical input vector x (standardize).",
              "Encode x using Pauli-Z map: apply Rz(x_i) on qubits.",
              "Apply adaptive Rx layer with trainable parameters.",
              "Generate quantum states ρ_i and ρ_j for all data pairs.",
              "Compute pairwise fidelity to form quantum kernel matrix K.",
              "Apply PCA on quantum kernel matrix (same math as kernel PCA)."
            ],
            "output": "Low-dimensional quantum-enhanced feature representation.",
            "advantages": [
              "Captures nonlinear & group-structured patterns.",
              "Produces low-redundancy kernel matrices.",
              "Better information retention in low dimensions."
            ]
          },
      
          "Quantum_PCA": {
            "name": "Quantum Kernel PCA (qPCA)",
            "purpose": "Dimensionality reduction in quantum-defined feature space.",
            "kernel_function": {
              "definition": "Φ(x) = K(·, x)",
              "similarity": "Fidelity between quantum states."
            },
            "procedure": [
              "Obtain quantum kernel matrix using fidelity: K_ij = F(ρ_i, ρ_j).",
              "Center and normalize the kernel matrix.",
              "Compute covariance matrix in kernel space.",
              "Solve eigenvalue problem C·v = λ·v.",
              "Select top k eigenvectors.",
              "Project data into reduced quantum feature space."
            ],
            "output": "Quantum principal components."
          },
      
          "Classical_Kernel_PCA": {
            "name": "Classical RBF Kernel PCA",
            "purpose": "Dimensionality reduction using Gaussian RBF kernel.",
            "kernel_function": {
              "definition": "K(x_i, x_j) = exp(-||x_i - x_j||² / 2σ²)",
              "properties": [
                "Captures nonlinear similarity.",
                "Introduces redundancy when data is noisy or correlated."
              ]
            },
            "procedure": [
              "Compute RBF kernel matrix for all samples.",
              "Center the kernel matrix.",
              "Compute covariance matrix using kernel trick.",
              "Solve eigenvalue decomposition C·v = λ·v.",
              "Select eigenvectors with largest eigenvalues.",
              "Project data to reduced k-dimensional space."
            ],
            "limitations": [
              "Preserves linear-like patterns well.",
              "Loses finer group-structured or high-order nonlinear patterns."
            ]
          },
      
          "TSNE": {
            "name": "t-SNE (Visualization Only)",
            "purpose": "Visualize high-dimensional PCA output in 2D.",
            "procedure": [
              "Model high-dimensional similarities using Gaussian distribution.",
              "Model low-dimensional similarities using Student-t distribution.",
              "Minimize KL-divergence between high- and low-dimensional similarities.",
              "Produce 2D embeddings revealing cluster structure."
            ],
            "role_in_paper": "Used to compare separability of cPCA vs SAQK qPCA."
          },
      
          "ML_Models": {
            "purpose": "Evaluate quality of compressed data.",
            "categories": {
              "linear_models": {
                "LR": "Logistic regression uses sigmoid-based decision boundaries.",
                "L-SVM": "Support vector machine with linear kernel."
              },
              "nonlinear_models": {
                "RBF-SVM": "SVM with Gaussian kernel to model nonlinear boundaries.",
                "KNN": "Classifies based on nearest neighbors in reduced space.",
                "NB": "Probabilistic classifier assuming feature independence.",
                "MLP": "Feedforward neural network with nonlinear activations."
              },
              "ensemble_learning": {
                "RF": "Random forest builds many decision trees using bagging.",
                "ET": "Extremely randomized trees with random splits.",
                "GBC": "Gradient boosting decision trees.",
                "XGB": "Optimized gradient boosting algorithm."
              }
            },
            "role_in_paper": "Used purely for benchmarking information retention."
          },
      
          "Low_Entropy_Synthetic_Data": {
            "name": "Artificial Linear & Nonlinear Data Generation",
            "purpose": "Validate algorithms on controlled low-entropy scenarios.",
            "linear_data": {
              "formula": "y = w·X + ε",
              "generation_steps": [
                "Sample features X from Gaussian distribution.",
                "Sample weights w from Gaussian distribution.",
                "Add noise ε sampled from N(0, σ²).",
                "Label samples using threshold (y > 0 → class 1, else class 0)."
              ]
            },
            "nonlinear_data": {
              "formula": "y = sin(w·X) + cos(w·X) + exp(-w·X) + ln(|X| + 1) + ε",
              "purpose": "Inject strong nonlinear structure.",
              "properties": [
                "Low entropy with structured relationships.",
                "Designed to test how kernels preserve complex shapes."
              ]
            },
            "standardization": "All features normalized using StandardScaler.",
            "role_in_paper": "SAQK PCA performed better than cPCA especially in nonlinear cases."
          }
        }
      }
    },
    
    {
      "id": 2,
      "title": "Efficient CVaR estimation using importance-sampling Monte Carlo",
      "authors": "Jiang et al.",
      "year": 2025,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/2502.04562",
      "performance-metrics": [
        "CVaR Estimation Error",
        "Variance Reduction Factor",
        "Sample Complexity Reduction",
        "Bias of CVaR Estimator",
        "Convergence Rate of Estimator",
        "Importance Sampling Efficiency (ESS)",
        "Tail Probability Accuracy",
        "Runtime vs Accuracy Tradeoff",
        "Relative Error in Tail Risk",
        "Confidence Interval Width"
      ],
      "tools-used-in": [
        "Monte-Carlo-Simulation-for-VAR-CVAR"
      ],
      "paper-details": {
        "algorithms": {
          "IS_MonteCarlo_for_CVaR": {
            "name": "Importance Sampling Monte Carlo for CVaR Estimation",
            "purpose": "Reduce the computational cost of estimating Conditional Value-at-Risk (CVaR) by drawing more samples from high-loss (tail) regions where CVaR is concentrated.",
            "components": {
              "original_distribution": {
                "definition": "p(x) — the natural loss distribution.",
                "role": "Represents true underlying risk behavior.",
                "limitation": "Naive Monte Carlo rarely samples extreme tail losses."
              },
              "importance_distribution": {
                "definition": "q(x) — alternative distribution emphasizing tail outcomes.",
                "role": "Increases the probability of sampling high-loss events.",
                "effect": "Substantially reduces variance of CVaR estimate."
              },
              "likelihood_ratio": {
                "definition": "w(x) = p(x) / q(x)",
                "role": "Reweights biased samples to maintain unbiased expectation.",
                "importance": "Ensures estimator correctness despite biased sampling."
              }
            },
            "procedure": [
              "Define loss variable L(x) and target CVaR confidence level α.",
              "Construct or learn an importance-sampling distribution q(x).",
              "Sample L₁, L₂, …, Lₙ from q(x) instead of p(x).",
              "Compute likelihood ratios wᵢ = p(Lᵢ) / q(Lᵢ).",
              "Sort losses and identify VaRα threshold using weighted samples.",
              "Compute CVaRα as weighted tail expectation using wᵢ.",
              "Estimate confidence intervals and evaluate sample efficiency."
            ],
            "output": "A low-variance, sample-efficient estimator of CVaRα.",
            "advantages": [
              "Significantly fewer samples needed to estimate CVaR accurately.",
              "Reduces estimator variance by focusing on tail events.",
              "Applicable to heavy-tailed and skewed distributions.",
              "Provides unbiased CVaR estimates with lower computational cost."
            ]
          },
    
          "Naive_MonteCarlo": {
            "name": "Standard Monte Carlo CVaR Estimation",
            "purpose": "Serve as the baseline by estimating CVaR from uniformly sampled losses.",
            "procedure": [
              "Sample L₁…Lₙ from original distribution p(x).",
              "Sort losses and find α-quantile VaRα.",
              "Compute CVaRα as average of losses ≥ VaRα."
            ],
            "limitations": [
              "Requires extremely large number of samples for accurate tail estimation.",
              "High variance when α is small (e.g., 0.01, 0.001).",
              "Inefficient for rare-event risk estimation."
            ]
          },
    
          "Weighted_CVaR_Estimator": {
            "name": "Likelihood-Weighted CVaR Estimator",
            "purpose": "Compute unbiased CVaR using importance sampling weights.",
            "formula": {
              "estimator": "CVaRα = (1 / α) * Σ wᵢ Lᵢ / Σ wᵢ (over tail samples)",
              "components": [
                "wᵢ — importance weights",
                "Lᵢ — losses",
                "α — confidence level"
              ]
            },
            "role_in_paper": "Provides the mathematical core enabling IS-based CVaR estimation.",
            "advantages": [
              "Unbiased estimator even under aggressive tail oversampling.",
              "Lower variance than naive Monte Carlo."
            ]
          },
    
          "Gaussian_Tilted_IS": {
            "name": "Exponential Tilting / Gaussian Shift Importance Sampling",
            "purpose": "Choose q(x) by shifting mean of underlying distribution to increase tail samples.",
            "steps": [
              "Identify mean shift vector θ to maximize tail sampling efficiency.",
              "Construct q(x) = p(x - θ).",
              "Compute likelihood ratios accordingly."
            ],
            "properties": [
              "Simple analytical form for many distributions.",
              "Commonly used for portfolio and derivative risk estimation."
            ]
          },
    
          "Adaptive_IS": {
            "name": "Adaptive Importance Sampling (AIS)",
            "purpose": "Learn an optimal importance distribution that minimizes estimator variance.",
            "procedure": [
              "Start with initial IS distribution q₀(x).",
              "Evaluate variance of estimator using q₀.",
              "Update parameters of q(x) to reduce variance.",
              "Iterate until convergence."
            ],
            "advantages": [
              "Automatic tuning of sampling distribution.",
              "Robust across different risk landscapes."
            ]
          }
        }
      }
    },
    
    {
      "id": 3,
      "title": "Quantum generative modeling for financial time series with temporal correlations",
      "authors": "Dechant et al.",
      "year": 2025,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/2507.22035",
      "performance-metrics": [
        "Distribution Fidelity (Quantum–Classical Distribution Match)",
        "Temporal Autocorrelation Preservation",
        "Cross-Correlation Reconstruction Accuracy",
        "Kullback–Leibler Divergence (KL)",
        "Maximum Mean Discrepancy (MMD)",
        "Wasserstein Distance",
        "Mean Absolute Error of Time-Series Reconstruction",
        "Volatility Clustering Reproduction Score",
        "Sampling Quality (Mode Coverage)",
        "Training Stability (Convergence Behavior)"
      ],
      "tools-used-in": [
        "Quantum-Generative-Adversarial-Network-(QGAN)-for-Scenario-Generation"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Generative_Model": {
            "name": "Quantum Generative Model for Financial Time Series",
            "purpose": "Generate synthetic financial time series that preserve temporal correlations using quantum circuits to model probabilistic structures.",
            "components": {
              "quantum_state_encoding": {
                "method": "Amplitude encoding or qubit-encoded returns",
                "role": "Store historical financial returns or log-returns as quantum states.",
                "effect": "Allows capturing multivariate joint distributions in compact quantum form."
              },
              "parametric_quantum_circuit": {
                "definition": "U(θ) — trainable quantum circuit representing generative distribution",
                "role": "Transforms latent noise distribution into time-series samples.",
                "effect": "Learns nonlinear and temporal dependency structures."
              },
              "temporal_correlation_module": {
                "method": "Sequential quantum circuit blocks",
                "definition": "Circuit layers encoding lagged dependencies between time steps",
                "effect": "Captures autocorrelation, volatility patterns, and cross-time dependencies."
              }
            },
            "procedure": [
              "Collect historical financial time series (returns, log-returns, or features).",
              "Normalize and encode data into quantum states.",
              "Initialize parametric circuit U(θ) for generative sampling.",
              "Train U(θ) using divergence-based or adversarial objective.",
              "Generate synthetic sequences by executing U(θ) repeatedly across time.",
              "Evaluate generated time series using statistical and temporal metrics.",
              "Refine parameters until distributions align with real data."
            ],
            "output": "A quantum-generated time series preserving empirical statistical and temporal structure.",
            "advantages": [
              "Quantum circuits capture complex multivariate structures efficiently.",
              "Better modeling of non-Gaussian, heavy-tailed financial data.",
              "Superior ability to encode sequential dependencies compared to simplistic classical models.",
              "Potential exponential compression compared to classical generative models."
            ]
          },
    
          "Quantum_GAN": {
            "name": "Quantum Generative Adversarial Network (QGAN)",
            "purpose": "Train quantum generator to match real time-series distribution using adversarial learning.",
            "architecture": {
              "generator": "Quantum circuit producing synthetic time-series samples.",
              "discriminator": "Classical or hybrid quantum–classical model distinguishing real vs fake samples."
            },
            "procedure": [
              "Sample real financial time-series segments.",
              "Generate synthetic samples from quantum generator.",
              "Discriminator classifies samples.",
              "Update generator parameters to fool discriminator.",
              "Repeat adversarial training until convergence."
            ],
            "advantages": [
              "Adversarial training encourages high-quality distribution matching.",
              "Handles multimodality and complex, spiky financial distributions.",
              "Effective even with small datasets due to quantum expressivity."
            ],
            "limitations": [
              "Training instability common to GANs.",
              "Requires careful tuning of number of qubits and circuit depth.",
              "Sensitive to quantum noise in NISQ devices."
            ]
          },
    
          "Lagged_Return_Encoding": {
            "name": "Lagged Feature Quantum Encoding",
            "purpose": "Encode temporal structure by embedding lagged returns across qubits.",
            "steps": [
              "Define time windows (lags) capturing autocorrelation patterns.",
              "Map each lagged return to separate qubits or amplitude slots.",
              "Train circuit to reproduce transitions between lagged states."
            ],
            "properties": [
              "Captures volatility clustering.",
              "Supports multivariate temporal correlations.",
              "Preserves ordering information essential for time-series dynamics."
            ]
          },
    
          "Quantum_Sampling_and_Reconstruction": {
            "name": "Quantum Sampling for Time-Series Generation",
            "purpose": "Generate sequential samples from trained quantum circuit.",
            "procedure": [
              "Sample latent state from quantum generator.",
              "Decode sampled amplitudes into time-series values.",
              "Iterate sampling process autoregressively or in parallel.",
              "Reconstruct full temporal sequences."
            ],
            "advantages": [
              "Efficient generation of correlated samples.",
              "Better tail modeling and volatility patterns vs classical sampling."
            ]
          },
    
          "Training_Objectives": {
            "purpose": "Define training losses for quantum generative modeling.",
            "types": {
              "KL_divergence": "Measures mismatch between quantum-generated and real distributions.",
              "MMD": "Matches high-dimensional statistical moments.",
              "Wasserstein_distance": "Provides stable training for spiky empirical distributions."
            },
            "role_in_paper": "Used to evaluate and refine quantum generative model performance."
          }
        }
      }
    },
    
    {
      "id": 4,
      "title": "Integrated GARCH-GRU in Financial Volatility Forecasting",
      "authors": null,
      "year": 2025,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/2504.09380",
      "performance-metrics": [
        "Mean Squared Error (MSE)",
        "Root Mean Squared Error (RMSE)",
        "Mean Absolute Error (MAE)",
        "Mean Absolute Percentage Error (MAPE)",
        "Directional Accuracy (DA)",
        "Volatility Forecast Error (VFE)",
        "Log-Likelihood Improvement",
        "Out-of-sample Forecast Stability",
        "Residual Autocorrelation Reduction",
        "Model Convergence Behavior"
      ],
      "tools-used-in": [
        "GARCH-1-1-Volatility-Forecasting"
      ],
      "paper-details": {
        "algorithms": {
          "Integrated_GARCH_GRU_Model": {
            "name": "Integrated GARCH–GRU Hybrid Volatility Forecaster",
            "purpose": "Combine econometric volatility modeling (GARCH) with deep recurrent neural learning (GRU) to capture nonlinear and long-term dependencies in financial volatility.",
            "components": {
              "garch_component": {
                "definition": "σ²_t = ω + αε²_{t−1} + βσ²_{t−1}",
                "role": "Model short-term volatility clustering and mean-reverting behavior.",
                "effect": "Provides baseline econometric structure for volatility estimates."
              },
              "gru_component": {
                "definition": "GRU processes historical returns and GARCH residuals.",
                "role": "Learn nonlinear temporal dependencies that GARCH cannot capture.",
                "gates": {
                  "update_gate": "Controls how much past information to retain.",
                  "reset_gate": "Controls blending of new input vs historical state."
                },
                "effect": "Improves long-memory modeling, regime shifts, and nonlinear responses."
              },
              "fusion_layer": {
                "method": "Concatenate or weighted-combine GARCH output with GRU hidden state.",
                "role": "Integrate statistical and neural forecasts into final volatility estimate.",
                "effect": "Balances linear (GARCH) and nonlinear (GRU) contributions."
              }
            },
            "procedure": [
              "Compute preliminary volatility estimates using GARCH(1,1).",
              "Encode returns, GARCH volatility, and residual series as GRU inputs.",
              "Train GRU on historical volatility patterns with backpropagation.",
              "Fuse GRU output with GARCH predictions using neural gating or linear combination.",
              "Predict next-step volatility σ²_{t+1}.",
              "Evaluate forecasting accuracy using time-series prediction metrics."
            ],
            "output": "Hybrid volatility prediction incorporating GARCH structure and GRU nonlinear memory.",
            "advantages": [
              "Captures both short-term volatility clustering and long-range nonlinear patterns.",
              "Outperforms standalone GARCH in turbulent market regimes.",
              "More robust to structural breaks and non-stationary behavior.",
              "Provides smoother, more stable forecasts with lower prediction error."
            ]
          },
    
          "GARCH_Model": {
            "name": "GARCH(1,1) Volatility Model",
            "purpose": "Provide a statistical backbone for volatility forecasting using past squared returns and past variance.",
            "formula": {
              "variance_update": "σ²_t = ω + αε²_{t−1} + βσ²_{t−1}",
              "conditions": [
                "α + β < 1 for stationarity",
                "ω > 0, α ≥ 0, β ≥ 0"
              ]
            },
            "procedure": [
              "Estimate parameters (ω, α, β) using maximum likelihood.",
              "Compute residuals ε_t from historical returns.",
              "Iteratively update σ²_t using GARCH recursion.",
              "Feed σ²_t and ε_t into GRU for hybrid learning (in this paper's context)."
            ],
            "advantages": [
              "Strong at modeling volatility clustering.",
              "Simple, interpretable, and historically well-validated."
            ],
            "limitations": [
              "Fails to capture nonlinear patterns.",
              "Has limited memory and weak at handling regime shifts.",
              "Assumes symmetric volatility response."
            ]
          },
    
          "GRU_Model": {
            "name": "Gated Recurrent Unit (GRU)",
            "purpose": "Model long-term, nonlinear financial volatility patterns from time series data.",
            "components": {
              "update_gate": "z_t = σ(W_z x_t + U_z h_{t−1})",
              "reset_gate": "r_t = σ(W_r x_t + U_r h_{t−1})",
              "hidden_state_update": "h_t = (1 − z_t) ⊙ h_{t−1} + z_t ⊙ tanh(W_h x_t + U_h (r_t ⊙ h_{t−1}))"
            },
            "procedure": [
              "Provide sequence inputs such as returns, volatility history, and GARCH residuals.",
              "GRU processes them sequentially to learn hidden volatility structure.",
              "Final hidden state serves as nonlinear volatility estimator.",
              "Fuse with GARCH output for final prediction."
            ],
            "advantages": [
              "Requires fewer parameters than LSTM.",
              "More stable for noisy financial time series.",
              "Learns nonlinear memory patterns missed by econometric models."
            ]
          },
    
          "Loss_Function_and_Training": {
            "name": "Training Objective for GARCH–GRU Hybrid",
            "purpose": "Optimize network weights and GARCH fusion parameters.",
            "types": {
              "mse_loss": "Minimize (σ²_pred − σ²_true)² for accurate volatility prediction.",
              "regularization": "Avoid overfitting during neural network training."
            },
            "training_steps": [
              "Initialize GARCH parameters using MLE.",
              "Pretrain GRU on historical volatility patterns.",
              "Jointly fine-tune fusion model using backpropagation.",
              "Validate using rolling-window forecasting."
            ],
            "advantages": [
              "Stable convergence for time-series prediction.",
              "Improves generalization on unseen data."
            ]
          }
        }
      }
    },
    
    {
      "id": 5,
      "title": "Quantum Subgradient Estimation for Conditional Value-at-Risk Optimization",
      "authors": null,
      "year": 2025,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/2510.04736",
      "performance-metrics": [
        "Subgradient Estimation Error",
        "CVaR Estimation Error",
        "Quantum Query Complexity",
        "Convergence Rate of CVaR Optimization",
        "Variance Reduction in Subgradient Estimates",
        "Approximation Error of Amplitude Estimation",
        "Error Scaling with Confidence Level α",
        "Runtime vs Accuracy Tradeoff",
        "Success Probability of Subgradient Oracle",
        "Stability of Optimization Updates"
      ],
      "tools-used-in": [
        "Quantum-Amplitude-Estimation-(QAE)-for-CVaR"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_CVaR_Subgradient": {
            "name": "Quantum Subgradient Estimation for CVaR Optimization",
            "purpose": "Estimate subgradients of the CVaR function more efficiently using quantum amplitude estimation, enabling faster optimization in risk-sensitive applications.",
            "components": {
              "cvar_definition": {
                "formula": "CVaRα = VaRα + (1 / (1 - α)) * E[(L - VaRα)_+]",
                "role": "Defines the tail-risk objective whose subgradient is required for optimization."
              },
              "loss_quantum_oracle": {
                "method": "Encode loss values L(x) into quantum amplitudes or phase shifts.",
                "role": "Acts as a quantum-accessible function whose expectation corresponds to CVaR tail loss.",
                "effect": "Enables amplitude estimation to extract expectation of tail losses."
              },
              "subgradient_expression": {
                "definition": "g = ∂CVaRα/∂θ computed using sampled quantum estimates of tail-loss expectation",
                "role": "Provides derivative direction for updating model parameters θ."
              }
            },
            "procedure": [
              "Encode loss distribution and decision variable θ into quantum state.",
              "Construct quantum oracle evaluating tail-loss indicator (L ≥ VaRα).",
              "Apply amplitude estimation to compute tail-loss expectation.",
              "Convert estimated expectation into CVaR subgradient value.",
              "Feed subgradient into classical optimizer to update θ.",
              "Repeat until CVaR objective converges."
            ],
            "output": "Estimated subgradient enabling efficient CVaR minimization.",
            "advantages": [
              "Improves scaling from O(1/ε²) in classical Monte Carlo to O(1/ε) with quantum amplitude estimation.",
              "Reduces variance of subgradient estimates.",
              "Enables optimization for small α (rare tail events) where classical methods fail.",
              "Suitable for portfolio optimization, derivative pricing, and risk-sensitive ML."
            ]
          },
    
          "Quantum_Amplitude_Estimation": {
            "name": "Quantum Amplitude Estimation (QAE)",
            "purpose": "Estimate expectations such as tail-loss probabilities more efficiently than classical Monte Carlo.",
            "components": {
              "amplitude_encoding": {
                "definition": "|ψ⟩ = √p |1⟩ + √(1−p) |0⟩",
                "role": "Amplitude p corresponds to probability of tail event (loss ≥ VaRα)."
              },
              "grover_operator": {
                "method": "Iterative Grover rotations amplify amplitudes for estimation.",
                "role": "Enables extraction of amplitude with O(1/ε) complexity."
              }
            },
            "procedure": [
              "Prepare quantum state encoding loss distribution.",
              "Mark tail-loss events using indicator function.",
              "Apply amplitude estimation circuit to estimate probability p.",
              "Estimate expected tail loss E[(L - VaRα)_+].",
              "Combine p and expectation to compute CVaR."
            ],
            "advantages": [
              "Quadratic speedup over classical sampling.",
              "Higher accuracy for rare tail events.",
              "Provides the foundation for quantum subgradient estimation."
            ],
            "limitations": [
              "Requires coherent quantum circuits (NISQ noise sensitivity).",
              "State preparation can be expensive depending on loss model."
            ]
          },
    
          "Classical_CVaR_Optimization": {
            "name": "Classical CVaR Optimization Loop",
            "purpose": "Update parameters θ using subgradients obtained from quantum estimation.",
            "procedure": [
              "Receive quantum-estimated subgradient g(θ).",
              "Update parameters θ ← θ − η g(θ) using gradient-based or subgradient method.",
              "Recompute VaRα and adjust loss oracle if necessary.",
              "Repeat until CVaR converges."
            ],
            "advantages": [
              "Compatible with existing stochastic optimization pipelines.",
              "Quantum subgradients reduce noise and improve directional accuracy."
            ]
          },
    
          "Tail_Event_Oracle": {
            "name": "Quantum Oracle for Tail Loss Events",
            "purpose": "Identify and encode whether a sample belongs to the CVaR tail (loss ≥ VaR).",
            "steps": [
              "Compute L(x) for sample x using quantum oracle.",
              "Compare L(x) against VaRα threshold.",
              "Mark indicator qubit if L(x) exceeds VaR threshold."
            ],
            "properties": [
              "Forms basis of amplitude estimation for tail probability.",
              "Enables selective sampling for CVaR calculation."
            ]
          },
    
          "State_Preparation_Module": {
            "name": "Quantum State Preparation for Loss Distribution",
            "purpose": "Prepare quantum state encoding financial loss distribution or model output distribution.",
            "steps": [
              "Map decision variable θ into quantum parameters.",
              "Encode probability distribution of losses via unitary transformations.",
              "Ensure smoothness for efficient amplitude estimation."
            ],
            "role_in_paper": "Provides the foundational quantum representation required for CVaR subgradient extraction."
          }
        }
      }
    },
    
    {
      "id": 6,
      "title": "Benchmarking Quantum Solvers in Noisy Digital Simulations for Financial Portfolio Optimization",
      "authors": null,
      "year": 2025,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/2508.21123",
      "performance-metrics": [
        "Approximation Ratio",
        "Energy Expectation Value",
        "Portfolio Objective Value (Risk–Return)",
        "Constraint Satisfaction Rate",
        "Probability of Optimal Portfolio State",
        "Noise Robustness Score",
        "Circuit Fidelity under Noise",
        "Gate Error Sensitivity",
        "Depth vs Accuracy Degradation",
        "Sampling Cost vs Solution Quality",
        "Convergence Stability across Noise Levels",
        "Hardware-Efficient Ansatz Performance"
      ],
      "tools-used-in": [
        "Quantum-Approximate-Optimization-Algorithm-(QAOA)-for-CVaR-based-Portfolio-Optimization"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Solver_Benchmarking_Framework": {
            "name": "Benchmarking Framework for Quantum Portfolio Solvers",
            "purpose": "Evaluate and compare quantum optimization algorithms under noisy digital simulations for portfolio optimization problems.",
            "components": {
              "portfolio_qubo_model": {
                "definition": "QUBO representation of mean–variance portfolio optimization.",
                "role": "Provides unified benchmark objective across solvers.",
                "effect": "Ensures all quantum solvers are tested on identical problem instances."
              },
              "noise_model": {
                "definition": "Custom digital noise simulation modeling depolarizing, amplitude-damping, and gate errors.",
                "role": "Mimics behavior of NISQ devices in classical simulation.",
                "effect": "Allows comparing solver robustness under realistic noise."
              },
              "solver_suite": {
                "includes": [
                  "Standard QAOA",
                  "Warm-start QAOA",
                  "Hardware-efficient variational circuits",
                  "Quantum Imaginary Time Evolution (QITE)",
                  "VQE-based portfolio solvers"
                ],
                "role": "Benchmark quantum and hybrid solvers under identical conditions."
              }
            },
            "procedure": [
              "Define portfolio optimization problem in QUBO/Hamiltonian form.",
              "Select noise models representing realistic digital quantum simulation environments.",
              "Run each solver across various noise strengths and circuit depths.",
              "Compute approximation ratios and constraint violation rates.",
              "Evaluate solver performance against classical baselines.",
              "Analyze robustness and scalability under noise."
            ],
            "output": "Performance comparison table of quantum solvers under noisy conditions.",
            "advantages": [
              "Provides quantitative insight into quantum advantage viability.",
              "Highlights solver robustness under realistic noise.",
              "Identifies optimal circuit depth and parameterization for financial problems."
            ]
          },
    
          "QAOA_Solver": {
            "name": "Quantum Approximate Optimization Algorithm (QAOA)",
            "purpose": "Minimize portfolio QUBO Hamiltonian using alternating quantum operators.",
            "operator_layers": {
              "cost_layer": "U_C(γ) = e^{-iγH_cost}",
              "mixer_layer": "U_M(β) = e^{-iβH_mix}"
            },
            "procedure": [
              "Initialize uniform superposition of all portfolio bitstrings.",
              "Apply p rounds of cost and mixer unitaries.",
              "Optimize variational parameters (γ, β) using noisy energy evaluations.",
              "Sample from final quantum state to obtain portfolio selections.",
              "Evaluate performance under varying noise strengths."
            ],
            "advantages": [
              "Simple, well-studied algorithm for QUBO problems.",
              "Can exploit problem structures with warm-start or layer-specific tuning.",
              "Performs reasonably well even in moderately noisy environments."
            ],
            "limitations": [
              "Highly sensitive to growing circuit depth.",
              "Noise reduces gradient clarity and hampers convergence.",
              "Requires many samples for accurate energy estimation under noise."
            ]
          },
    
          "Warm_Start_QAOA": {
            "name": "Warm-Start Quantum Approximate Optimization Algorithm",
            "purpose": "Improve QAOA performance by initializing quantum states closer to classical optimum.",
            "components": {
              "initial_state_preparation": {
                "method": "Encode classical convex relaxation solution into amplitudes or biased bitstring distribution.",
                "role": "Provides head-start that reduces search space under noise."
              }
            },
            "procedure": [
              "Solve convex relaxation of portfolio problem using classical optimizer.",
              "Construct quantum initial state reflecting classical solution probabilities.",
              "Run QAOA with reduced requirement for deep circuits.",
              "Evaluate noise-resilience and approximation quality."
            ],
            "advantages": [
              "Better convergence in noisy environments.",
              "Lower circuit depth needed for high-quality solutions.",
              "Improved approximation ratio vs standard QAOA."
            ]
          },
    
          "Hardware_Efficient_Ansatz": {
            "name": "Hardware-Efficient Variational Ansatz",
            "purpose": "Provide noise-resilient quantum circuits tailored to realistic hardware topologies.",
            "components": {
              "parameterized_rotations": "Single-qubit Ry, Rz gates.",
              "entangling_layers": "Nearest-neighbor CX gates optimized for hardware connectivity."
            },
            "procedure": [
              "Initialize circuit with hardware-efficient block structure.",
              "Train ansatz to minimize portfolio Hamiltonian.",
              "Benchmark performance against QAOA under noise."
            ],
            "advantages": [
              "Low circuit depth reduces noise impact.",
              "Scalable to larger qubit systems.",
              "Often converges faster despite non-problem-specific design."
            ]
          },
    
          "QITE_Solver": {
            "name": "Quantum Imaginary Time Evolution (QITE)",
            "purpose": "Approximate classical imaginary-time evolution to project toward ground state of portfolio Hamiltonian.",
            "procedure": [
              "Construct Trotterized imaginary time steps.",
              "Apply local unitary approximations representing e^{-τH}.",
              "Iteratively evolve quantum state toward low-energy solutions.",
              "Benchmark energy convergence under noise."
            ],
            "advantages": [
              "Less sensitive to noise than QAOA for shallow-depth approximations.",
              "Provides alternative solver with different error characteristics."
            ],
            "limitations": [
              "Requires careful parameter tuning.",
              "May not scale well for highly nonlocal Hamiltonians."
            ]
          },
    
          "Classical_Baseline_Models": {
            "name": "Classical Solvers for Benchmarking",
            "purpose": "Provide reference results for evaluating quantum solver performance.",
            "types": {
              "simulated_annealing": "Stochastic global optimization baseline.",
              "convex_relaxation_methods": "Solve relaxed portfolio models for warm starts or ground truth."
            },
            "role_in_paper": "Serve as baselines to measure whether quantum solvers achieve competitive performance under noise."
          }
        }
      }
    },
    
    {
      "id": 7,
      "title": "Quantum Risk Analysis of Financial Derivatives",
      "authors": "Stamatopoulos, Clader, Woerner, Zeng",
      "year": 2024,
      "source": "arXiv",
      "url": "https://arxiv.org/pdf/2404.10088",
      "performance-metrics": [
        "Pricing Accuracy vs Classical Monte Carlo",
        "Variance Reduction in Risk Metrics",
        "Quantum Query Complexity",
        "Convergence Rate of QAE-based Estimators",
        "Expected Value Estimation Error",
        "VaR and CVaR Estimation Error",
        "Confidence Interval Width Reduction",
        "Sampling Efficiency (Samples vs Accuracy)",
        "Noise Sensitivity of Amplitude Estimation",
        "Probability of Correct Risk Metric Extraction"
      ],
      "tools-used-in": [
        "Quantum-Amplitude-Estimation-(QAE)-for-CVaR"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Risk_Analysis_Framework": {
            "name": "Quantum Risk Analysis Framework for Financial Derivatives",
            "purpose": "Use quantum amplitude estimation (QAE) and quantum state preparation methods to evaluate expected derivative payoffs and tail-risk metrics faster than classical Monte Carlo.",
            "components": {
              "quantum_state_preparation": {
                "method": "Encode underlying asset price distribution into amplitudes.",
                "role": "Represent stochastic price paths or payoff distributions quantum-mechanically.",
                "effect": "Enables quantum estimation of expected values and tail probabilities."
              },
              "payoff_operator": {
                "definition": "Unitary oracle that computes derivative payoff f(S_T) for asset price S_T.",
                "role": "Maps payoff values to amplitudes or ancilla qubits for amplitude estimation.",
                "effect": "Allows estimation of expected derivative payoff via quantum circuits."
              },
              "risk_metric_encoder": {
                "definition": "Binary or amplitude-based marking for loss/payoff thresholds.",
                "role": "Enable VaR and CVaR computation using quantum metric estimators.",
                "effect": "Provides quantum-accessible representation of tail-risk behavior."
              }
            },
            "procedure": [
              "Prepare probability distribution of underlying asset(s) via quantum state encoding.",
              "Construct payoff oracle for derivative pricing or loss evaluation.",
              "Apply amplitude estimation to extract expected payoff or tail probability.",
              "Compute VaR, CVaR, or expected payoff using QAE outputs.",
              "Compare quantum estimator performance against classical Monte Carlo."
            ],
            "output": "Quantum-estimated derivative price and associated risk metrics (VaR/CVaR).",
            "advantages": [
              "Quadratic speedup in sample complexity compared to classical Monte Carlo.",
              "Improved precision for rare-event (tail) probability estimation.",
              "Efficient for pricing complex derivatives with path-dependent payoffs.",
              "Provides a scalable foundation for quantum risk management."
            ]
          },
    
          "Quantum_Amplitude_Estimation": {
            "name": "Quantum Amplitude Estimation (QAE) for Expected Value and CVaR",
            "purpose": "Estimate expectations and tail probabilities using quantum amplitudes with quadratic speedup.",
            "components": {
              "amplitude_encoding": {
                "definition": "|ψ⟩ = √p |1⟩ + √(1−p) |0⟩",
                "role": "Encodes probability of payoff exceeding a threshold or belonging to tail distribution."
              },
              "grover_iterations": {
                "method": "Repeated Grover-like reflections to amplify amplitudes.",
                "role": "Enable extraction of amplitude p with O(1/ε) complexity."
              }
            },
            "procedure": [
              "Prepare quantum state encoding payoff or loss distribution.",
              "Construct marking operation for event-of-interest (e.g., payoff > K or L > VaR).",
              "Apply amplitude estimation to compute probability p.",
              "Convert p into derivative price, VaR, or CVaR depending on metric."
            ],
            "advantages": [
              "Quadratic improvement vs classical Monte Carlo's error scaling.",
              "Better performance for financial applications requiring small error tolerance.",
              "Ideal for derivative pricing and tail-risk estimation."
            ],
            "limitations": [
              "Requires fault-tolerant quantum hardware for full QAE.",
              "State preparation complexity depends on underlying model."
            ]
          },
    
          "Derivative_Payoff_Oracle": {
            "name": "Quantum Payoff Oracle",
            "purpose": "Evaluate derivative payoff inside quantum circuitry.",
            "steps": [
              "Encode asset price S into quantum registers.",
              "Apply arithmetic circuits to compute f(S_T).",
              "Write payoff into ancilla amplitude or binary register.",
              "Use resulting amplitude/state for QAE input."
            ],
            "properties": [
              "Supports European, Asian, and path-dependent derivatives.",
              "Allows direct estimation of payoff expectations and distributions."
            ]
          },
    
          "VaR_CVaR_Quantum_Computation": {
            "name": "Quantum Computation of VaR & CVaR",
            "purpose": "Leverage QAE outputs to estimate tail-based financial risks.",
            "procedure": [
              "Use QAE to estimate distribution tail probability P(L ≥ ℓ).",
              "Find VaRα via binary search using amplitude estimation.",
              "Compute CVaRα using expected tail loss estimator.",
              "Combine these metrics into full quantum risk analysis."
            ],
            "advantages": [
              "Accurate estimation of low-probability tail events.",
              "Better scaling for high-confidence CVaR (α → 0.99, 0.995, 0.999).",
              "Useful for highly nonlinear derivatives and stress-testing."
            ]
          },
    
          "Classical_Baseline_Comparison": {
            "name": "Classical Monte Carlo Baseline",
            "purpose": "Provide benchmarks for evaluating quantum speedups.",
            "steps": [
              "Simulate asset paths.",
              "Compute derivative payoffs across simulations.",
              "Estimate expected payoff, VaR, and CVaR.",
              "Compare accuracy and computational cost with QAE."
            ],
            "role_in_paper": "Establishes relative efficiency of quantum risk analysis."
          }
        }
      }
    },
    
    {
      "id": 8,
      "title": "Enhancing Financial Time Series Prediction with Quantum-Enhanced Synthetic Data Generation: A Case Study on the S&P 500 Using a Quantum Wasserstein GAN",
      "authors": "Orlandi, Barbierato & Gatti",
      "year": 2024,
      "source": "MDPI",
      "url": "https://www.mdpi.com/2079-9292/13/11/2158",
      "performance-metrics": [
        "Wasserstein Distance (W1)",
        "Distribution Divergence (KL / JS Divergence)",
        "Autocorrelation Reconstruction Score",
        "Cross-Correlation Preservation",
        "Prediction Accuracy Improvement (RMSE / MAE)",
        "Volatility Pattern Similarity",
        "Synthetic–Real Statistical Alignment Score",
        "Temporal Dependency Preservation Score",
        "Training Stability of Quantum GAN",
        "Mode Collapse Severity Metric",
        "Quality of Synthetic Data for Downstream Prediction Tasks"
      ],
      "tools-used-in": [
        "Quantum-Generative-Adversarial-Network-(QGAN)-for-Scenario-Generation"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Wasserstein_GAN": {
            "name": "Quantum Wasserstein GAN (QWGAN)",
            "purpose": "Generate high-fidelity synthetic S&P 500 time-series data by leveraging quantum circuits to improve distribution learning and temporal structure modeling.",
            "components": {
              "quantum_generator": {
                "definition": "Parameterized quantum circuit U(θ) generating synthetic return distributions.",
                "role": "Acts as the generator in the GAN architecture.",
                "effect": "Quantum expressivity improves modeling of non-Gaussian, heavy-tailed financial distributions."
              },
              "wasserstein_loss": {
                "definition": "W1(G, R) = E[D(real)] − E[D(fake)]",
                "role": "Stabilizes training, reduces mode collapse.",
                "effect": "Provides smoother gradients for quantum generator updates."
              },
              "gradient_penalty": {
                "definition": "λ · (||∇D(ẋ)||₂ − 1)²",
                "role": "Enforces Lipschitz constraint in Wasserstein GAN.",
                "effect": "Ensures stable convergence, especially with quantum-generated samples."
              }
            },
            "procedure": [
              "Encode real S&P 500 returns into feature space.",
              "Initialize parameterized quantum generator circuit.",
              "Train Wasserstein critic to distinguish real vs quantum-generated samples.",
              "Update generator parameters to minimize Wasserstein distance.",
              "Generate synthetic financial time series.",
              "Evaluate statistical alignment between synthetic and real series.",
              "Use synthetic data to improve downstream forecasting models."
            ],
            "output": "Quantum-enhanced synthetic time series with improved statistical realism.",
            "advantages": [
              "Quantum generator captures nonlinear temporal dependencies better than classical GANs.",
              "Improves forecasting accuracy on downstream ML models.",
              "Reduces divergence between real and synthetic distributions.",
              "Significantly mitigates mode collapse compared to classical GAN baselines."
            ]
          },
    
          "TimeSeries_Encoding": {
            "name": "Quantum-Compatible Time Series Encoding",
            "purpose": "Convert S&P 500 daily returns into quantum-representable states.",
            "methods": {
              "amplitude_encoding": "Stores normalized time-series samples in quantum amplitudes.",
              "angle_encoding": "Maps returns to rotation angles (e.g., Rx, Rz)."
            },
            "procedure": [
              "Standardize time-series values.",
              "Select encoding scheme based on qubit budget.",
              "Embed temporal slices or sliding windows into quantum states.",
              "Feed encoded data into QWGAN training loop."
            ],
            "advantages": [
              "Preserves statistical and temporal consistency.",
              "Enables efficient representation of multivariate features."
            ]
          },
    
          "Classical_Critic": {
            "name": "Wasserstein Critic (Classical Neural Network)",
            "purpose": "Evaluate the distance between real and synthetic data distributions.",
            "components": {
              "architecture": "Feedforward critic network with Lipschitz constraint.",
              "loss": "Wasserstein-1 distance with gradient penalty."
            },
            "procedure": [
              "Compute critic score for real samples.",
              "Compute critic score for quantum-generated samples.",
              "Update critic weights to maximize distance.",
              "Guide quantum generator updates."
            ],
            "advantages": [
              "Stable and expressive discriminator for training quantum generators.",
              "Supports smooth gradients required for quantum optimization."
            ]
          },
    
          "Forecasting_Model": {
            "name": "Downstream Forecasting Model (Classical ML)",
            "purpose": "Predict future S&P 500 values using enriched datasets that include quantum-synthetic data.",
            "examples": {
              "LSTM": "Captures long-range dependencies in time series.",
              "GRU": "Efficient recurrent model for noisy financial data.",
              "CNN": "Extracts local temporal patterns."
            },
            "procedure": [
              "Train baseline model on real data alone.",
              "Augment training dataset with QWGAN-synthetic samples.",
              "Retrain model on combined dataset.",
              "Compare forecasting accuracy improvement."
            ],
            "advantages": [
              "Synthetic quantum data mitigates overfitting.",
              "Improves generalization in volatile market regimes."
            ]
          },
    
          "Evaluation_Framework": {
            "name": "Statistical and Predictive Evaluation Framework",
            "purpose": "Quantify how realistic and useful the synthetic time series are.",
            "metrics": {
              "distribution_alignment": [
                "Wasserstein Distance",
                "KL Divergence",
                "Jensen–Shannon Divergence"
              ],
              "temporal_metrics": [
                "Autocorrelation Similarity",
                "Volatility Clustering Match",
                "Cross-correlation Preservation"
              ],
              "forecasting_metrics": [
                "RMSE Improvement",
                "MAE Improvement",
                "Directional Accuracy Increase"
              ]
            },
            "role_in_paper": "Demonstrates the effectiveness of quantum-enhanced synthetic data in real forecasting tasks."
          }
        }
      }
    },
    
    {
      "id": 9,
      "title": "Forecasting conditional volatility based on hybrid GARCH",
      "authors": "Huang et al.",
      "year": 2024,
      "source": "ScienceDirect",
      "url": "https://www.sciencedirect.com/science/article/abs/pii/S1062940824000731",
      "performance-metrics": [
        "Mean Squared Error (MSE)",
        "Root Mean Squared Error (RMSE)",
        "Mean Absolute Error (MAE)",
        "Mean Absolute Percentage Error (MAPE)",
        "Quasi-Likelihood Error",
        "Directional Accuracy (DA)",
        "Volatility Persistence Measurement",
        "Residual Autocorrelation Reduction",
        "Out-of-Sample Forecast Accuracy",
        "Stability Across Market Regimes"
      ],
      "tools-used-in": [
        "GARCH-1-1-Volatility-Forecasting"
      ],
      "paper-details": {
        "algorithms": {
          "Hybrid_GARCH_Model": {
            "name": "Hybrid GARCH Framework for Volatility Forecasting",
            "purpose": "Improve volatility forecasting accuracy by integrating classical GARCH with nonlinear components, such as machine learning or nonlinear filters, to capture complex market dynamics.",
            "components": {
              "baseline_garch": {
                "definition": "σ²_t = ω + α ε²_{t−1} + β σ²_{t−1}",
                "role": "Captures traditional volatility clustering and mean reversion.",
                "effect": "Provides stable short-term volatility structure."
              },
              "nonlinear_component": {
                "methods": [
                  "Neural network residual correction",
                  "Nonlinear autoregressive mapping",
                  "Smoothing / signal decomposition"
                ],
                "role": "Model nonlinear volatility patterns GARCH alone cannot represent.",
                "effect": "Enhances prediction accuracy during turbulent or irregular market phases."
              },
              "fusion_module": {
                "definition": "Weighted or functional combination of GARCH output with nonlinear model output.",
                "role": "Integrate econometric and ML-derived signal.",
                "effect": "Balances linear and nonlinear dynamics for improved volatility forecasts."
              }
            },
            "procedure": [
              "Estimate classical GARCH model using historical returns.",
              "Extract GARCH residuals and volatility estimates.",
              "Feed residuals or volatility series into nonlinear model.",
              "Fuse outputs using adaptive weighting or regression mapping.",
              "Evaluate and refine hybrid model using rolling-window forecast tests."
            ],
            "output": "Next-period conditional volatility forecast σ²_{t+1}.",
            "advantages": [
              "Captures both linear GARCH dynamics and nonlinear market behavior.",
              "More robust in high-volatility market regimes.",
              "Reduces forecast error vs. simple GARCH and other econometric models."
            ]
          },
    
          "GARCH_Model": {
            "name": "Standard GARCH(1,1)",
            "purpose": "Provide a statistical backbone for conditional volatility estimation.",
            "formula": {
              "variance_update": "σ²_t = ω + α ε²_{t−1} + β σ²_{t−1}",
              "conditions": [
                "α + β < 1 for stationarity",
                "ω > 0, α ≥ 0, β ≥ 0"
              ]
            },
            "procedure": [
              "Estimate (ω, α, β) via maximum likelihood.",
              "Generate volatility series σ²_t.",
              "Provide GARCH forecasts to hybrid nonlinear model."
            ],
            "advantages": [
              "Simple, interpretable, historically established model.",
              "Well suited for capturing volatility clustering."
            ],
            "limitations": [
              "Struggles with nonlinear relationships.",
              "Fails under heavy-tailed returns and structural breaks."
            ]
          },
    
          "Nonlinear_Filter_or_ML_Component": {
            "name": "Nonlinear Enhancement Model",
            "purpose": "Model higher-order dependencies not captured by classical GARCH.",
            "possible_methods": {
              "ML": [
                "Feedforward Neural Networks",
                "GRU/LSTM (if used)",
                "Kernel-based regressors"
              ],
              "signal_models": [
                "Nonlinear autoregressive filters",
                "Wavelet decomposition of volatility signal"
              ]
            },
            "procedure": [
              "Receive GARCH outputs and residuals.",
              "Learn nonlinear transformation mapping.",
              "Provide correction terms or refined volatility signals.",
              "Combine with GARCH predictions for final output."
            ],
            "advantages": [
              "Improves forecast accuracy when markets behave nonlinearly.",
              "Allows capturing irregular volatility spikes."
            ]
          },
    
          "Fusion_and_Optimization": {
            "name": "Hybrid Model Fusion Layer",
            "purpose": "Combine econometric and nonlinear predictions into a unified volatility estimate.",
            "methods": [
              "Linear weighted combination",
              "Adaptive regression-based combination",
              "Neural gating mechanism"
            ],
            "procedure": [
              "Compute volatility estimates from GARCH and nonlinear model.",
              "Apply fusion formula or learned weights.",
              "Output final hybrid volatility estimate."
            ],
            "advantages": [
              "Allows flexible integration depending on data regime.",
              "Improves robustness and reduces prediction variance."
            ]
          },
    
          "Evaluation_Framework": {
            "name": "Volatility Forecast Evaluation System",
            "purpose": "Assess performance of hybrid GARCH vs. benchmarks.",
            "metrics": {
              "error_metrics": [
                "MSE",
                "RMSE",
                "MAE",
                "MAPE"
              ],
              "statistical_tests": [
                "Ljung–Box test on residuals",
                "ARCH-LM test",
                "Volatility persistence evaluation"
              ],
              "forecasting_tests": [
                "Out-of-sample predictive accuracy",
                "Regime-specific performance",
                "Directional Accuracy"
              ]
            },
            "role_in_paper": "Demonstrates superiority of hybrid GARCH over classical GARCH and competing econometric models."
          }
        }
      }
    },
    
    {
      "id": 10,
      "title": "Assessing risk forecasting models: Monte Carlo and machine learning approaches",
      "authors": "Giot & Laurent",
      "year": 2024,
      "source": "ScienceDirect",
      "url": "https://www.sciencedirect.com/science/article/abs/pii/S1057521924000152",
      "performance-metrics": [
        "Value-at-Risk Prediction Accuracy",
        "Conditional Value-at-Risk (CVaR) Prediction Accuracy",
        "Backtesting Failure Rate (Kupiec Test)",
        "Independence Test Statistic (Christoffersen Test)",
        "Expected Shortfall Backtesting Error",
        "Mean Squared Error of Forecasted Loss Distribution",
        "Hit Ratio (Correct Tail Exceedances)",
        "Loss Function Value (Quantile Loss)",
        "Monte Carlo Simulation Convergence Rate",
        "Model Stability Across Market Regimes"
      ],
      "tools-used-in": [
        "Monte-Carlo-Simulation-for-VAR-CVAR"
      ],
      "paper-details": {
        "algorithms": {
          "MonteCarlo_RiskForecasting": {
            "name": "Monte Carlo Simulation for Risk Forecasting",
            "purpose": "Generate simulated paths of financial returns or losses to estimate VaR and CVaR using empirical distribution properties.",
            "components": {
              "stochastic_process": {
                "definition": "Simulate returns using models such as geometric Brownian motion or GARCH-based volatility.",
                "role": "Reproduce realistic loss distributions needed for tail-risk estimation.",
                "effect": "Allows risk forecasting under different market volatility conditions."
              },
              "path_simulation": {
                "definition": "Large number of simulated future asset price paths.",
                "role": "Construct empirical loss distribution.",
                "effect": "Estimate VaR and CVaR through statistical quantiles."
              },
              "tail_estimation": {
                "method": "Compute quantiles and average tail losses.",
                "role": "Quantify extreme downside risk (VaR/CVaR).",
                "effect": "Provides unbiased estimator under sufficient sample size."
              }
            },
            "procedure": [
              "Define stochastic model of returns or volatility.",
              "Generate N simulated price or return paths.",
              "Construct empirical loss distribution.",
              "Compute VaRα as α-tail quantile.",
              "Compute CVaRα as mean of losses beyond VaRα.",
              "Backtest model against realized losses."
            ],
            "output": "Monte Carlo-based VaR and CVaR forecasts.",
            "advantages": [
              "Flexible: works with any distribution or volatility assumption.",
              "Captures nonlinear and tail-heavy behavior.",
              "Widely used in regulatory risk frameworks."
            ],
            "limitations": [
              "Slow convergence: requires large number of simulations.",
              "Sensitive to model assumptions (e.g., volatility model).",
              "Fails to adapt quickly to regime shifts."
            ]
          },
    
          "MachineLearning_RiskModels": {
            "name": "Machine Learning Approaches for Risk Forecasting",
            "purpose": "Use ML models to learn complex nonlinear relationships in financial returns, improving VaR and CVaR predictions.",
            "components": {
              "feature_set": {
                "includes": [
                  "Lagged returns",
                  "Volatility indices",
                  "Macroeconomic indicators",
                  "Technical indicators"
                ],
                "role": "Provide richer input space than classical econometric models."
              },
              "learning_models": {
                "types": [
                  "Random Forest Regressors",
                  "Gradient Boosted Trees",
                  "Neural Networks",
                  "Support Vector Regression"
                ],
                "role": "Predict distribution parameters or quantiles directly."
              },
              "quantile_loss": {
                "definition": "Lossτ(y, ŷ) = max(τ(y − ŷ), (τ−1)(y − ŷ))",
                "role": "Trains models to estimate quantiles (e.g., VaR)."
              }
            },
            "procedure": [
              "Collect historical return and market data.",
              "Engineer predictive features for ML algorithms.",
              "Train ML models to estimate quantile predictions (for VaR) or tail expectations (for CVaR).",
              "Validate predictions using backtesting.",
              "Compare ML forecasts against Monte Carlo and econometric baselines."
            ],
            "output": "ML-based forecasts of VaR, CVaR, or full conditional loss distribution.",
            "advantages": [
              "Captures nonlinear and regime-dependent risk patterns.",
              "Requires fewer assumptions than classical statistical models.",
              "Can outperform Monte Carlo in fast-changing markets."
            ],
            "limitations": [
              "Risk of overfitting, especially during low-volatility regimes.",
              "Dependence on feature quality.",
              "Lack of interpretability in deep models."
            ]
          },
    
          "Backtesting_Framework": {
            "name": "Backtesting and Statistical Validation Framework",
            "purpose": "Evaluate the quality of VaR and CVaR predictions using regulatory-standard tests.",
            "tests": {
              "kupiec_test": {
                "definition": "Tests if violation frequency matches expected α-level.",
                "purpose": "Ensures correct unconditional coverage."
              },
              "christoffersen_test": {
                "definition": "Tests clustering of violations.",
                "purpose": "Ensures independence of tail events."
              },
              "expected_shortfall_backtest": {
                "definition": "Evaluates whether CVaR forecasts match actual tail losses.",
                "purpose": "Tests tail severity prediction accuracy."
              }
            },
            "procedure": [
              "Generate VaR/CVaR forecasts.",
              "Compare predicted quantiles with realized losses.",
              "Record exceedances and evaluate frequency.",
              "Apply Kupiec and Christoffersen tests.",
              "Evaluate average realized tail losses against CVaR forecasts."
            ],
            "role_in_paper": "Demonstrates relative performance differences between Monte Carlo and ML models."
          },
    
          "Econometric_Baseline_Models": {
            "name": "Econometric Baseline Models for Comparison",
            "purpose": "Provide reference VaR/CVaR predictions for evaluating ML and MC approaches.",
            "models": [
              "GARCH(1,1)",
              "EGARCH",
              "GJR-GARCH",
              "Historical Simulation"
            ],
            "role_in_paper": "Used as benchmarks to highlight improvements from ML and Monte Carlo techniques."
          }
        }
      }
    },
    
    {
      "id": 11,
      "title": "Dynamic threshold exceedance models and extreme risk forecasting",
      "authors": "Candia et al.",
      "year": 2024,
      "source": null,
      "url": "https://www.sciencedirect.com/science/article/abs/pii/S0927539824000239",
      "performance-metrics": [
        "Value-at-Risk (VaR) Forecast Accuracy",
        "Conditional Value-at-Risk (CVaR) Forecast Accuracy",
        "Threshold exceedance frequency error",
        "Generalized Pareto Distribution (GPD) Fit Quality",
        "Backtesting Failure Rate (Kupiec Test)",
        "Independence Test Statistic (Christoffersen Test)",
        "Extreme Tail Prediction Error",
        "Expected Shortfall Accuracy",
        "Peak Over Threshold (POT) exceedance intensity error",
        "Stability Across Market Regimes",
        "Tail Index Estimation Error (Shape Parameter ξ)"
      ],
      "tools-used-in": [
        "Extreme-Value-Theory-(EVT)–Peaks-Over-Threshold"
      ],
      "paper-details": {
        "algorithms": {
          "Dynamic_Threshold_Exceedance_Model": {
            "name": "Dynamic Threshold Exceedance Model (DTEM)",
            "purpose": "Model extreme losses by dynamically adjusting the threshold for peaks-over-threshold (POT) modeling, improving tail estimation and extreme risk forecasting.",
            "components": {
              "dynamic_threshold": {
                "definition": "Threshold u_t updated over time instead of a static, fixed threshold.",
                "role": "Adjusts to changing volatility and distributional dynamics.",
                "effect": "Improves reliability of extreme-value predictions during turbulent markets."
              },
              "generalized_pareto_distribution": {
                "definition": "GPD(x | ξ, β) = 1 − (1 + ξx/β)^(-1/ξ)",
                "role": "Models exceedances beyond the dynamic threshold.",
                "parameters": [
                  "ξ — tail index (shape parameter)",
                  "β — scale parameter"
                ],
                "effect": "Allows flexible modeling of fat-tailed return distributions."
              },
              "exceedance_intensity_process": {
                "definition": "λ_t models arrival rate of threshold exceedances.",
                "role": "Captures clustering of extremes.",
                "effect": "Handles time-varying extremal risk."
              }
            },
            "procedure": [
              "Select initial threshold based on quantiles or scale-space rules.",
              "Update threshold dynamically using volatility, return levels, or probabilistic filters.",
              "Extract exceedances X_t = R_t − u_t whenever R_t < u_t (loss convention).",
              "Fit Generalized Pareto Distribution (GPD) to exceedances.",
              "Estimate VaR and CVaR from fitted GPD tail.",
              "Evaluate and backtest forecasts using extreme event tests."
            ],
            "output": "Dynamic tail risk forecasts including VaR, CVaR, and exceedance probability estimates.",
            "advantages": [
              "Adapts to changing market regimes.",
              "More accurate extreme tail modeling vs fixed-threshold EVT.",
              "Improves forecasting stability and robustness.",
              "Captures clustering of extreme events (volatility burst periods)."
            ]
          },
    
          "Peaks_Over_Threshold_EVT": {
            "name": "Peaks-Over-Threshold (POT) Extreme Value Theory Model",
            "purpose": "Provide statistical modeling of tail losses using the GPD distribution for exceedances beyond a chosen threshold.",
            "components": {
              "threshold_selection": {
                "methods": [
                  "Mean Excess Plot",
                  "Stability Plot",
                  "Quantile-based threshold selection"
                ],
                "role": "Ensure enough exceedances while minimizing bias."
              },
              "tail_index_estimation": {
                "definition": "Estimate ξ to determine heaviness of tail.",
                "role": "Crucial for accurate VaR and CVaR projections under EVT."
              }
            },
            "procedure": [
              "Choose an appropriate threshold u.",
              "Extract exceedances above threshold.",
              "Fit GPD to exceedances using maximum likelihood.",
              "Compute tail probability and quantile estimates.",
              "Produce EVT-based VaR and CVaR metrics."
            ],
            "advantages": [
              "Statistically grounded model for extreme losses.",
              "More accurate for tail estimation than Gaussian/GARCH models.",
              "Effective even for small exceedance datasets."
            ],
            "limitations": [
              "Static thresholds cannot adapt to time-varying risk.",
              "Sensitive to threshold selection."
            ]
          },
    
          "Tail_Risk_Forecasting": {
            "name": "Extreme Tail Risk Forecasting using EVT and Dynamic Thresholds",
            "purpose": "Forecast VaR, CVaR, and exceedance probabilities using time-varying EVT parameters.",
            "metrics_used": [
              "Tail quantiles",
              "GPD survival function",
              "Conditional exceedance probabilities"
            ],
            "procedure": [
              "Model threshold exceedance process over time.",
              "Estimate time-varying GPD parameters (ξ_t, β_t).",
              "Compute dynamic VaR as quantile of tail distribution.",
              "Compute dynamic CVaR as expected extreme loss beyond VaR.",
              "Backtest predictions using extreme-value tests."
            ],
            "advantages": [
              "Captures dynamic changes in tail extremity.",
              "More robust under volatility clustering.",
              "Improves forecasting accuracy during crises."
            ]
          },
    
          "Backtesting_Framework": {
            "name": "Extreme Risk Backtesting Framework",
            "purpose": "Evaluate accuracy and reliability of extreme risk forecasts.",
            "tests": {
              "Kupiec_test": {
                "definition": "Tests if number of exceedances matches expected VaR frequency.",
                "purpose": "Verifies unconditional coverage."
              },
              "Christoffersen_test": {
                "definition": "Tests independence of exceedances.",
                "purpose": "Checks if extreme events cluster undesirably."
              },
              "ES_backtest": {
                "definition": "Compares realized extreme losses to forecasted CVaR.",
                "purpose": "Validates tail severity estimation."
              }
            },
            "role_in_paper": "Demonstrates improvements of dynamic threshold models over static EVT and GARCH-based risk forecasts."
          }
        }
      }
    },
    
    {
      "id": 12,
      "title": "Robust estimation of the range-based GARCH model",
      "authors": null,
      "year": 2024,
      "source": "ScienceDirect",
      "url": "https://www.sciencedirect.com/science/article/pii/S026499932400244X",
      "performance-metrics": [
        "Mean Squared Error (MSE)",
        "Root Mean Squared Error (RMSE)",
        "Mean Absolute Error (MAE)",
        "Quasi-Likelihood Estimation Error",
        "Forecast Bias",
        "Directional Accuracy (DA)",
        "Volatility Persistence Estimate Accuracy",
        "Robustness to Outliers and Jumps",
        "In-Sample Log-Likelihood",
        "Out-of-Sample Volatility Forecast Error"
      ],
      "tools-used-in": [
        "GARCH-1-1-Volatility-Forecasting"
      ],
      "paper-details": {
        "algorithms": {
          "Range_Based_GARCH_Model": {
            "name": "Range-Based GARCH Model",
            "purpose": "Improve volatility estimation by incorporating high–low price range data, which captures more information than open–close returns.",
            "components": {
              "price_range": {
                "definition": "Range_t = High_t − Low_t",
                "role": "Serves as a more efficient volatility proxy than squared returns.",
                "effect": "Reduces measurement noise and enhances robustness against price jumps."
              },
              "garch_volatility_process": {
                "formula": "σ²_t = ω + α · Range²_{t−1} + β · σ²_{t−1}",
                "role": "Use range-squared as volatility shock instead of squared returns.",
                "effect": "Improves accuracy in calm and volatile markets."
              },
              "robust_estimation_method": {
                "method": "Robust M-estimator or loss-modified quasi-likelihood",
                "role": "Prevent extreme returns or microstructure noise from distorting parameter estimates.",
                "effect": "Stabilizes volatility forecasts under extreme conditions."
              }
            },
            "procedure": [
              "Collect high–low daily price data for the asset.",
              "Compute daily range and use it as the input series.",
              "Estimate GARCH parameters using robust estimation.",
              "Generate volatility forecasts from the estimated model.",
              "Compare forecasts with returns-based GARCH and other baselines."
            ],
            "output": "Range-based volatility estimates more robust to market noise and jumps.",
            "advantages": [
              "Range data provides more efficient volatility signal than returns.",
              "Robust estimation reduces sensitivity to outliers.",
              "Improves both in-sample and out-of-sample forecasting.",
              "Better suited for assets with large intraday fluctuations."
            ]
          },
    
          "Robust_Estimation_Framework": {
            "name": "Robust Estimation Framework for GARCH Parameters",
            "purpose": "Enhance stability and accuracy of GARCH parameter estimation under heavy-tailed returns, jumps, or noise.",
            "components": {
              "m_estimator": {
                "definition": "Minimizes weighted loss penalizing extreme residuals.",
                "role": "Limits influence of large, abnormal innovations."
              },
              "quasi_likelihood_modification": {
                "definition": "Adjust likelihood function to reduce sensitivity to extreme observations.",
                "role": "Ensure parameter estimates remain stable under fat-tailed distributions."
              }
            },
            "procedure": [
              "Define robust objective function for estimation.",
              "Apply optimization to estimate (ω, α, β).",
              "Evaluate residual behavior and volatility persistence.",
              "Use estimates for forecasting future volatility."
            ],
            "advantages": [
              "Less sensitive to heavy tails and price jumps.",
              "More reliable parameter estimates.",
              "Improves real-world applicability of GARCH models."
            ]
          },
    
          "Standard_GARCH_Comparison": {
            "name": "Comparison with Standard GARCH(1,1)",
            "purpose": "Benchmark range-based GARCH against classical returns-based GARCH.",
            "procedure": [
              "Fit returns-based GARCH(1,1) on the same dataset.",
              "Compute volatility forecasts.",
              "Compare out-of-sample forecast accuracy.",
              "Evaluate robustness under return shocks."
            ],
            "findings": [
              "Range-based GARCH outperforms standard GARCH in accuracy.",
              "Parameter estimates are more stable across regimes.",
              "Superior handling of extreme movements."
            ]
          },
    
          "Volatility_Forecast_Evaluation": {
            "name": "Volatility Forecast Evaluation and Backtesting",
            "purpose": "Assess forecast quality using statistical and economic metrics.",
            "metrics": {
              "forecast_error": [
                "MSE",
                "RMSE",
                "MAE"
              ],
              "robustness_checks": [
                "Outlier resistance",
                "Jump sensitivity",
                "Residual autocorrelation tests"
              ],
              "statistical_tests": [
                "ARCH-LM test",
                "Ljung–Box test"
              ]
            },
            "role_in_paper": "Demonstrates superiority of robust range-based GARCH over classical GARCH models."
          }
        }
      }
    },
    
    {
      "id": 13,
      "title": "Quantum Monte Carlo Integration for Simulation-Based Optimization",
      "authors": null,
      "year": 2024,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/2410.03926",
      "performance-metrics": [
        "Integration Error Reduction",
        "Quantum Speedup Factor (vs Classical MC)",
        "Estimator Variance",
        "Bias of Quantum Monte Carlo Estimator",
        "Amplitude Estimation Error",
        "Query Complexity Reduction",
        "Convergence Rate Under Noise",
        "Runtime vs Accuracy Tradeoff",
        "Success Probability of Quantum Integrator",
        "Stability Across Different Integrand Distributions"
      ],
      "tools-used-in": [
        "Quantum-Amplitude-Estimation-(QAE)-for-CVaR"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Monte_Carlo_Integration": {
            "name": "Quantum Monte Carlo Integration (QMCI)",
            "purpose": "Provide a quadratic speedup for estimating integrals used in simulation-based optimization via quantum amplitude estimation.",
            "components": {
              "integrand_encoding": {
                "definition": "Encode f(x) values into amplitudes or phase rotations of quantum states.",
                "role": "Transforms numerical integration into an amplitude estimation problem.",
                "effect": "Allows quantum methods to estimate integrals faster than classical Monte Carlo."
              },
              "sampling_distribution": {
                "definition": "Probability distribution p(x) from which samples are drawn on a quantum device.",
                "role": "Defines how integrand information is embedded in the quantum circuit.",
                "effect": "Ensures unbiased integral estimation when combined with amplitude estimation."
              },
              "quantum_integral_estimator": {
                "definition": "I = E[f(X)] computed via amplitude estimation.",
                "role": "Central estimator for simulation-based optimization tasks.",
                "effect": "Quadratic speedup: O(1/ε) instead of classical O(1/ε²)."
              }
            },
            "procedure": [
              "Define integral I = ∫ f(x) p(x) dx required for optimization.",
              "Encode f(x) and sampling distribution p(x) into quantum oracles.",
              "Prepare quantum state representing weighted integrand values.",
              "Apply amplitude estimation to compute expected value of f(x).",
              "Use estimated integral inside optimization objective.",
              "Repeat until convergence of simulation-based optimizer."
            ],
            "output": "Fast quantum estimate of integrals required for optimization algorithms.",
            "advantages": [
              "Quadratic reduction in sample complexity vs classical Monte Carlo.",
              "Efficient for high-dimensional integrals where classical MC is slow.",
              "Enables quantum enhancement of simulation-driven optimization pipelines."
            ]
          },
    
          "Quantum_Amplitude_Estimation": {
            "name": "Quantum Amplitude Estimation (QAE)",
            "purpose": "Estimate expectation values (integrals) with superior scaling vs classical Monte Carlo.",
            "components": {
              "oracle_operator": {
                "definition": "O_f prepares amplitude encoding of f(x).",
                "role": "Maps integrand values to measurable amplitudes."
              },
              "grover_style_operator": {
                "definition": "Q = -A S₀ A⁻¹ S_ψ used to amplify relevant amplitudes.",
                "role": "Enables extraction of amplitude with fewer queries."
              }
            },
            "procedure": [
              "Prepare initial quantum state A|0⟩ encoding f(x).",
              "Apply Grover-like amplification operator repeatedly.",
              "Perform inverse quantum Fourier transform to estimate amplitude.",
              "Convert amplitude to integral value.",
              "Feed integral estimate into simulation-based optimization loop."
            ],
            "advantages": [
              "Achieves error scaling O(1/ε) instead of O(1/ε²).",
              "Improves performance on integrals with heavy-tailed distributions.",
              "Handles rare-event integrals more efficiently."
            ],
            "limitations": [
              "Noise-sensitive — requires low-error quantum gates.",
              "State preparation cost depends on complexity of integrand."
            ]
          },
    
          "Simulation_Based_Optimization": {
            "name": "Simulation-Based Optimization with Quantum Integration",
            "purpose": "Use quantum Monte Carlo integration inside classical-quantum hybrid optimization loops.",
            "steps": [
              "Define objective function J(θ) = ∫ f(x, θ) p(x) dx.",
              "Use QMCI + QAE to compute integral efficiently.",
              "Evaluate gradient or subgradient of J(θ) if needed.",
              "Update θ using classical optimization methods.",
              "Iterate until convergence."
            ],
            "advantages": [
              "Significantly accelerates simulation-heavy optimization problems.",
              "Reduces computational burden in finance, physics, and engineering.",
              "Allows optimization over high-dimensional stochastic landscapes."
            ]
          },
    
          "State_Preparation_Module": {
            "name": "State Preparation for Integrand Encoding",
            "purpose": "Prepare a quantum state representing function values or probability weights.",
            "procedure": [
              "Normalize integrand values or probability densities.",
              "Embed normalized values into amplitudes or phases.",
              "Construct unitary A that maps |0⟩ to the encoded integrand state.",
              "Use A and A⁻¹ in amplitude estimation."
            ],
            "properties": [
              "Central component of QMCI performance.",
              "Accuracy of state encoding directly affects estimator variance."
            ]
          },
    
          "Classical_Baseline_Comparison": {
            "name": "Comparison with Classical Monte Carlo Integration",
            "purpose": "Benchmark quantum integration speed and accuracy.",
            "baseline_methods": [
              "Standard Monte Carlo",
              "Quasi-Monte Carlo",
              "Importance Sampling Monte Carlo"
            ],
            "role_in_paper": "Demonstrates that quantum integration achieves lower error with fewer samples."
          }
        }
      }
    },
    
    {
      "id": 14,
      "title": "Hybrid quantum–classical Monte Carlo VaR",
      "authors": "Zhang et al.",
      "year": 2023,
      "source": "IEEE Trans. Quantum Engineering",
      "url": "https://ieeexplore.ieee.org/document/10090514",
      "performance-metrics": [
        "VaR Estimation Error",
        "CVaR Estimation Error",
        "Variance Reduction Compared to Classical MC",
        "Hybrid Speedup Ratio",
        "Sampling Efficiency",
        "Tail Probability Estimation Accuracy",
        "Convergence Rate",
        "Quantum Resource Usage (shots, depth)",
        "Noise Robustness of Estimates",
        "Bias-Variance Tradeoff in Hybrid Estimator"
      ],
      "tools-used-in": [
        "Monte-Carlo-Simulation-for-VAR-CVAR"
      ],
      "paper-details": {
        "algorithms": {
          "Hybrid_Quantum_Classical_MC_VaR": {
            "name": "Hybrid Quantum–Classical Monte Carlo VaR",
            "purpose": "Improve Value-at-Risk estimation by combining classical Monte Carlo sampling with quantum subroutines for probability estimation and variance reduction.",
            "components": {
              "quantum_probability_estimator": {
                "definition": "Quantum subroutine estimates probability of tail events (loss ≥ threshold).",
                "role": "Reduces sample complexity required for accurate VaR estimation.",
                "effect": "Provides more stable tail probability estimation vs classical MC."
              },
              "classical_monte_carlo": {
                "definition": "Generate asset price paths using classical simulation.",
                "role": "Produce loss samples for hybrid VaR computation.",
                "effect": "Leverages efficient classical simulation for large scenarios."
              },
              "hybrid_loss_distribution": {
                "definition": "Loss distribution generated via classical MC but corrected using quantum-estimated tail probability.",
                "role": "Combines strengths of classical and quantum estimation.",
                "effect": "Results in lower error for VaR compared to classical MC alone."
              }
            },
            "procedure": [
              "Simulate price paths using classical Monte Carlo.",
              "Define loss threshold L* for VaR (e.g., 95% or 99%).",
              "Encode tail event indicator (loss ≥ L*) in quantum oracle.",
              "Use quantum probability estimation to compute tail event probability.",
              "Combine quantum-estimated tail probability with classical loss samples.",
              "Compute VaR as quantile corresponding to corrected tail probability.",
              "Evaluate estimation error and runtime improvements."
            ],
            "output": "Hybrid VaR estimate with reduced sample complexity.",
            "advantages": [
              "Improves VaR accuracy without requiring full quantum simulation.",
              "Quadratic speedup in probability estimation (via amplitude-based methods).",
              "More robust tail estimation under heavy-tailed loss distributions.",
              "Practical for near-term devices since quantum circuits remain shallow."
            ]
          },
    
          "Quantum_Tail_Probability_Estimation": {
            "name": "Quantum Tail Probability Estimator",
            "purpose": "Estimate probability of losses exceeding a VaR threshold using quantum circuits.",
            "components": {
              "indicator_encoding": {
                "definition": "Mark bit when loss exceeds threshold: I(loss ≥ L*).",
                "role": "Transforms tail event detection into an amplitude estimation problem."
              },
              "amplitude_estimation_kernel": {
                "definition": "Quantum operator to extract probability amplitude associated with tail events.",
                "role": "Achieves quadratic speedup vs classical probability estimation."
              }
            },
            "procedure": [
              "Encode classical loss sample distribution into quantum registers.",
              "Construct oracle that marks tail-loss events.",
              "Apply amplitude estimation (or hybrid AE variants).",
              "Return probability P(loss ≥ L*)."
            ],
            "advantages": [
              "Reduces required number of classical Monte Carlo samples.",
              "Accurate even for rare extreme events (small α)."
            ],
            "limitations": [
              "Sensitive to quantum noise.",
              "State preparation overhead depends on loss distribution model."
            ]
          },
    
          "MonteCarlo_Loss_Simulation": {
            "name": "Classical Monte Carlo Loss Simulation",
            "purpose": "Generate asset paths and corresponding losses for hybrid VaR computation.",
            "steps": [
              "Simulate asset price paths using stochastic model (e.g., GBM or GARCH).",
              "Compute terminal portfolio values and losses.",
              "Construct empirical loss distribution.",
              "Feed losses into quantum tail event oracle."
            ],
            "advantages": [
              "Efficient for large-scale scenario generation.",
              "Provides realistic loss distribution inputs for quantum correction."
            ]
          },
    
          "VaR_Backtesting_Framework": {
            "name": "VaR Backtesting and Evaluation",
            "purpose": "Assess quality of hybrid VaR predictions.",
            "tests": {
              "kupiec_test": "Checks unconditional exceedance frequency.",
              "christoffersen_test": "Checks independence/clustering of exceedances.",
              "coverage_ratio": "Compares predicted VaR with realized losses."
            },
            "procedure": [
              "Compute hybrid VaR estimates for test period.",
              "Record exceptions (loss > VaR).",
              "Apply backtesting tests.",
              "Compare against classical MC VaR baseline."
            ],
            "role_in_paper": "Demonstrates improved accuracy and reduced sample complexity of hybrid approach."
          },
    
          "Classical_Distribution_Correction": {
            "name": "Quantum-Corrected Loss Distribution Adjustment",
            "purpose": "Use quantum-estimated tail probability to refine classical empirical distribution.",
            "procedure": [
              "Compute empirical tail probability from classical MC.",
              "Replace or adjust tail probability using quantum AE estimate.",
              "Recompute tail quantile corresponding to VaR.",
              "Optionally compute CVaR using corrected tail density."
            ],
            "advantages": [
              "Aligns classical loss distribution with more accurate quantum tail estimates.",
              "Improves performance without requiring full quantum Monte Carlo."
            ]
          }
        }
      }
    },
    
    {
      "id": 15,
      "title": "Efficient Estimation in Extreme Value Regression Models",
      "authors": "Hambuckers et al.",
      "year": 2023,
      "source": null,
      "url": "https://arxiv.org/abs/2304.06950",
      "performance-metrics": [
        "Tail Index Estimation Error (ξ estimation accuracy)",
        "Scale Parameter Estimation Error (β estimation accuracy)",
        "Mean Squared Error of Parameter Estimates",
        "Variance Efficiency Relative to MLE",
        "Asymptotic Bias of GPD Parameter Estimates",
        "Quantile Forecast Accuracy (Extreme Quantiles)",
        "VaR and CVaR Tail Prediction Error",
        "Goodness-of-Fit for Extreme Value Regression",
        "Likelihood Convergence Stability",
        "Robustness to Model Misspecification"
      ],
      "tools-used-in": [
        "Extreme-Value-Theory-(EVT)–Peaks-Over-Threshold"
      ],
      "paper-details": {
        "algorithms": {
          "Extreme_Value_Regression_Model": {
            "name": "Generalized Extreme Value (GEV) / Generalized Pareto Regression Framework",
            "purpose": "Estimate extreme quantiles and tail parameters under a regression model where distribution parameters vary with covariates.",
            "components": {
              "gev_distribution": {
                "formula": "GEV(z | μ, σ, ξ)",
                "role": "Model block maxima or extreme outcomes with location, scale, and tail index dependent on covariates."
              },
              "gpd_distribution": {
                "formula": "GPD(y | β, ξ) = 1 − (1 + ξy/β)^(-1/ξ)",
                "role": "Model exceedances above a threshold using regression-driven parameters."
              },
              "covariate_effects": {
                "definition": "Links regression covariates to tail parameters (μ, σ, ξ).",
                "role": "Capture how economic/market conditions affect tail heaviness.",
                "examples": [
                  "μ = xᵀβμ",
                  "log(σ) = xᵀβσ",
                  "ξ = xᵀβξ (or constrained link functions)"
                ]
              }
            },
            "procedure": [
              "Choose appropriate EVT model type (GEV for maxima, GPD for exceedances).",
              "Collect covariates that potentially influence tail behavior.",
              "Fit regression model linking parameters to covariates.",
              "Estimate parameters using efficient estimators proposed in paper.",
              "Compute extreme quantiles, VaR, CVaR, or return levels.",
              "Evaluate model using goodness-of-fit and prediction tests."
            ],
            "output": "Regression-conditional estimates of extreme quantiles and tail indices.",
            "advantages": [
              "Captures covariate dependence in extreme events.",
              "More accurate than static EVT models under varying conditions.",
              "Enables forecasting of extreme market risk under changing regimes."
            ]
          },
    
          "Efficient_Estimation_Method": {
            "name": "Efficient Estimation Method for Extreme Value Regression",
            "purpose": "Improve parameter estimation efficiency by reducing variance and bias relative to classical MLE.",
            "components": {
              "score_matching_or_adjusted_likelihood": {
                "definition": "Modified likelihood or estimating equations to reduce sensitivity to small sample effects.",
                "role": "Improves stability of tail parameter estimation, especially for ξ."
              },
              "efficient_influence_functions": {
                "definition": "Derived to minimize asymptotic variance of parameter estimators.",
                "role": "Ensures optimality under regularity conditions."
              },
              "robust_parameterization": {
                "definition": "Constraining ξ or using link functions to ensure stable estimation.",
                "role": "Prevents divergence when ξ ≈ 0 (Gumbel-like behavior)."
              }
            },
            "procedure": [
              "Define estimating equations for GPD/GEV parameters.",
              "Incorporate covariate structure to form regression model.",
              "Construct efficient influence functions to reduce estimator variance.",
              "Optimize parameters using iterative methods (Newton–Raphson or gradient-based).",
              "Compute standard errors and asymptotic variance estimates."
            ],
            "advantages": [
              "Lower variance vs classical MLE.",
              "Higher numerical stability in tail estimation.",
              "Better performance for small samples and heavy-tailed distributions."
            ]
          },
    
          "Peaks_Over_Threshold_Approach": {
            "name": "Peaks-Over-Threshold Extreme Value Theory (POT-EVT)",
            "purpose": "Use exceedances above a threshold to estimate tail behavior more efficiently than block maxima methods.",
            "components": {
              "threshold_selection": {
                "methods": [
                  "Mean Residual Life Plot",
                  "Parameter Stability Plot",
                  "Quantile-based threshold"
                ],
                "role": "Ensure enough exceedances while reducing bias."
              },
              "exceedance_extraction": {
                "definition": "yi = Xi − u for Xi > u",
                "role": "Generate excess values to fit GPD regression."
              }
            },
            "procedure": [
              "Select dynamic or static threshold u.",
              "Extract exceedances yi.",
              "Fit GPD regression model using efficient estimator.",
              "Predict extreme quantiles and tail probabilities."
            ],
            "advantages": [
              "More data-efficient than block maxima.",
              "Better modeling of extreme events.",
              "Naturally integrates with regression-based tail analysis."
            ]
          },
    
          "Extreme_Risk_Metrics": {
            "name": "Extreme Risk Metrics Estimation",
            "purpose": "Compute financial risk measures from regression-based EVT models.",
            "metrics": {
              "var_alpha": "VaR = u + (β/ξ) [(α^{-ξ}) − 1]",
              "cvar_alpha": "CVaR derived from expected tail losses under GPD.",
              "return_levels": "z_T = μ + (β/ξ)[(−log(1−1/T))^(−ξ) − 1]"
            },
            "procedure": [
              "Use fitted parameters (β, ξ, covariates) to compute tail quantiles.",
              "Adjust quantiles dynamically depending on regression covariates.",
              "Evaluate accuracy via backtesting and prediction error analysis."
            ],
            "advantages": [
              "Produces risk metrics conditioned on covariates such as volatility, liquidity, or macro factors.",
              "More adaptive than fixed-parameter EVT models."
            ]
          },
    
          "Model_Evaluation_Framework": {
            "name": "Model Evaluation and Goodness-of-Fit",
            "purpose": "Assess performance of regression-based EVT and estimation efficiency.",
            "methods": {
              "likelihood_ratio_tests": "Compare efficient estimator vs classical MLE.",
              "quantile_backtesting": "Check VaR exceedances for extreme quantiles.",
              "tail_fit_diagnostics": [
                "QQ-plots",
                "Tail index stability plots",
                "Residual diagnostics"
              ]
            },
            "role_in_paper": "Shows that efficient estimators outperform classical MLE in bias, variance, and predictive accuracy."
          }
        }
      }
    },
    
    {
      "id": 16,
      "title": "Threshold selection and tail risk estimation",
      "authors": "Benito et al.",
      "year": 2023,
      "source": null,
      "url": null,
      "performance-metrics": [
        "Threshold Selection Accuracy",
        "Bias–Variance Tradeoff in Tail Estimates",
        "GPD Parameter Estimation Error (ξ, β)",
        "VaR Tail Quantile Estimation Error",
        "CVaR Tail Severity Estimation Error",
        "Exceedance Frequency Prediction Error",
        "Goodness-of-Fit of GPD Tail Model",
        "Stability of Tail Index Across Threshold Choices",
        "Out-of-Sample Tail Forecast Accuracy",
        "Coverage Probability for Extreme Quantiles"
      ],
      "tools-used-in": [
        "Extreme-Value-Theory-(EVT)–Peaks-Over-Threshold"
      ],
      "paper-details": {
        "algorithms": {
          "Threshold_Selection_Methods": {
            "name": "Dynamic and Statistical Threshold Selection Methods",
            "purpose": "Select an optimal threshold for Peaks-Over-Threshold (POT) modeling that balances bias and variance in extreme tail estimation.",
            "components": {
              "quantile_based_thresholds": {
                "definition": "Choose threshold u as a high quantile (e.g., 95%, 97.5%).",
                "role": "Ensures sufficient number of exceedances while modeling only the extreme tail.",
                "effect": "Useful for heavy-tailed financial returns."
              },
              "mean_residual_life_plot": {
                "definition": "Plot mean excess vs threshold to find linear region.",
                "role": "Identify where GPD assumptions start to hold.",
                "effect": "Guides selection of stable tail modeling range."
              },
              "parameter_stability_plot": {
                "definition": "Observe ξ, β estimates as threshold u varies.",
                "role": "Stable parameter region indicates good threshold choice.",
                "effect": "Prevents bias from too-low thresholds."
              }
            },
            "procedure": [
              "Analyze empirical tail distribution.",
              "Compute exceedances for candidate thresholds.",
              "Evaluate parameter stability and mean residual life.",
              "Choose threshold maximizing model stability and minimizing bias.",
              "Fit GPD to exceedances using final threshold."
            ],
            "output": "Statistically optimal threshold for EVT tail modeling.",
            "advantages": [
              "Improves accuracy of extreme risk forecasts.",
              "Reduces sensitivity to noise and sample size.",
              "Provides systematic method instead of arbitrary threshold choice."
            ]
          },
    
          "Peaks_Over_Threshold_Model": {
            "name": "Peaks-Over-Threshold (POT) EVT Tail Model",
            "purpose": "Estimate extreme tail risk using Generalized Pareto Distribution fitted to exceedances above an optimal threshold.",
            "components": {
              "generalized_pareto_distribution": {
                "formula": "GPD(y | ξ, β) = 1 − (1 + ξy/β)^(-1/ξ)",
                "parameters": [
                  "ξ — tail index (controls heaviness)",
                  "β — scale parameter"
                ],
                "role": "Model severity and probability of extreme losses."
              },
              "exceedance_process": {
                "definition": "y_i = X_i − u for X_i > u",
                "role": "Creates data suitable for fitting the GPD tail."
              }
            },
            "procedure": [
              "Apply chosen threshold u.",
              "Extract exceedances above u.",
              "Fit GPD using MLE or robust estimation.",
              "Compute tail probabilities and extreme quantiles.",
              "Validate using diagnostics (stability plots, QQ-plots)."
            ],
            "output": "Tail estimates for VaR, CVaR, return levels, and exceedance probabilities.",
            "advantages": [
              "Flexible and statistically grounded for extreme values.",
              "More data-efficient than block-maxima EVT.",
              "Effective for financial heavy-tail modeling."
            ]
          },
    
          "Tail_Risk_Estimation": {
            "name": "Tail Risk Estimation via EVT",
            "purpose": "Compute extreme financial risk measures using GPD tail model and optimized threshold selection.",
            "metrics": {
              "var_formula": "VaRα = u + (β/ξ)[(α^{-ξ}) − 1]",
              "cvar_formula": "CVaRα = VaRα + (β − ξ u)/(1 − ξ)"
            },
            "procedure": [
              "Use fitted GPD to obtain tail quantile estimates.",
              "Calculate VaR at high confidence levels (e.g., 99%, 99.5%).",
              "Compute CVaR as expected tail loss beyond VaR.",
              "Evaluate estimation uncertainty using bootstrap or asymptotic theory."
            ],
            "advantages": [
              "Adapts to changing market regimes through threshold refinement.",
              "More accurate than Gaussian or GARCH-based risk measures for extreme losses."
            ]
          },
    
          "Diagnostic_and_Evaluation_Framework": {
            "name": "EVT Diagnostic and Model Evaluation Framework",
            "purpose": "Assess whether chosen threshold and GPD fit are statistically valid.",
            "methods": {
              "goodness_of_fit": [
                "QQ-plots",
                "Kolmogorov–Smirnov test",
                "Likelihood-based diagnostics"
              ],
              "stability_criteria": [
                "Tail parameter stability across thresholds",
                "Residual analysis",
                "Mean excess stability"
              ],
              "backtesting_metrics": [
                "Extreme VaR exceedance frequency",
                "CVaR severity error",
                "Coverage probability checks"
              ]
            },
            "role_in_paper": "Ensures that chosen threshold achieves optimal EVT performance while minimizing model bias."
          }
        }
      }
    },
    
    {
      "id": 17,
      "title": "Monte Carlo and copula-based VaR in turbulent markets",
      "authors": "Aziz & Nadarajah",
      "year": 2023,
      "source": "SpringerLink",
      "url": "https://link.springer.com/article/10.1007/s10479-023-05176-4",
      "performance-metrics": [
        "VaR Estimation Error",
        "CVaR Estimation Error",
        "Tail Dependence Accuracy",
        "Copula Fit Quality",
        "Monte Carlo Convergence Rate",
        "Extreme Quantile Forecast Accuracy",
        "Portfolio Loss Distribution Fit Error",
        "Backtesting Failure Rate (Kupiec Test)",
        "Independence Test for Exceedances (Christoffersen Test)",
        "Stability Under Turbulent Market Regimes"
      ],
      "tools-used-in": [
        "Monte-Carlo-Simulation-for-VAR-CVAR"
      ],
      "paper-details": {
        "algorithms": {
          "Copula_Based_VaR_Model": {
            "name": "Copula-Based VaR Model",
            "purpose": "Estimate portfolio Value-at-Risk during turbulent markets by capturing nonlinear and tail dependencies using copula-based joint distribution modeling.",
            "components": {
              "marginal_distribution_modeling": {
                "definition": "Fit distributions to each asset’s returns (e.g., t-distribution, skewed distributions).",
                "role": "Ensures accurate individual asset behavior before constructing joint distribution."
              },
              "copula_function": {
                "definition": "C(u₁, …, uₙ | θ) links marginal distributions into joint dependence structure.",
                "types": [
                  "Gaussian Copula",
                  "t-Copula",
                  "Clayton Copula",
                  "Gumbel Copula"
                ],
                "role": "Captures nonlinear dependence and tail interactions between assets."
              },
              "tail_dependence": {
                "definition": "Probability of simultaneous extreme losses across assets.",
                "role": "Critical for VaR accuracy in turbulent markets."
              }
            },
            "procedure": [
              "Fit marginal return distributions for each asset.",
              "Transform returns into uniform space via CDFs.",
              "Choose and calibrate copula model to capture dependence.",
              "Simulate multivariate returns via copula-based sampling.",
              "Construct portfolio loss distribution.",
              "Compute VaR as tail quantile of simulated losses.",
              "Backtest results using exceedance-based tests."
            ],
            "output": "VaR estimates incorporating nonlinear dependence and tail co-movement.",
            "advantages": [
              "More accurate than Gaussian assumptions during crises.",
              "Captures asymmetric tail behavior.",
              "Suitable for portfolios with strong contagion effects."
            ]
          },
    
          "MonteCarlo_VaR": {
            "name": "Monte Carlo VaR Simulation",
            "purpose": "Simulate large numbers of portfolio return scenarios to compute empirical loss distribution and VaR/CVaR.",
            "components": {
              "sampling_engine": {
                "definition": "Simulates joint asset returns from copula-based or marginal-based models.",
                "role": "Generate scenarios for portfolio loss distribution."
              },
              "loss_calculation": {
                "definition": "Compute portfolio loss L = wᵀR for each scenario.",
                "role": "Form empirical distribution of losses."
              }
            },
            "procedure": [
              "Simulate thousands to millions of return paths.",
              "Compute portfolio losses for each sample.",
              "Construct empirical CDF of losses.",
              "Extract VaR and CVaR from empirical distribution.",
              "Validate predictions with backtesting tests."
            ],
            "advantages": [
              "Flexible and model-agnostic.",
              "Captures nonlinearities and complex loss shapes.",
              "Accurate when sufficient samples are available."
            ],
            "limitations": [
              "Slow convergence (O(1/√N)).",
              "Fails under extreme market turbulence unless dependence is modeled properly."
            ]
          },
    
          "Copula_Selection_and_Estimation": {
            "name": "Copula Selection & Tail Dependence Estimation",
            "purpose": "Select the copula that best captures market dependence, especially during crises.",
            "methods": {
              "AIC/BIC_scores": "Used to compare copula models.",
              "Kendall_tau_matching": "Used to estimate copula dependence parameters.",
              "Tail_dependence_coefficients": "Measure joint extreme behavior."
            },
            "procedure": [
              "Evaluate candidate copulas (Gaussian, t, Clayton, Gumbel, etc.).",
              "Estimate dependence parameters using rank-based statistics.",
              "Perform goodness-of-fit tests.",
              "Select copula with best tail dependence and fit score."
            ],
            "advantages": [
              "Accurate modeling of extreme co-movements.",
              "Improves VaR estimation under stress events."
            ]
          },
    
          "Tail_Risk_Estimation": {
            "name": "Tail Risk Estimation using Copula + Monte Carlo",
            "purpose": "Compute VaR and CVaR incorporating tail dependence structure.",
            "metrics": {
              "var": "Quantile of simulated loss distribution.",
              "cvar": "Mean loss exceeding VaR threshold.",
              "tail_dependence": "Probability of joint extreme losses."
            },
            "procedure": [
              "Simulate multivariate return scenarios.",
              "Compute loss quantiles and expected tail loss.",
              "Assess sensitivity to copula choice and market regime."
            ],
            "advantages": [
              "More reliable risk estimates during crises.",
              "Addresses underestimation problems of Gaussian VaR."
            ]
          },
    
          "Backtesting_Framework": {
            "name": "Backtesting Framework for Copula-Based VaR",
            "purpose": "Evaluate accuracy and statistical validity of VaR predictions.",
            "tests": {
              "kupiec_test": "Unconditional coverage test.",
              "christoffersen_test": "Independence and conditional coverage.",
              "violation_ratio": "Actual exceedances / expected exceedances."
            },
            "procedure": [
              "Compute VaRs for evaluation period.",
              "Record exceedances (loss > VaR).",
              "Apply regulatory backtesting tests.",
              "Compare copula-based VaR vs simple Monte Carlo VaR."
            ],
            "role_in_paper": "Demonstrates superiority of copula-based VaR during turbulent markets."
          }
        }
      }
    },
    
    {
      "id": 18,
      "title": "Real quantum amplitude estimation",
      "authors": null,
      "year": 2023,
      "source": "EPJ Quantum Technology",
      "url": "https://epjquantumtechnology.springeropen.com/articles/10.1140/epjqt/s40507-023-00159-0",
      "performance-metrics": [
        "Amplitude Estimation Error",
        "Quantum Query Complexity",
        "Bias in Real-QAE Estimator",
        "Variance of Expectation Estimation",
        "Convergence Rate vs Traditional QAE",
        "Noise Robustness of Real-QAE",
        "Success Probability of Amplitude Extraction",
        "Shot Efficiency (Fewer Measurements Required)",
        "Confidence Interval Width",
        "Stability Across Different Amplitude Ranges"
      ],
      "tools-used-in": [
        "Quantum-Amplitude-Estimation-(QAE)-for-CVaR"
      ],
      "paper-details": {
        "algorithms": {
          "Real_Quantum_Amplitude_Estimation": {
            "name": "Real Quantum Amplitude Estimation (Real-QAE)",
            "purpose": "Provide a more practical, noise-resilient amplitude estimation method that avoids Quantum Phase Estimation (QPE), enabling efficient expectation estimation for NISQ devices.",
            "components": {
              "real_amplitude_encoding": {
                "definition": "Encode amplitude a ∈ [0,1] directly into measurement probabilities without phase estimation.",
                "role": "Avoids complex QFT/QPE circuits.",
                "effect": "Makes amplitude estimation feasible on near-term (NISQ) devices."
              },
              "linear_combination_of_unitaries": {
                "definition": "Construct estimators via repeated state preparations and linear combinations.",
                "role": "Reduces circuit depth while retaining precision.",
                "effect": "Improves estimation stability under noise."
              },
              "iterative_sampling_scheme": {
                "definition": "Use repeated sampling and polynomial-based reconstruction of amplitude.",
                "role": "Achieves estimator convergence with fewer quantum resources.",
                "effect": "Balances bias–variance tradeoff without requiring deep circuits."
              }
            },
            "procedure": [
              "Prepare quantum state encoding amplitude a in computational basis.",
              "Apply series of controlled unitaries or rotations enhancing amplitude visibility.",
              "Collect measurement samples for different circuit depths or scaling parameters.",
              "Fit estimator polynomial or apply likelihood-based reconstruction.",
              "Output amplitude estimate ā with bounded error.",
              "Use ā to compute expected values or risk metrics (e.g., CVaR)."
            ],
            "output": "Accurate amplitude estimate suited for NISQ hardware.",
            "advantages": [
              "Fully avoids phase estimation and QFT.",
              "Robust to quantum noise and gate imperfections.",
              "Requires fewer shots than classical Monte Carlo.",
              "Compatible with hybrid algorithms (e.g., CVaR estimation, quantum finance)."
            ]
          },
    
          "Comparison_with_Traditional_QAE": {
            "name": "Comparison: Real-QAE vs Standard QAE",
            "purpose": "Compare resource scaling, accuracy, and noise robustness between Real-QAE and canonical amplitude estimation.",
            "metrics": {
              "speedup": "Real-QAE achieves O(1/ε) scaling similar to QAE but with simpler circuits.",
              "robustness": "Resistant to decoherence due to absence of QPE.",
              "precision": "Improved bias properties over sampling-based estimators (MLAE)."
            },
            "procedure": [
              "Apply both estimators to known amplitudes.",
              "Evaluate estimation error distribution.",
              "Measure resource usage (depth, shots, qubits).",
              "Analyze performance under simulated quantum noise."
            ],
            "advantages": [
              "Much shallower circuits.",
              "Better success probability under realistic noise.",
              "More practical for financial risk estimation and expectation problems."
            ]
          },
    
          "Amplitude_Encoded_Expectation_Estimation": {
            "name": "Expectation Estimation using Real-QAE",
            "purpose": "Estimate E[f(X)] by encoding f(X) into amplitudes and applying Real-QAE.",
            "steps": [
              "Prepare amplitude encoding where probability of |1⟩ corresponds to expectation of interest.",
              "Apply Real-QAE estimator to extract amplitude.",
              "Convert amplitude to expected value.",
              "Optionally use for downstream CVaR, VaR, and pricing computations."
            ],
            "advantages": [
              "Enables quantum-enhanced Monte Carlo for risk and derivative pricing.",
              "Suitable for integration with hybrid algorithms (e.g., QMC–QAE hybrids)."
            ]
          },
    
          "Noise_Model_and_Robustness": {
            "name": "Noise Robustness and Error Mitigation for Real-QAE",
            "purpose": "Analyze and mitigate noise effects on amplitude estimation.",
            "methods": {
              "shallow_circuit_design": "Minimizes noise exposure by avoiding QPE.",
              "bootstrap_resampling": "Improves estimate stability.",
              "regularized_estimation": "Reduces variance-induced divergence."
            },
            "role_in_paper": "Shows that Real-QAE outperforms QPE-based QAE on noisy quantum hardware."
          }
        }
      }
    },
    
    {
      "id": 19,
      "title": "Q-LSTM for financial time series forecasting",
      "authors": "Zlokapa et al.",
      "year": 2022,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/2206.02426",
      "performance-metrics": [
        "Mean Squared Error (MSE)",
        "Root Mean Squared Error (RMSE)",
        "Mean Absolute Error (MAE)",
        "Directional Accuracy (DA)",
        "Forecast Bias",
        "Volatility Forecast Error",
        "Training Stability",
        "Quantum Circuit Fidelity",
        "Hybrid Model Convergence Rate",
        "Memory Retention Score in Recurrent Dynamics"
      ],
      "tools-used-in": [
        "Quantum-LSTM-(Q-LSTM)"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_LSTM_Model": {
            "name": "Quantum Long Short-Term Memory (Q-LSTM)",
            "purpose": "Extend the classical LSTM architecture by encoding gating operations into quantum circuits, enabling quantum-enhanced sequence modeling for financial time series.",
            "components": {
              "quantum_gate_encoding": {
                "definition": "Represent LSTM gates via variational quantum circuits (VQCs).",
                "role": "Replace some or all classical affine transformations with quantum operations.",
                "effect": "Allows the model to learn complex nonlinear transformations in high-dimensional Hilbert space."
              },
              "quantum_hidden_state_update": {
                "definition": "h_t = f(VQC(x_t, h_{t-1}))",
                "role": "Quantum circuit outputs act as gating signals.",
                "effect": "Provides richer expressive power than classical LSTMs."
              },
              "hybrid_classical_quantum_structure": {
                "definition": "Classical embedding + quantum recurrent layer + classical output layer.",
                "role": "Achieves feasibility on NISQ hardware.",
                "effect": "Reduces quantum resource requirements while improving performance."
              }
            },
            "procedure": [
              "Encode input time-series values into quantum states (angle/amplitude encoding).",
              "Pass encoded inputs through quantum variational circuits acting as gates.",
              "Update hidden and cell states using hybrid classical–quantum recurrence rules.",
              "Generate forecasts via classical output layers.",
              "Train end-to-end using gradient-based optimization and parameter shift rules."
            ],
            "output": "Quantum-enhanced hidden state representation for time-series forecasting.",
            "advantages": [
              "Higher expressive capacity due to quantum Hilbert space.",
              "May capture long-range dependencies more efficiently.",
              "Potential advantage in nonlinear and high-frequency financial time series.",
              "Hybrid design allows implementation on near-term quantum hardware."
            ]
          },
    
          "Input_Encoding_Module": {
            "name": "Quantum Input Encoding for Time Series",
            "purpose": "Transform raw financial time-series data into quantum state representations.",
            "methods": {
              "angle_encoding": "Map inputs x_t to rotation angles (e.g., RY(x_t)).",
              "amplitude_encoding": "Embed normalized feature vector into amplitudes of qubits."
            },
            "procedure": [
              "Normalize or scale input values.",
              "Select encoding appropriate for qubit count.",
              "Prepare quantum state each time step.",
              "Feed encoded state into quantum recurrent layer."
            ],
            "advantages": [
              "Preserves sequential structure.",
              "Allows embedding of multivariate financial features."
            ]
          },
    
          "Variational_Quantum_Gates_for_LSTM": {
            "name": "Variational Quantum Circuits for LSTM Gates",
            "purpose": "Quantum analogs of input, forget, and output gates.",
            "components": {
              "input_gate": {
                "definition": "VQC controlling how much new information enters the cell state.",
                "role": "Learns nonlinear gating behavior."
              },
              "forget_gate": {
                "definition": "VQC determining which past information to retain.",
                "role": "Key to memory control in Q-LSTM."
              },
              "output_gate": {
                "definition": "VQC generating hidden state output signals.",
                "role": "Determines time-series forecast contribution at each step."
              }
            },
            "procedure": [
              "Prepare quantum register representing concatenation of x_t and h_{t-1}.",
              "Apply parameterized rotations and entangling gates.",
              "Measure quantum circuits to produce gate outputs.",
              "Update LSTM cell and hidden state values."
            ],
            "advantages": [
              "Exploits entanglement for richer gating transformations.",
              "Potentially reduces number of classical parameters."
            ]
          },
    
          "Hybrid_Training_Framework": {
            "name": "Hybrid Classical–Quantum Training Procedure",
            "purpose": "Train Q-LSTM efficiently using gradient-based optimization.",
            "steps": [
              "Compute forward pass with quantum circuits and classical layers.",
              "Use parameter-shift rule to compute gradients for VQC parameters.",
              "Update quantum and classical weights jointly.",
              "Backpropagate through time (BPTT) for sequential dependencies.",
              "Monitor training loss and quantum circuit stability."
            ],
            "advantages": [
              "Compatible with NISQ hardware.",
              "Efficient gradient estimation.",
              "Allows hybrid scaling with classical layers taking heavy workload."
            ]
          },
    
          "Baseline_Comparison": {
            "name": "Comparison with Classical LSTM Models",
            "purpose": "Evaluate forecasting improvement due to quantum recurrence.",
            "metrics_compared": [
              "MSE/RMSE Forecast Accuracy",
              "Directional Accuracy",
              "Training Time",
              "Model Stability",
              "Generalization Across Time Horizons"
            ],
            "key_findings": [
              "Q-LSTM performs comparably or better than classical LSTM on certain financial datasets.",
              "Quantum gates improve expressivity for nonlinear temporal structures.",
              "Hybrid architecture is more robust than fully quantum models."
            ]
          }
        }
      }
    },
    
    {
      "id": 20,
      "title": "Conditional EVT models (2T-POT Hawkes)",
      "authors": "Tomlinson et al.",
      "year": 2022,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/2202.01043",
      "performance-metrics": [
        "Tail Index Estimation Error (ξ)",
        "Conditional Exceedance Rate Error",
        "Hawkes Intensity Estimation Error",
        "VaR Tail Quantile Forecast Accuracy",
        "CVaR Tail Severity Forecast Accuracy",
        "Bias–Variance Tradeoff Under Conditional EVT",
        "Goodness-of-Fit of GPD Conditional Tail Model",
        "Likelihood Improvement vs Static EVT Models",
        "Out-of-Sample Extreme Risk Forecast Accuracy",
        "Clustering-of-Exceedances Detection Accuracy"
      ],
      "tools-used-in": [
        "Extreme-Value-Theory-(EVT)–Peaks-Over-Threshold"
      ],
      "paper-details": {
        "algorithms": {
          "TwoThreshold_POT_Model": {
            "name": "Two-Threshold Peaks-Over-Threshold (2T-POT) Model",
            "purpose": "Enhance classical POT-EVT by introducing two thresholds to stabilize tail inference under regime-switching or clustered extremes.",
            "components": {
              "upper_threshold_u2": {
                "definition": "High threshold capturing the most extreme tail events.",
                "role": "Used for GPD modeling of severe extremes.",
                "effect": "Reduces bias from mixing moderate and extreme losses."
              },
              "lower_threshold_u1": {
                "definition": "Intermediate threshold capturing near-extreme exceedance behavior.",
                "role": "Provides more exceedances for improved parameter stability.",
                "effect": "Improves the estimation of ξ and β by increasing sample size."
              },
              "gpd_tail_model": {
                "formula": "GPD(y | ξ, β) = 1 − (1 + ξ y/β)^(-1/ξ)",
                "role": "Models exceedances above u2 or corrected via u1.",
                "effect": "Captures heavy-tailed nature of financial returns."
              }
            },
            "procedure": [
              "Choose u1 (moderate threshold) and u2 (extreme threshold).",
              "Extract exceedances above u1 and u2 separately.",
              "Model upper tail using GPD fitted to u2 exceedances.",
              "Use u1 exceedances to stabilize tail parameter estimates (borrowing strength).",
              "Combine conditional estimates for improved VaR/CVaR forecasting."
            ],
            "output": "More stable and accurate extreme quantile estimates than single-threshold EVT.",
            "advantages": [
              "Reduces threshold sensitivity.",
              "Better parameter stability when extreme events cluster.",
              "Improves tail quantile accuracy during volatile market regimes."
            ]
          },
    
          "Hawkes_Process_Conditional_EVT": {
            "name": "Hawkes Process for Conditional EVT Tail Modeling",
            "purpose": "Capture temporal clustering of extreme events by linking excess arrival intensity to a self-exciting Hawkes process.",
            "components": {
              "hawkes_intensity": {
                "formula": "λ(t) = μ + Σ α e^{-β (t - t_i)}",
                "role": "Models arrival rate of exceedances influenced by past extremes.",
                "effect": "Captures contagion and burstiness of financial tail risks."
              },
              "conditional_exceedance_probability": {
                "definition": "Probability of exceedance depends on current Hawkes intensity.",
                "role": "Makes EVT fully dynamic with respect to time.",
                "effect": "Adapts VaR/CVaR as risk regimes shift."
              }
            },
            "procedure": [
              "Identify exceedance timestamps using 2T-POT or standard threshold.",
              "Fit Hawkes process to exceedance arrival times.",
              "Estimate time-varying exceedance intensity λ(t).",
              "Condition EVT tail probabilities on λ(t).",
              "Compute time-dependent VaR and CVaR."
            ],
            "output": "Dynamic extreme risk forecasts accounting for tail clustering.",
            "advantages": [
              "Captures clustering of extremes, unlike static EVT.",
              "Predicts elevated risk periods based on recent shocks.",
              "Improves forecasting performance in crisis periods."
            ]
          },
    
          "Combined_2T_POT_Hawkes_Framework": {
            "name": "Combined 2T-POT + Hawkes Conditional EVT Model",
            "purpose": "Integrate threshold stabilization (2T-POT) with temporal clustering (Hawkes) to create a fully dynamic EVT tail model.",
            "components": {
              "joint_tail_probability": {
                "definition": "Tail probability depends on both extreme severity (GPD) and arrival intensity (Hawkes).",
                "role": "Provides richer modeling of both frequency and magnitude of extremes."
              },
              "dynamic_gpd_parameters": {
                "definition": "ξ(t), β(t) may adjust based on intensity λ(t) or regime.",
                "role": "Models evolving heavy-tail behavior."
              }
            },
            "procedure": [
              "Estimate thresholds u1, u2.",
              "Fit GPD to exceedances above u2.",
              "Model exceedance times with Hawkes process.",
              "Condition tail estimation on λ(t).",
              "Generate dynamic VaR/CVaR forecasts."
            ],
            "advantages": [
              "Handles both extremal dependence and clustering.",
              "Substantially reduces bias in turbulent markets.",
              "Better risk estimation during regime shifts."
            ]
          },
    
          "Tail_Risk_Estimation": {
            "name": "Tail Risk Estimation for Conditional EVT",
            "purpose": "Compute extreme quantiles and expected tail loss under 2T-POT and Hawkes-based dynamic EVT.",
            "metrics": {
              "var_formula": "VaRα(t) = u2 + (β(t)/ξ(t)) [(α^{-ξ(t)}) − 1]",
              "cvar_formula": "CVaR depends on dynamic tail loss distribution beyond VaR."
            },
            "procedure": [
              "Compute dynamic tail distribution using conditional GPD.",
              "Update ξ(t), β(t) based on Hawkes intensity.",
              "Compute time-varying VaR/CVaR for desired confidence levels.",
              "Backtest results on historical extreme events."
            ],
            "advantages": [
              "Forecasts extreme losses more accurately during clustered volatility.",
              "Improves risk sensitivity by incorporating temporal self-excitation."
            ]
          },
    
          "Model_Evaluation_and_Backtesting": {
            "name": "Model Evaluation & Backtesting",
            "purpose": "Assess conditional EVT model performance relative to classical static EVT and single-threshold POT.",
            "methods": {
              "goodness_of_fit": [
                "Likelihood comparison",
                "Parameter stability tests",
                "Tail diagnostic plots"
              ],
              "backtesting": [
                "Exceedance frequency tests",
                "Christoffersen independence test",
                "CVaR tail severity validation"
              ]
            },
            "role_in_paper": "Demonstrates that 2T-POT + Hawkes significantly improves tail risk modeling in financial time series."
          }
        }
      }
    },
    
    {
      "id": 21,
      "title": "Hybrid quantum-classical policy gradient methods",
      "authors": "Skolik et al.",
      "year": 2022,
      "source": "Quantum",
      "url": "https://quantum-journal.org/papers/q-2022-02-24-651/",
      "performance-metrics": [
        "Cumulative Reward",
        "Policy Gradient Variance",
        "Sample Efficiency",
        "Convergence Speed",
        "Quantum Circuit Fidelity",
        "Training Stability",
        "Gradient Estimation Error",
        "Return-to-Variance Ratio",
        "Exploration–Exploitation Performance",
        "Robustness to Noise in Quantum Circuits"
      ],
      "tools-used-in": [
        "Quantum-Reinforcement-Learning-(QRL)"
      ],
      "paper-details": {
        "algorithms": {
          "Hybrid_Quantum_Classical_Policy_Gradient": {
            "name": "Hybrid Quantum–Classical Policy Gradient Method",
            "purpose": "Use parameterized quantum circuits (PQCs) as policy networks within reinforcement learning (RL), trained with classical gradient-based optimization.",
            "components": {
              "quantum_policy_network": {
                "definition": "A variational quantum circuit that maps observations to action probabilities.",
                "role": "Acts as a policy parameterized by quantum gates.",
                "effect": "Provides expressive, high-dimensional policy representations difficult for classical models."
              },
              "classical_value_and_reward_processing": {
                "definition": "Reward accumulation, advantage estimation, and gradient computation performed classically.",
                "role": "Retains reliability and scalability of classical RL methods."
              },
              "policy_gradient_rule": {
                "formula": "∇θ J(θ) = E[∇θ log πθ(a|s) · A(s,a)]",
                "role": "Optimizes quantum policy parameters using classical RL gradients."
              }
            },
            "procedure": [
              "Initialize quantum policy with trainable parameters θ.",
              "Collect trajectories by interacting with the environment.",
              "Compute rewards, returns, and advantages classically.",
              "Estimate quantum gradients using the parameter-shift rule.",
              "Update θ using SGD/Adam or other optimizers.",
              "Repeat until convergence of cumulative reward."
            ],
            "output": "Optimized quantum policy capable of solving RL tasks.",
            "advantages": [
              "Quantum circuits may generalize better in high-dimensional state spaces.",
              "Provides richer nonlinear expressivity vs classical networks.",
              "Allows hybrid training feasible on near-term (NISQ) devices."
            ]
          },
    
          "Parameterized_Quantum_Policies": {
            "name": "Variational Quantum Policies",
            "purpose": "Define policies using PQCs that take classical observations and output probabilistic actions.",
            "components": {
              "data_encoding_layer": {
                "methods": [
                  "Angle encoding",
                  "Amplitude encoding",
                  "IQP-style feature maps"
                ],
                "role": "Map classical state s into qubit rotations."
              },
              "trainable_entangling_layers": {
                "definition": "Parameterized rotations + entangling gates capture correlations between inputs.",
                "role": "Increase expressiveness of quantum policy."
              },
              "measurement_head": {
                "definition": "Measurements map qubit states to action probabilities.",
                "role": "Allows sampling of stochastic actions."
              }
            },
            "procedure": [
              "Encode state into quantum circuit.",
              "Run PQC to generate quantum state.",
              "Measure qubits to obtain action probabilities.",
              "Sample action and interact with environment."
            ],
            "advantages": [
              "High expressivity even with small number of qubits.",
              "Potential quantum advantage for non-convex RL tasks."
            ]
          },
    
          "Quantum_Gradient_Estimation": {
            "name": "Gradient Estimation Using the Parameter-Shift Rule",
            "purpose": "Compute gradients of quantum policies efficiently for policy gradient RL.",
            "components": {
              "parameter_shift_rule": {
                "formula": "∂/∂θ f(θ) = 1/2[f(θ + π/2) − f(θ − π/2)]",
                "role": "Enables unbiased gradients of PQCs."
              }
            },
            "procedure": [
              "Evaluate PQC at θ + π/2 and θ − π/2.",
              "Compute gradient via finite difference-like rule.",
              "Use gradient in classical RL update loop."
            ],
            "advantages": [
              "Compatible with noisy hardware.",
              "Requires fewer circuit evaluations than full tomography.",
              "Enables end-to-end differentiability of hybrid systems."
            ]
          },
    
          "Policy_Gradient_RL_Framework": {
            "name": "Policy Gradient Reinforcement Learning Framework",
            "purpose": "Integrate quantum policies inside standard RL pipelines.",
            "steps": [
              "Collect rollouts using quantum policy.",
              "Compute returns and advantage function.",
              "Estimate gradients (quantum + classical).",
              "Update policy parameters.",
              "Evaluate cumulative reward."
            ],
            "advantages": [
              "Fully compatible with classical RL metrics.",
              "Allows plug-and-play replacement of neural networks with PQCs."
            ]
          },
    
          "Baseline_Comparison": {
            "name": "Baseline Comparisons: Quantum vs Classical Policies",
            "purpose": "Evaluate whether quantum policies offer advantages over classical neural networks.",
            "metrics_compared": [
              "Cumulative reward across episodes",
              "Convergence rate",
              "Sample efficiency",
              "Policy stability",
              "Exploration–exploitation tradeoff"
            ],
            "key_findings": [
              "Quantum policies match or outperform classical policies in several environments.",
              "Hybrid approaches more stable than fully quantum RL.",
              "Quantum circuits provide more expressive policy classes in low-parameter settings."
            ]
          }
        }
      }
    },
    
    {
      "id": 22,
      "title": "Generative Adversarial Networks Applied to Synthetic Financial Scenarios Generation",
      "authors": "Rizzato et al.",
      "year": 2022,
      "source": "ressources-actuarielles.net",
      "url": "https://www.ressources-actuarielles.net/...",
      "performance-metrics": [
        "Distributional Similarity (Wasserstein Distance)",
        "Kolmogorov–Smirnov (KS) Test Distance",
        "Mean Absolute Error of Financial Statistics",
        "Return Distribution Matching Error",
        "Volatility Clustering Similarity",
        "Autocorrelation Structure Preservation",
        "Scenario Realism Score",
        "Mode Collapse Detection Metrics",
        "Training Stability Metrics",
        "Scenario Diversity Index"
      ],
      "tools-used-in": [
        "Quantum-Generative-Adversarial-Network-(QGAN)-for-Scenario-Generation"
      ],
      "paper-details": {
        "algorithms": {
          "GAN_for_Financial_Scenarios": {
            "name": "Generative Adversarial Network (GAN) for Financial Scenario Generation",
            "purpose": "Generate synthetic financial return paths that preserve statistical properties observed in real market data.",
            "components": {
              "generator": {
                "definition": "Neural network G(z) transforms noise z into synthetic financial data.",
                "role": "Learns the underlying distribution of financial returns.",
                "effect": "Produces realistic synthetic time-series respecting market characteristics."
              },
              "discriminator": {
                "definition": "Neural network D(x) classifies real vs generated data.",
                "role": "Guides generator toward producing more realistic samples.",
                "effect": "Improves statistical fidelity of generated scenarios."
              },
              "adversarial_loss": {
                "formula": "min_G max_D E[log D(x)] + E[log(1 − D(G(z)))]",
                "role": "Drives adversarial learning between generator and discriminator."
              }
            },
            "procedure": [
              "Collect historical financial return data.",
              "Train generator to create synthetic scenarios from noise.",
              "Train discriminator to distinguish real vs synthetic samples.",
              "Iteratively optimize both networks until convergence.",
              "Evaluate generated scenarios using statistical similarity metrics.",
              "Use scenarios for risk forecasting, stress testing, or model validation."
            ],
            "output": "Synthetic financial scenarios that mimic real market behavior.",
            "advantages": [
              "Captures nonlinear dependencies and heavy tails.",
              "Generates large samples efficiently for Monte Carlo risk models.",
              "Preserves stylized facts of financial time series."
            ]
          },
    
          "Financial_Stylized_Facts_Module": {
            "name": "Stylized Facts Preservation Module",
            "purpose": "Ensure GAN-generated time-series reflect statistical properties of real markets.",
            "stylized_facts": {
              "heavy_tails": "Empirical returns exhibit fat-tailed distributions.",
              "volatility_clustering": "Periods of high volatility tend to cluster.",
              "autocorrelation_decay": "Autocorrelation of returns is near zero, but square returns decay slowly.",
              "asymmetry": "Negative skewness or leverage effects."
            },
            "procedure": [
              "Analyze real data stylized facts.",
              "Monitor corresponding metrics during GAN training.",
              "Penalize generator when stylized facts diverge from targets.",
              "Use statistical tests (KS, Wasserstein, Ljung–Box)."
            ],
            "advantages": [
              "Ensures realism beyond mere distribution matching.",
              "Improves usefulness for stress testing and risk simulations."
            ]
          },
    
          "Wasserstein_GAN_Extension": {
            "name": "Wasserstein GAN (WGAN) for Finance",
            "purpose": "Address mode collapse and unstable training of classical GANs.",
            "components": {
              "critic_network": {
                "definition": "Replaces discriminator to estimate Wasserstein distance.",
                "role": "Provides smoother gradients and stable training."
              },
              "gradient_penalty": {
                "definition": "Penalty term to enforce Lipschitz constraint.",
                "formula": "λ (||∇ D(x)||₂ - 1)²"
              }
            },
            "procedure": [
              "Replace discriminator with critic network.",
              "Optimize Wasserstein distance between real and synthetic distributions.",
              "Train generator to minimize critic's score."
            ],
            "advantages": [
              "Stable training dynamics.",
              "Higher-quality synthetic financial scenarios.",
              "Better preservation of tail behavior."
            ]
          },
    
          "Scenario_Generation_Pipeline": {
            "name": "Financial Scenario Generation Pipeline",
            "purpose": "Integrate GAN-based generation into risk management workflows.",
            "steps": [
              "Preprocess and normalize historical returns.",
              "Train GAN/WGAN model on historical data.",
              "Generate synthetic market scenarios.",
              "Transform outputs back to original scale.",
              "Use scenarios for VaR/CVaR estimation, stress tests, or portfolio simulations."
            ],
            "advantages": [
              "Generates diverse stress scenarios.",
              "Enhances simulation-based risk models."
            ]
          },
    
          "Model_Evaluation_and_Backtesting": {
            "name": "Model Evaluation & Statistical Backtesting",
            "purpose": "Validate realism and utility of synthetically generated scenarios.",
            "methods": {
              "distribution_tests": [
                "Wasserstein distance",
                "KS test",
                "Energy distance"
              ],
              "dependence_tests": [
                "ACF/PACF matching",
                "Volatility clustering score"
              ],
              "risk_tests": [
                "VaR backtesting",
                "Tail coverage comparison"
              ]
            },
            "role_in_paper": "Shows that GAN-generated scenarios align well with real-world market distributions and improve scenario-based risk forecasting."
          }
        }
      }
    },
    
    {
      "id": 23,
      "title": "Quantum Risk Analysis: Beyond (Conditional) Value-at-Risk",
      "authors": "Laudagé & Turkalj",
      "year": 2022,
      "source": "arXiv",
      "url": "https://arxiv.org/pdf/2211.04456",
      "performance-metrics": [
        "Amplitude Estimation Error",
        "Tail Probability Estimation Error",
        "Risk Measure Convergence Rate",
        "QAE Query Complexity",
        "Sampling Cost Reduction vs Classical MC",
        "Noise Robustness in Risk Estimation",
        "Accuracy of Higher-Order Risk Metrics (beyond VaR/CVaR)",
        "Bias–Variance Tradeoff of Quantum Estimators",
        "Success Probability of Quantum Risk Estimator",
        "Resource Scaling with Portfolio Dimension"
      ],
      "tools-used-in": [
        "Quantum-Amplitude-Estimation-(QAE)-for-CVaR"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Risk_Analysis_Framework": {
            "name": "Quantum Risk Analysis Framework",
            "purpose": "Extend quantum risk estimation beyond VaR and CVaR by computing a broader family of risk measures using amplitude estimation.",
            "components": {
              "quantum_risk_oracle": {
                "definition": "Oracle encodes portfolio loss distribution into quantum amplitudes.",
                "role": "Allows efficient estimation of risk measures as expectation values.",
                "effect": "Achieves quadratic speedup vs classical Monte Carlo."
              },
              "general_risk_function_f": {
                "definition": "Risk metric defined as ρ(X) = E[f(X)], where f may represent nonlinear tail risk functions.",
                "examples": [
                  "Spectral risk measures",
                  "Distortion risk measures",
                  "Higher-moment risk measures"
                ],
                "role": "Unifies VaR/CVaR with more general risk analysis frameworks."
              },
              "amplitude_encoding": {
                "definition": "Map scaled losses L into amplitudes of quantum states.",
                "role": "Allows expectation of f(L) to be estimated through QAE."
              }
            },
            "procedure": [
              "Define portfolio loss random variable L.",
              "Choose risk function f(L) representing desired risk metric.",
              "Encode f(L) into quantum oracle amplitude.",
              "Apply Quantum Amplitude Estimation (QAE) to estimate E[f(L)].",
              "Convert estimated amplitude into numerical risk measure.",
              "Repeat for multiple risk functions if needed."
            ],
            "output": "Generalized quantum risk estimates including VaR, CVaR, spectral risks, and more.",
            "advantages": [
              "Quadratic speedup for estimating expectations vs classical MC.",
              "Supports a wide family of risk measures, not only VaR/CVaR.",
              "More flexible than classical simulation-based approaches."
            ]
          },
    
          "Generalized_CVaR_and_Spectral_Risk": {
            "name": "Quantum Estimation of CVaR and Spectral Risk",
            "purpose": "Extend quantum CVaR estimation to weighted tail expectations where weights depend on quantiles.",
            "components": {
              "spectral_weight_function": {
                "definition": "Risk = ∫₀¹ φ(p) VaR_p dp",
                "examples": [
                  "Wang distortion function",
                  "Power law weighting",
                  "Exponential tail emphasis"
                ],
                "role": "Captures investor risk appetite more finely than CVaR."
              },
              "qae_expectation_estimator": {
                "definition": "Estimates weighted tail expectations via expectation of f(L).",
                "role": "Generalizable to arbitrary spectral risk measures."
              }
            },
            "procedure": [
              "Compute quantile-based weighting function φ(p).",
              "Encode f(L) = tail-weighted losses into amplitudes.",
              "Use QAE to estimate expected weighted tail loss.",
              "Return spectral risk or generalized CVaR."
            ],
            "advantages": [
              "Broadens applicability of quantum finance frameworks.",
              "Handles risk measures sensitive to deep tail behavior.",
              "Allows consistent comparison across multiple risk definitions."
            ]
          },
    
          "Quantum_Amplitude_Estimation_Core": {
            "name": "Quantum Amplitude Estimation (QAE)",
            "purpose": "Efficiently estimate expected values of functions of portfolio losses.",
            "components": {
              "oracle_operator": {
                "definition": "O_f encodes f(L) into amplitudes of the target qubit.",
                "role": "Transforms risk measurement into estimation of amplitude a."
              },
              "grover_operator": {
                "definition": "Q = -A S₀ A⁻¹ S_ψ used for amplitude amplification.",
                "role": "Amplifies relevant subspace for better estimation."
              }
            },
            "procedure": [
              "Prepare amplitude-encoded loss distribution.",
              "Apply controlled Grover iterates.",
              "Perform inverse quantum Fourier transform.",
              "Extract amplitude estimate â.",
              " Convert amplitude into risk measure."
            ],
            "advantages": [
              "O(1/ε) scaling for error ε vs O(1/ε²) for classical MC.",
              "Allows efficient computation even for complex risk metrics.",
              "General-purpose tool for quantum financial risk analysis."
            ],
            "limitations": [
              "Requires coherent circuits with low noise.",
              "State preparation may dominate cost in large portfolios."
            ]
          },
    
          "Risk_Function_Encoding_Module": {
            "name": "Encoding Arbitrary Risk Functions into Quantum Circuits",
            "purpose": "Convert a broad family of risk functions into amplitude-encoded expectation values.",
            "steps": [
              "Scale and normalize portfolio losses.",
              "Define risk function f(L) (piecewise, nonlinear, quantile-based).",
              "Encode f(L) into controlled rotations or controlled amplitude preparation.",
              "Integrate function encoding with QAE pipeline."
            ],
            "advantages": [
              "Works for VaR, CVaR, spectral risk, moment-based risk, and distortion risk.",
              "Flexible framework for extending quantum risk analytics."
            ]
          },
    
          "Evaluation_Framework": {
            "name": "Evaluation and Benchmarking Framework",
            "purpose": "Benchmark quantum risk estimation vs classical Monte Carlo.",
            "metrics": {
              "error_scaling": "Compare O(1/ε) (QAE) vs O(1/ε²) (MC).",
              "runtime_scaling": "Number of oracle calls vs classical samples.",
              "tail_accuracy": "Fidelity in estimating deep tail probabilities.",
              "stability_under_noise": "Performance on noisy quantum hardware or simulations."
            },
            "findings": [
              "Quantum risk estimators provide quadratic speedup in theory.",
              "Generalized risk measures can be estimated within the same QAE framework.",
              "Noise remains the major challenge but hybrid variants remain viable."
            ]
          }
        }
      }
    },
    
    {
      "id": 24,
      "title": "Quantum computing for financial risk measurement",
      "authors": null,
      "year": 2022,
      "source": "SpringerLink",
      "url": "https://link.springer.com/article/10.1007/s11128-022-03777-2",
      "performance-metrics": [
        "Amplitude Estimation Error",
        "VaR Estimation Error",
        "CVaR Estimation Error",
        "Quantum Query Complexity",
        "Sampling Cost Reduction vs Classical MC",
        "Tail Probability Estimation Accuracy",
        "Convergence Rate Under Noisy Circuits",
        "State Preparation Complexity",
        "Risk Metric Stability Across Multiple Runs",
        "Estimator Confidence Interval Width"
      ],
      "tools-used-in": [
        "Quantum-Amplitude-Estimation-(QAE)-for-CVaR"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Risk_Measurement_Framework": {
            "name": "Quantum Framework for Financial Risk Measurement",
            "purpose": "Apply quantum computing—primarily amplitude estimation—to compute VaR, CVaR, and other expectation-based risk metrics more efficiently than classical Monte Carlo.",
            "components": {
              "quantum_state_preparation": {
                "definition": "Encode loss distribution into amplitudes of quantum states.",
                "role": "Foundation of quantum probability estimation.",
                "effect": "Allows expectation values to be accessed via amplitude estimation."
              },
              "risk_measure_function": {
                "definition": "General risk metric defined as ρ(L) = E[f(L)].",
                "examples": [
                  "VaR via quantile search",
                  "CVaR via conditional expectation",
                  "Distortion/spectral risk measures"
                ],
                "role": "Transforms risk evaluation into quantum expectation estimation."
              },
              "quantum_estimator": {
                "definition": "Quantum Amplitude Estimation (QAE) for computing expectations.",
                "role": "Provides quadratic speedup vs classical MC."
              }
            },
            "procedure": [
              "Prepare quantum state encoding probability distribution of losses.",
              "Define function f(L) corresponding to risk metric (VaR, CVaR, etc.).",
              "Encode f(L) into amplitudes using quantum oracles.",
              "Apply amplitude estimation to extract expectation of f(L).",
              "Convert amplitude estimate into numerical risk metric.",
              "Repeat or refine for accuracy."
            ],
            "output": "Quantum-accelerated estimates of VaR, CVaR, and more general risk measures.",
            "advantages": [
              "Achieves O(1/ε) scaling vs O(1/ε²) for classical Monte Carlo.",
              "Supports a broad family of risk measures.",
              "Applicable to large portfolios where classical simulation becomes expensive."
            ]
          },
    
          "Amplitude_Estimation_for_VaR": {
            "name": "Quantum Amplitude Estimation (QAE) for VaR",
            "purpose": "Estimate Value-at-Risk by computing quantiles using quantum-enhanced probability estimation.",
            "components": {
              "indicator_function_encoding": {
                "definition": "𝟙(L ≥ τ) encoded as amplitude for threshold τ.",
                "role": "Quantum circuit marks tail-loss events."
              },
              "probability_estimation": {
                "definition": "Use QAE to estimate P(L ≥ τ).",
                "role": "Determines quantile level for VaR search."
              }
            },
            "procedure": [
              "Choose a candidate threshold τ.",
              "Encode indicator of L ≥ τ into amplitude rotation.",
              "Run QAE to estimate probability of exceedance.",
              "Adjust τ via binary search until P(L ≥ τ) = 1 − α.",
              "Return VaRα = τ."
            ],
            "advantages": [
              "Faster quantile estimation via quantum-enhanced probability computation.",
              "More accurate tail modeling with fewer samples."
            ],
            "limitations": [
              "Requires repeated QAE runs for quantile search.",
              "Noise affects precision for deep tail thresholds."
            ]
          },
    
          "Amplitude_Estimation_for_CVaR": {
            "name": "Quantum Estimation of CVaR via Amplitude Encoding",
            "purpose": "Compute Conditional Value-at-Risk as tail expectation more efficiently than classical Monte Carlo.",
            "components": {
              "tail_loss_encoding": {
                "definition": "Encode f(L) = max(0, L − VaR) as amplitudes.",
                "role": "Allows CVaR = E[L | L ≥ VaR] to be computed through QAE."
              }
            },
            "procedure": [
              "Estimate VaR (via previous module or classically).",
              "Encode tail losses into amplitude rotations conditioned on L ≥ VaR.",
              "Run amplitude estimation to compute expected tail loss.",
              "Combine with VaR to compute CVaRα."
            ],
            "advantages": [
              "Quadratic speedup for tail expectation computation.",
              "Efficient for large Monte Carlo or nested simulation tasks."
            ]
          },
    
          "State_Preparation_and_Data_Loading": {
            "name": "State Preparation for Loss Distributions",
            "purpose": "Load financial return or loss distributions into quantum states for amplitude encoding.",
            "methods": {
              "qRAM_loading": "Load large datasets into amplitude-encoded states (theoretical).",
              "function_based_state_preparation": "Generate distribution from analytical models (lognormal, mixture models)."
            },
            "procedure": [
              "Normalize loss distribution.",
              "Map probability mass to amplitudes.",
              "Prepare initial quantum state A|0⟩.",
              "Validate distribution via measurement sampling."
            ],
            "advantages": [
              "Enables quantum Monte Carlo frameworks.",
              "Works for both analytical and empirical distributions."
            ],
            "limitations": [
              "qRAM not yet available in practice.",
              "State preparation can dominate cost."
            ]
          },
    
          "Comparison_With_Classical_Methods": {
            "name": "Comparison with Classical Monte Carlo Risk Measurement",
            "purpose": "Benchmark quantum techniques against classical simulation-based risk estimation.",
            "metrics_compared": [
              "Error scaling",
              "Runtime vs precision",
              "Required sample size",
              "Accuracy of tail probability estimates"
            ],
            "key_findings": [
              "Quantum amplitude estimation offers theoretical quadratic speedup.",
              "Quantum techniques excel in deep tail probability estimation.",
              "Practical performance depends on noise and state-preparation costs."
            ]
          },
    
          "Generalized_Risk_Measures_Module": {
            "name": "Generalized Risk Measures Beyond VaR and CVaR",
            "purpose": "Apply QAE to compute broader classes of risk metrics used in finance.",
            "risk_examples": [
              "Spectral risk measures",
              "Distortion risk metrics",
              "Entropic risk",
              "Higher-moment tail risks"
            ],
            "procedure": [
              "Define risk-transformation function f(L).",
              "Encode f(L) into quantum amplitudes.",
              "Use QAE to estimate expectation of f(L).",
              "Output generalized risk measure."
            ],
            "advantages": [
              "Flexible and extensible quantum risk engine.",
              "Unified mechanism for many risk measures."
            ]
          }
        }
      }
    },
    
    {
      "id": 25,
      "title": "Benchmarking the performance of portfolio optimization with QAOA",
      "authors": null,
      "year": 2022,
      "source": "SpringerLink",
      "url": "https://link.springer.com/article/10.1007/s11128-022-03766-5",
      "performance-metrics": [
        "Approximation Ratio (Energy Gap)",
        "Portfolio Return vs Risk Tradeoff",
        "Convergence Speed of QAOA Layers (p-depth)",
        "Sampling Fidelity",
        "Probability of Optimal Portfolio State",
        "Noise Robustness on NISQ Devices",
        "Expected Portfolio Loss Variance",
        "Classical vs Quantum Runtime Comparison",
        "Circuit Depth vs Solution Quality",
        "Qubit Count Scaling vs Asset Universe Size"
      ],
      "tools-used-in": [
        "Quantum-Approximate-Optimization-Algorithm-(QAOA)-for-CVaR-based-Portfolio-Optimization"
      ],
      "paper-details": {
        "algorithms": {
          "QAOA_for_Portfolio_Optimization": {
            "name": "Quantum Approximate Optimization Algorithm (QAOA) for Portfolio Optimization",
            "purpose": "Use QAOA to solve the quadratic unconstrained binary optimization (QUBO) formulation of mean–variance portfolio selection.",
            "components": {
              "qubo_formulation": {
                "definition": "Transform portfolio optimization into minimizing H = −μᵀx + λ xᵀΣx.",
                "role": "Encodes expected returns μ and covariance Σ into a Hamiltonian.",
                "effect": "Maps classical optimization problem to quantum energy minimization."
              },
              "cost_hamiltonian": {
                "definition": "H_C encodes objective function using ZZ and Z terms.",
                "role": "Represents portfolio variance and return contributions."
              },
              "mixer_hamiltonian": {
                "definition": "H_M = Σ X_i",
                "role": "Induces transitions between portfolio configurations."
              },
              "variational_parameters": {
                "definition": "Angles (γ, β) optimized to minimize energy.",
                "role": "Control depth, expressivity, and convergence."
              }
            },
            "procedure": [
              "Encode portfolio problem into QUBO Hamiltonian.",
              "Initialize qubits in |+⟩ state.",
              "Apply p alternating layers of cost and mixer unitaries.",
              "Sample resulting quantum state to estimate energy.",
              "Optimize parameters (γ, β) to minimize expected energy.",
              "Decode optimal bitstring into portfolio allocation."
            ],
            "output": "Portfolio configuration approximating optimal mean–variance tradeoff.",
            "advantages": [
              "Naturally fits binary selection tasks such as constrained portfolios.",
              "Can outperform classical heuristics for certain problem classes.",
              "Scales with increasing depth p for improved accuracy."
            ]
          },
    
          "Hamiltonian_Construction_Module": {
            "name": "Hamiltonian Construction for Portfolio Mean–Variance Model",
            "purpose": "Encode expected returns and risk into a quantum Hamiltonian suitable for QAOA.",
            "components": {
              "return_term": {
                "formula": "H_returns = − Σ μ_i Z_i",
                "role": "Rewards including an asset in the portfolio."
              },
              "risk_term": {
                "formula": "H_risk = λ Σ Σ_ij Z_i Z_j",
                "role": "Penalizes variance through pairwise covariance terms."
              },
              "budget_constraint": {
                "definition": "Soft constraint added through penalty terms.",
                "role": "Enforces number-of-assets or weight constraints if needed."
              }
            },
            "procedure": [
              "Compute μ (expected returns) and Σ (covariance matrix).",
              "Map return and variance terms into Z and ZZ operators.",
              "Combine terms into cost Hamiltonian H_C."
            ],
            "advantages": [
              "Allows representation of classical quadratic objective as quantum energy landscape.",
              "Directly compatible with QAOA."
            ]
          },
    
          "Parameter_Optimization_Framework": {
            "name": "Parameter Optimization for QAOA",
            "purpose": "Tune variational parameters (γ, β) for best portfolio solution.",
            "methods": {
              "classical_optimizers": [
                "COBYLA",
                "SPSA",
                "Nelder–Mead",
                "BFGS"
              ],
              "landscape_exploration": "Analyze energy landscape sensitivity to γ and β."
            },
            "procedure": [
              "Choose initial parameter guesses.",
              "Iteratively run QAOA circuit and compute energy.",
              "Update parameters via classical optimizer.",
              "Repeat until energy converges or target quality reached."
            ],
            "advantages": [
              "Hybrid loop leverages classical optimizers while using quantum circuits for evaluation.",
              "SPSA reduces shot requirements for noisy hardware."
            ]
          },
    
          "Sampling_and_Decoding_Module": {
            "name": "Sampling and Portfolio Decoding Module",
            "purpose": "Interpret sampled bitstrings from the QAOA state as portfolio selections.",
            "steps": [
              "Measure quantum state after QAOA layers.",
              "Collect repeated samples to approximate probability distribution.",
              "Identify bitstring with minimum energy.",
              "Interpret bits as asset inclusion/exclusion."
            ],
            "advantages": [
              "Works naturally for cardinality-constrained portfolios.",
              "Allows Monte-Carlo–style sampling of near-optimal portfolios."
            ]
          },
    
          "Benchmarking_Framework": {
            "name": "Benchmarking Quantum vs Classical Portfolio Optimization",
            "purpose": "Evaluate QAOA performance against classical solvers.",
            "metrics": {
              "classical_baselines": [
                "Simulated annealing",
                "Genetic algorithms",
                "Greedy heuristics",
                "Mixed-integer quadratic programming (MIQP)"
              ],
              "evaluation_metrics": [
                "Approximation ratio",
                "Portfolio mean return",
                "Portfolio variance",
                "Probability of sampling optimal solution",
                "Circuit depth vs performance curves"
              ]
            },
            "findings": [
              "QAOA can match or outperform classical heuristics for certain small/medium instances.",
              "Performance depends strongly on p-depth and noise levels.",
              "Hamiltonian structure impacts expressivity and convergence."
            ]
          }
        }
      }
    },
    
    {
      "id": 26,
      "title": "QBM for financial data generation",
      "authors": "Zoufal et al.",
      "year": 2021,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/2006.06004",
      "performance-metrics": [
        "Log-Likelihood of Generated Data",
        "Kullback–Leibler (KL) Divergence",
        "Wasserstein Distance Between Real and Generated Returns",
        "Autocorrelation Structure Preservation",
        "Volatility Clustering Similarity",
        "Distributional Fit Error (Heavy Tails, Skewness)",
        "Training Convergence Stability",
        "Mode Coverage / Diversity of Generated Samples",
        "Quantum State Fidelity",
        "Sampling Efficiency from the QBM"
      ],
      "tools-used-in": [
        "Quantum-Boltzmann-Machine-(QBM)"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Boltzmann_Machine": {
            "name": "Quantum Boltzmann Machine (QBM)",
            "purpose": "Model and generate financial return distributions using a quantum analog of classical Boltzmann machines, leveraging quantum states to encode probability distributions.",
            "components": {
              "quantum_hamiltonian": {
                "definition": "H(θ) = Σ_i h_i Z_i + Σ_{i<j} J_ij Z_i Z_j (Ising-type model)",
                "role": "Energy function determining probability distribution P(x) ∝ exp(−H(x)).",
                "effect": "Allows modeling of complex, correlated distributions in Hilbert space."
              },
              "thermal_state_representation": {
                "definition": "ρ = e^{−H} / Tr(e^{−H})",
                "role": "Quantum thermal state encodes the learned distribution.",
                "effect": "Provides richer representation than classical Boltzmann distributions."
              },
              "quantum_training_rule": {
                "definition": "Gradient computed from difference between data expectation ⟨O⟩_data and model expectation ⟨O⟩_model.",
                "role": "Updates Hamiltonian parameters (h_i, J_ij) to match data distribution."
              }
            },
            "procedure": [
              "Encode training financial returns into binary/qubit form (bucketed returns).",
              "Initialize Hamiltonian parameters (h, J).",
              "Prepare quantum thermal state using simulation mechanisms (QITE/QAOA-like).",
              "Compute expectation values for observables under data and model distributions.",
              "Update parameters to minimize divergence between model and training data.",
              "Sample from learned quantum distribution to generate synthetic returns."
            ],
            "output": "A trained quantum distribution that approximates real financial return distributions.",
            "advantages": [
              "Can model complex dependencies with fewer parameters.",
              "Potential to capture heavy tails, skewness, volatility clusters.",
              "Quantum states may represent distributions unreachable by classical models."
            ]
          },
    
          "Quantum_State_Preparation": {
            "name": "Quantum State Preparation for QBM",
            "purpose": "Prepare quantum states approximating thermal distributions of the QBM Hamiltonian.",
            "methods": {
              "qaoa_thermalization": "Use QAOA-like variational circuits to mimic thermal states.",
              "qite": "Quantum Imaginary Time Evolution approximates e^{−βH}.",
              "gibbs_sampling": "Quantum method for sampling from ρ ∝ e^{−H}."
            },
            "procedure": [
              "Choose method (QITE, QAOA, or variational Gibbs).",
              "Prepare approximate thermal state corresponding to Hamiltonian parameters.",
              "Use state for model expectation estimation."
            ],
            "advantages": [
              "Avoids classical sampling bottlenecks.",
              "Enables learning of distributions classically hard to compute."
            ]
          },
    
          "QBM_Training_Loop": {
            "name": "QBM Training Procedure",
            "purpose": "Optimize Hamiltonian parameters so the quantum distribution matches empirical financial returns.",
            "steps": [
              "Compute data expectations ⟨Z_i⟩ and ⟨Z_i Z_j⟩ from training data.",
              "Compute model expectations via quantum state measurements.",
              "Update parameters using gradient-based or contrastive divergence-like methods.",
              "Iterate until convergence."
            ],
            "advantages": [
              "Generalizes classical Boltzmann machine training.",
              "Leverages quantum sampling for improved expressive capacity."
            ]
          },
    
          "Financial_Data_Generation_with_QBM": {
            "name": "QBM for Financial Scenario Generation",
            "purpose": "Generate synthetic financial return paths consistent with real market structure.",
            "procedure": [
              "Train QBM on historical returns.",
              "Sample bitstrings from learned thermal quantum state.",
              "Map samples back to discretized return levels.",
              "Assemble time-series scenarios for use in stress testing or risk forecasting."
            ],
            "advantages": [
              "Captures nonlinear and multivariate dependencies.",
              "Generates realistic stress scenarios and fat-tailed distributions."
            ]
          },
    
          "Evaluation_and_Validation": {
            "name": "Evaluation Framework for QBM Financial Modeling",
            "purpose": "Assess the quality of learned quantum-generated financial distributions.",
            "methods": {
              "distribution_tests": [
                "KL divergence",
                "Wasserstein distance",
                "JS divergence"
              ],
              "stylized_fact_tests": [
                "Fat tails",
                "Volatility clustering",
                "Autocorrelation structure"
              ],
              "model_fit_tests": [
                "Log-likelihood on test data",
                "Moment matching",
                "Tail probability accuracy"
              ]
            },
            "role_in_paper": "Shows QBMs can approximate real financial distributions and outperform classical generative models in certain tasks."
          }
        }
      }
    },
    
    {
      "id": 27,
      "title": "Simultaneous quantum estimation of multiple moments",
      "authors": "Zhao et al.",
      "year": 2021,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/2103.09230",
      "performance-metrics": [
        "Estimation Error for First Moment (Mean)",
        "Estimation Error for Second Moment (Variance)",
        "Estimation Error for Higher-Order Moments",
        "Quantum Query Complexity",
        "Sample Complexity Reduction vs Classical MC",
        "Bias–Variance Tradeoff Across Estimated Moments",
        "Robustness to Noise and Decoherence",
        "Success Probability of Moment Extraction",
        "Scaling with Number of Moments",
        "Circuit Depth Efficiency"
      ],
      "tools-used-in": [
        "Quantum-Amplitude-Estimation-for-Expected-Return-and-Risk"
      ],
      "paper-details": {
        "algorithms": {
          "Simultaneous_Moment_Estimation_Framework": {
            "name": "Simultaneous Quantum Estimation of Multiple Statistical Moments",
            "purpose": "Estimate several statistical moments—such as mean, variance, skewness, and kurtosis—within a single unified quantum algorithm using amplitude encoding.",
            "components": {
              "moment_functions_f_k": {
                "definition": "fₖ(x) = xᵏ encoded into rotation angles or controlled amplitude functions.",
                "role": "Allows simultaneous extraction of E[xᵏ] for multiple k.",
                "effect": "Enables multi-moment estimation from a single quantum state preparation."
              },
              "amplitude_encoding": {
                "definition": "Encode classical random variable X into quantum amplitudes.",
                "role": "Allows estimation of expectations via amplitude estimation subroutines."
              },
              "multi-moment_estimation_oracle": {
                "definition": "Quantum oracle computes fₖ(x) in parallel via controlled rotations.",
                "role": "Enables estimation of several expected values from a shared circuit."
              }
            },
            "procedure": [
              "Encode distribution of X (e.g., financial returns) into quantum amplitudes.",
              "Define moment functions f₁(x), f₂(x), … for mean, variance, etc.",
              "Construct controlled-unitary operators encoding these functions.",
              "Apply amplitude estimation to extract expectation values simultaneously.",
              "Post-process amplitudes to compute moments of interest."
            ],
            "output": "Estimates of statistical moments such as mean (µ), variance (σ²), skewness, and kurtosis.",
            "advantages": [
              "Quadratic speedup vs classical Monte Carlo.",
              "Simultaneous estimation reduces repeated circuit executions.",
              "Useful for risk analysis requiring multiple moments (e.g., portfolio expected return, variance, skewness)."
            ]
          },
    
          "Quantum_Amplitude_Estimation_Module": {
            "name": "Quantum Amplitude Estimation (QAE) for Multi-Moment Extraction",
            "purpose": "Use a generalized form of amplitude estimation to recover several moments from a shared quantum state.",
            "components": {
              "parallel_estimation_structure": {
                "definition": "Use multiple ancilla qubits or controlled rotations to encode different fₖ(x) functions.",
                "role": "Allows independent estimation of multiple expectations from same samples."
              },
              "grover_like_operator": {
                "definition": "Generalized Grover operator that amplifies amplitudes encoding moment information.",
                "role": "Enables extraction of different amplitudes corresponding to different statistical moments."
              }
            },
            "procedure": [
              "Prepare amplitude-encoded state for X.",
              "Apply different controlled-unitary blocks corresponding to fₖ(x).",
              "Perform amplitude estimation on combined circuit.",
              "Measure appropriate ancilla qubits for different moments."
            ],
            "advantages": [
              "Reduces number of separate QAE runs required.",
              "Efficient for computing variance, skewness, kurtosis needed in risk models."
            ],
            "limitations": [
              "Complexity increases with number of estimated moments.",
              "Noise affects accuracy of higher-order moment estimates."
            ]
          },
    
          "Financial_Risk_Moments_Module": {
            "name": "Financial Risk Moments Estimation",
            "purpose": "Use quantum moment estimation for portfolio risk modeling, where multiple moments govern expected return, volatility, and tail characteristics.",
            "moments": {
              "mean": "E[X] — expected return.",
              "variance": "E[X²] − (E[X])² — risk/volatility.",
              "skewness": "Asymmetry of return distribution.",
              "kurtosis": "Fat-tailedness of return distribution."
            },
            "procedure": [
              "Encode asset or portfolio return distribution.",
              "Apply multi-moment QAE method.",
              "Extract µ, σ², skewness, kurtosis.",
              "Use moments for portfolio optimization or risk evaluation."
            ],
            "advantages": [
              "Enhances multi-criteria risk measures.",
              "Improves scenario generation or derivative pricing that depends on many moments."
            ]
          },
    
          "State_Preparation_and_Data_Encoding": {
            "name": "State Preparation for Moment Estimation",
            "purpose": "Load random variable distribution into quantum amplitudes.",
            "methods": {
              "function_loading": "Generate distribution from analytical model (Gaussian mixture, lognormal).",
              "qRAM_based_loading": "Load empirical samples directly (theoretical capability)."
            },
            "procedure": [
              "Normalize data distribution.",
              "Embed values into quantum amplitudes.",
              "Validate preparation via measurement sampling."
            ],
            "advantages": [
              "General-purpose encoding works for any financial return distribution."
            ]
          },
    
          "Benchmarking_and_Error_Analysis": {
            "name": "Benchmarking & Error Analysis",
            "purpose": "Evaluate performance of multi-moment estimation relative to classical MC and standard QAE.",
            "metrics": {
              "mean_error": "Error in E[X].",
              "variance_error": "Error in E[X²] and σ².",
              "higher_moment_error": "Error in skewness/kurtosis.",
              "scaling_analysis": "Query complexity and runtime scaling.",
              "noise_analysis": "Impact of decoherence, SPAM error, gate infidelity."
            },
            "findings": [
              "Quantum method provides asymptotic quadratic speedup for all moments.",
              "Simultaneous estimation reduces state-preparation overhead.",
              "Higher-order moments require deeper circuits but remain feasible on simulated devices."
            ]
          }
        }
      }
    },
    
    {
      "id": 28,
      "title": "Transformational Approach to Analytical Value-at-Risk",
      "authors": "Prakash et al.",
      "year": 2021,
      "source": "MDPI",
      "url": "https://www.mdpi.com/1911-8074/14/2/51",
      "performance-metrics": [
        "Analytical VaR Accuracy",
        "Tail Probability Error",
        "Mean Squared Error of VaR Estimate",
        "Deviation from Historical VaR",
        "Variance–Covariance Matrix Stability",
        "Error Sensitivity to Non-Normality",
        "Model Robustness Under Skewness",
        "Computation Speed vs Classical Delta–Normal VaR",
        "Portfolio Aggregation Error",
        "Scenario Transformation Consistency"
      ],
      "tools-used-in": [
        "Variance-Covariance-Value-at-Risk"
      ],
      "paper-details": {
        "algorithms": {
          "Transformational_Analytical_VaR": {
            "name": "Transformational Approach to Analytical Value-at-Risk",
            "purpose": "Improve analytical VaR estimation by applying distributional transformations that adjust for skewness, kurtosis, and non-normal return characteristics.",
            "components": {
              "distribution_transformation_function": {
                "definition": "T(X) transforms raw return distribution X into a more Gaussian-like distribution Y = T(X).",
                "examples": [
                  "Box–Cox transformation",
                  "Johnson SU transformation",
                  "Log-sinh transformation"
                ],
                "role": "Corrects heavy tails and skewness prior to variance–covariance VaR computation."
              },
              "variance_covariance_framework": {
                "definition": "Use mean µ, variance σ², and correlation matrix Σ to compute parametric VaR.",
                "formula": "VaR = µ + z_α σ for single asset; portfolio VaR = sqrt(wᵀ Σ w).",
                "role": "Produces fast, closed-form VaR under transformed distribution."
              },
              "inverse_transformation": {
                "definition": "Map VaR computed in transformed space back to original scale via T⁻¹.",
                "role": "Ensures VaR remains interpretable in original financial units."
              }
            },
            "procedure": [
              "Collect historical return data.",
              "Apply transformation T to reduce skewness and correctly shape tails.",
              "Compute mean vector µ and covariance matrix Σ in transformed space.",
              "Calculate analytical VaR using parametric variance–covariance method.",
              "Apply inverse transformation to obtain VaR in original scale.",
              "Validate transformed VaR through backtesting and distributional checks."
            ],
            "output": "Analytically computed VaR that adjusts for non-normality using distributional transformations.",
            "advantages": [
              "Improves VaR accuracy for skewed or heavy-tailed returns.",
              "Closed-form computation remains extremely fast.",
              "Compatible with traditional covariance-based risk engines.",
              "Provides interpretable and easily implementable risk adjustments."
            ]
          },
    
          "Variance_Covariance_VaR_Module": {
            "name": "Variance–Covariance VaR Calculation Module",
            "purpose": "Compute VaR analytically using mean, variance, and covariance matrices (classic delta–normal method).",
            "components": {
              "mean_estimation": {
                "definition": "µ = average return for asset or portfolio."
              },
              "volatility_estimation": {
                "definition": "σ = standard deviation of asset or portfolio returns."
              },
              "correlation_structure": {
                "definition": "Σ = covariance matrix linking asset dependencies."
              },
              "confidence_interval": {
                "definition": "z_α = quantile of assumed standard normal distribution.",
                "examples": [
                  "z_0.95 ≈ 1.645",
                  "z_0.99 ≈ 2.33"
                ]
              }
            },
            "procedure": [
              "Estimate µ and σ for individual assets.",
              "Construct Σ for multi-asset portfolio.",
              "Compute portfolio volatility using sqrt(wᵀ Σ w).",
              "Multiply by z_α to obtain VaR at confidence level α.",
              "Adjust results for non-normality via transformation module."
            ],
            "advantages": [
              "Very fast computation.",
              "Extremely common in industry VaR engines.",
              "Easily extended to risk budgeting and shock scenario analysis."
            ],
            "limitations": [
              "Standard method assumes normality.",
              "Fails under heavy-tailed or asymmetric distributions without transformation."
            ]
          },
    
          "Transformation_Selection_Framework": {
            "name": "Transformation Selection Framework",
            "purpose": "Identify appropriate distributional transformation T(x) to optimize VaR accuracy.",
            "criteria": [
              "Reduction of skewness and kurtosis",
              "Improved normality tests (Shapiro–Wilk, Anderson–Darling)",
              "Improved tail fit",
              "Stability of covariance matrix after transformation",
              "Computational tractability"
            ],
            "procedure": [
              "Evaluate multiple transformation families (Box–Cox, Johnson, log-sinh).",
              "Select transformation that yields near-Gaussian behavior.",
              "Fit transformation parameters via maximum likelihood.",
              "Validate using Q–Q plots and distributional tests."
            ],
            "advantages": [
              "Adaptable to different asset classes.",
              "Improves robustness under turbulence or extreme markets.",
              "Provides systematic framework for enhanced analytical VaR."
            ]
          },
    
          "Backtesting_and_Validation": {
            "name": "Backtesting Framework for Analytical Transformed VaR",
            "purpose": "Evaluate transformed VaR accuracy compared to standard delta–normal VaR.",
            "methods": {
              "exception_ratio": "Compare observed exceedances vs expected exceedances.",
              "Kupiec_test": "Tests unconditional coverage.",
              "Christoffersen_test": "Tests independence of exceedances.",
              "tail_risk_alignment": "Check alignment of predicted vs empirical tail loss."
            },
            "findings": [
              "Transformational VaR significantly outperforms classical VCVaR under heavy-tailed data.",
              "Tail alignment improves for equity and FX series.",
              "Model achieves consistent backtest performance across multiple datasets."
            ]
          }
        }
      }
    },
    
    {
      "id": 29,
      "title": "Toward pricing financial derivatives with an IBM quantum computer",
      "authors": "Martin, Candelas et al.",
      "year": 2021,
      "source": "Physical Review Research",
      "url": "https://link.aps.org/doi/10.1103/PhysRevResearch.3.013167",
      "performance-metrics": [
        "Pricing Accuracy vs Classical Benchmark",
        "Payoff Expectation Estimation Error",
        "Quantum Sampling Variance",
        "Noise Robustness on IBM Quantum Devices",
        "Convergence Rate of Quantum Estimators",
        "Circuit Execution Fidelity",
        "Resource Scaling (Qubits vs Pricing Dimension)",
        "State Preparation Error",
        "Variance Reduction from Quantum PCA",
        "Gate Depth Efficiency in Derivative Pricing Circuits"
      ],
      "tools-used-in": [
        "Quantum-PCA-(qPCA)-for-Factor-Risk-Analysis"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Derivative_Pricing_Framework": {
            "name": "Quantum Framework for Pricing Financial Derivatives",
            "purpose": "Price European-style financial derivatives on NISQ hardware using quantum state preparation, amplitude estimation, and dimensionality reduction techniques such as quantum PCA.",
            "components": {
              "quantum_state_encoding": {
                "definition": "Map underlying asset price distribution into quantum amplitudes.",
                "methods": [
                  "Lognormal approximation loading",
                  "Brownian increment mapping",
                  "Circuit-based amplitude loading"
                ],
                "role": "Represents stochastic asset dynamics needed for derivative payoff computation."
              },
              "payoff_operator": {
                "definition": "U_f encodes f(S_T), the derivative payoff at maturity.",
                "examples": [
                  "European call payoff: max(S_T − K, 0)",
                  "Put payoff",
                  "Digital option payoff"
                ],
                "role": "Transforms state amplitudes so expectation equals option price."
              },
              "quantum_expectation_estimator": {
                "definition": "Use QAE-like estimators to compute E[f(S_T)].",
                "role": "Reduces sampling complexity compared to classical Monte Carlo."
              }
            },
            "procedure": [
              "Encode stochastic underlying asset distribution into a quantum register.",
              "Apply payoff operator to encode f(S_T).",
              "Use amplitude estimation or sampling to estimate expected payoff.",
              "Discount expectation to obtain derivative price.",
              "Compare results across classical and quantum methods."
            ],
            "output": "Estimated derivative price with quantum-assisted variance reduction.",
            "advantages": [
              "Quadratic sampling speedup (ideal theoretical QAE).",
              "Demonstrates feasibility of quantum pricing pipelines on IBM hardware.",
              "Allows integration of PCA techniques for dimensionality reduction."
            ]
          },
    
          "Quantum_PCA_Factor_Reduction": {
            "name": "Quantum PCA for Dimensionality Reduction in Derivative Pricing",
            "purpose": "Reduce the number of correlated risk factors (e.g., Brownian increments or volatility components) using eigenvalue decomposition on quantum states.",
            "components": {
              "covariance_matrix_encoding": {
                "definition": "Encode covariance Σ of asset returns or Brownian factors into a density matrix ρ.",
                "role": "Enables quantum PCA to extract principal components."
              },
              "quantum_phase_estimation_subroutine": {
                "definition": "Estimates eigenvalues/eigenvectors of ρ.",
                "role": "Identifies dominant risk factors."
              },
              "factor_reduction": {
                "definition": "Keep top-k principal components capturing most variance.",
                "role": "Reduces number of qubits and circuit depth for pricing tasks."
              }
            },
            "procedure": [
              "Construct covariance matrix of market factors.",
              "Encode covariance into a quantum density matrix.",
              "Perform quantum PCA to extract leading eigencomponents.",
              "Use reduced factor set for pricing simulations.",
              "Recompute derivative payoffs under compressed distribution."
            ],
            "advantages": [
              "Supports high-dimensional derivative pricing tasks using fewer qubits.",
              "Improves noise tolerance through dimensionality reduction.",
              "Reduces circuit complexity and sampling variance."
            ]
          },
    
          "State_Preparation_for_Asset_Dynamics": {
            "name": "Quantum State Preparation for Asset Price Dynamics",
            "purpose": "Construct quantum states representing uncertain terminal prices or stochastic paths.",
            "methods": {
              "lognormal_distribution_loading": "Encode lognormal density used in Black–Scholes model.",
              "binomial_tree_encoding": "Map discrete tree nodes into quantum amplitudes.",
              "Gaussian_increment_encoding": "Represent Brownian increments as superpositions."
            },
            "procedure": [
              "Normalize asset distribution.",
              "Encode required time-step information into qubits.",
              "Ensure payoff operator matches state encoding format."
            ],
            "advantages": [
              "Supports European options, baskets, and multi-dimensional underlyings."
            ]
          },
    
          "Quantum_Amplitude_and_Sampling_Methods": {
            "name": "Quantum Sampling and Amplitude Estimation for Pricing",
            "purpose": "Estimate expected payoff with fewer samples than classical Monte Carlo.",
            "methods": {
              "standard_amplitude_estimation": "Ideal QAE with quadratic speedup.",
              "iterative_amplitude_estimation": "Practical, noise-tolerant version for NISQ.",
              "likelihood_amplitude_estimation": "Classically assisted QAE variant."
            },
            "procedure": [
              "Apply payoff operator.",
              "Run QAE or sampling estimator to compute mean payoff.",
              "Discount result to obtain option price."
            ],
            "advantages": [
              "Reduced sampling variance.",
              "Better scaling for multi-asset derivatives."
            ]
          },
    
          "Benchmarking_and_Performance_Analysis": {
            "name": "Performance Benchmarking on IBM Quantum Devices",
            "purpose": "Evaluate how well current NISQ hardware supports derivative pricing.",
            "metrics": {
              "pricing_error": "Difference vs Black–Scholes or high-precision Monte Carlo.",
              "circuit_depth_vs_accuracy": "Effect of depth on noise accumulation.",
              "hardware_noise_characteristics": "Readout error, decoherence, gate infidelity.",
              "resource_scaling": "Qubits and depth required for multi-factor pricing."
            },
            "findings": [
              "Quantum derivative pricing is feasible for small-scale cases.",
              "Noise limits deep QAE circuits; hybrid sampling methods perform better.",
              "Dimensionality reduction (via qPCA) significantly improves robustness.",
              "Performance improves as qubits and hardware calibration quality increase."
            ]
          }
        }
      }
    },
    
    {
      "id": 30,
      "title": "Quantum recurrent neural networks",
      "authors": "Kyriienko et al.",
      "year": 2021,
      "source": "Quantum Sci. Technol.",
      "url": "https://iopscience.iop.org/article/10.1088/2058-9565/ac1ab9",
      "performance-metrics": [
        "Sequence Prediction Accuracy",
        "Mean Squared Error (MSE)",
        "Training Convergence Rate",
        "Gate Fidelity of Recurrent Quantum Circuits",
        "Expressivity of Quantum Recurrent Layer",
        "Memory Retention Efficiency",
        "Gradient Stability During Training",
        "Noise Robustness on NISQ Devices",
        "Generalization Performance on Sequential Tasks",
        "Circuit Depth vs Forecast Accuracy Tradeoff"
      ],
      "tools-used-in": [
        "Quantum-LSTM-(Q-LSTM)"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Recurrent_Neural_Network": {
            "name": "Quantum Recurrent Neural Network (QRNN)",
            "purpose": "Extend recurrent neural networks into the quantum domain by using parameterized quantum circuits (PQCs) as trainable recurrent units for sequence modeling tasks.",
            "components": {
              "quantum_hidden_state": {
                "definition": "A set of qubits whose state encodes the recurrent memory h_t.",
                "role": "Acts as the quantum analog of classical hidden states.",
                "effect": "Allows storage and evolution of sequence information through quantum transformations."
              },
              "recurrent_unitary": {
                "definition": "U(θ) applied to both input-encoded qubits and hidden-state qubits.",
                "role": "Defines nonlinear transformation that evolves hidden states.",
                "effect": "Updates quantum memory based on both previous hidden state and current input."
              },
              "input_encoding_layer": {
                "definition": "Encoding classical input x_t into quantum states via rotations (e.g., RY(x_t)).",
                "role": "Allows classical time-series or sequential data to interact with quantum recurrent layer."
              }
            },
            "procedure": [
              "Encode input x_t into quantum rotation gates.",
              "Apply U(θ) to combined input + hidden qubits.",
              "Obtain updated hidden state via measurement or forward propagation.",
              "Iterate through sequence for t = 1…T.",
              "Use final hidden state for prediction or classification.",
              "Train parameters via hybrid gradient descent (parameter-shift rule)."
            ],
            "output": "Quantum-enhanced recurrent feature representation for sequence modeling.",
            "advantages": [
              "Potential quantum advantage for long-range dependency modeling.",
              "Quantum states provide richer expressive power than classical RNNs.",
              "Suitable for NISQ devices due to shallow architectures."
            ]
          },
    
          "Quantum_LSTM_Unit": {
            "name": "Quantum LSTM (Q-LSTM)",
            "purpose": "Quantum analog of classical LSTM with input, forget, and output gates implemented as parameterized quantum circuits.",
            "components": {
              "input_gate": {
                "definition": "Quantum circuit controlling how much new information enters the cell state.",
                "role": "Analog to sigmoid(input × weights) gate in classical LSTM."
              },
              "forget_gate": {
                "definition": "Quantum circuit determining which information to retain.",
                "role": "Controls memory decay."
              },
              "output_gate": {
                "definition": "Controls contribution of cell state to hidden output using measurement-driven gating."
              },
              "cell_state_update": {
                "definition": "Quantum unitary evolution applied to hidden and memory qubits.",
                "role": "Replaces classical arithmetic update with quantum transformations."
              }
            },
            "procedure": [
              "Encode input x_t and previous state h_{t-1}.",
              "Apply quantum circuits that simulate gate functions.",
              "Update quantum 'cell' state and hidden state.",
              "Proceed across sequence using hybrid quantum-classical feedback.",
              "Train using parameter-shift gradients."
            ],
            "advantages": [
              "Captures nonlinear temporal dynamics in higher-dimensional Hilbert space.",
              "Potentially reduces vanishing gradient problem.",
              "More expressive than simple QRNN architectures."
            ]
          },
    
          "Variational_Training_Framework": {
            "name": "Hybrid Variational Training Framework",
            "purpose": "Train QRNN and Q-LSTM models on classical hardware using quantum gradients.",
            "components": {
              "parameter_shift_gradient": {
                "definition": "∂⟨O⟩/∂θ = [⟨O⟩(θ + π/2) − ⟨O⟩(θ − π/2)] / 2",
                "role": "Computes exact gradients of PQCs."
              },
              "loss_function": {
                "definition": "Task-dependent loss (MSE, cross-entropy, etc.)."
              },
              "optimizer": {
                "methods": [
                  "Adam",
                  "SGD",
                  "RMSProp"
                ]
              }
            },
            "procedure": [
              "Run forward QRNN/Q-LSTM pass on quantum device or simulator.",
              "Compute loss from predictions.",
              "Estimate gradients using parameter-shift rule.",
              "Update parameters using classical optimizer.",
              "Repeat until convergence."
            ],
            "advantages": [
              "Scalable for hybrid workloads.",
              "Compatible with noisy quantum devices.",
              "Supports end-to-end differentiable training."
            ]
          },
    
          "Input_Encoding_and_Preprocessing": {
            "name": "Quantum Input Encoding Layer",
            "purpose": "Embed sequential classical time-series data into quantum states.",
            "methods": {
              "angle_encoding": "Encode each scalar x_t as a qubit rotation angle.",
              "amplitude_encoding": "Encode normalized vectors as amplitude distributions.",
              "IQP_feature_maps": "Embed data via highly expressive phase circuits."
            },
            "procedure": [
              "Normalize inputs.",
              "Select encoding method based on qubit budget.",
              "Embed into qubit rotations before recurrent unitary."
            ],
            "advantages": [
              "Preserves sequential structure.",
              "Facilitates interaction with quantum recurrent memory."
            ]
          },
    
          "Benchmarking_and_Performance_Analysis": {
            "name": "Benchmarking & Performance Analysis",
            "purpose": "Compare QRNN and Q-LSTM against classical RNN/LSTM architectures.",
            "metrics": {
              "forecast_accuracy": "MSE, RMSE, MAE across prediction tasks.",
              "temporal_modeling_strength": "Ability to capture long-range dependencies.",
              "noise_resilience": "Resistance to decoherence-induced degradation.",
              "circuit_cost": "Gate depth, qubit count, training time."
            },
            "findings": [
              "QRNNs offer competitive or superior performance on some sequential tasks.",
              "Quantum recurrence provides expressive advantage for nonlinear dynamics.",
              "Training remains feasible on NISQ hardware via shallow circuits."
            ]
          }
        }
      }
    },
    
    {
      "id": 31,
      "title": "Quantum kernel methods for financial datasets",
      "authors": "Kusumoto et al.",
      "year": 2021,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/2108.04534",
      "performance-metrics": [
        "Classification Accuracy",
        "ROC-AUC Score",
        "F1 Score",
        "Kernel Alignment Score",
        "Kernel Target Similarity",
        "Quantum Kernel Fidelity Error",
        "Generalization Gap",
        "Training Stability Under Noise",
        "Quantum Kernel Expressivity",
        "Support Vector Margin Width"
      ],
      "tools-used-in": [
        "Quantum-Support-Vector-Machine-(QSVM)"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Kernel_Methods": {
            "name": "Quantum Kernel Method",
            "purpose": "Use quantum circuits to compute kernel functions that map financial data into a high-dimensional quantum Hilbert space, enabling enhanced classification performance for nonlinearly separable datasets.",
            "components": {
              "feature_map": {
                "definition": "Parameterized quantum circuit encoding classical financial features into quantum states.",
                "examples": [
                  "ZZFeatureMap",
                  "IQP Feature Map",
                  "Pauli-based feature encodings"
                ],
                "role": "Encodes data such that inner products approximate nonlinear similarity."
              },
              "quantum_kernel_function": {
                "definition": "K(x, y) = |⟨ψ(x)|ψ(y)⟩|²",
                "role": "Provides nonlinear similarity measure computed via quantum fidelity."
              },
              "kernel_estimator": {
                "definition": "Circuit that prepares |ψ(x)⟩ and |ψ(y)⟩ and measures overlap.",
                "role": "Computes kernel matrix entries efficiently."
              }
            },
            "procedure": [
              "Encode financial samples x into quantum states |ψ(x)⟩.",
              "Compute pairwise fidelities between encoded states.",
              "Construct quantum kernel matrix K.",
              "Train classical SVM using K as the kernel.",
              "Evaluate performance on classification tasks (e.g., price direction, volatility regime)."
            ],
            "output": "Quantum-enhanced kernel matrix enabling QSVM classification.",
            "advantages": [
              "Higher expressivity than classical kernels (RBF, polynomial).",
              "Can capture complex nonlinear correlations in financial data.",
              "May provide quantum advantage in high-dimensional feature spaces."
            ]
          },
    
          "Quantum_Support_Vector_Machine": {
            "name": "Quantum Support Vector Machine (QSVM)",
            "purpose": "Use quantum-computed kernels inside a classical SVM to classify financial datasets.",
            "components": {
              "svm_dual_formulation": {
                "definition": "Lagrangian dual uses kernel matrix K for margin maximization.",
                "formula": "maximize αᵀ1 − ½ αᵀYK Y α",
                "role": "Finds optimal support vectors and decision boundary."
              },
              "kernel_matrix": {
                "definition": "Matrix K_ij = K(x_i, x_j) computed on quantum device.",
                "role": "Input to classical SVM solver."
              }
            },
            "procedure": [
              "Compute quantum kernel matrix K.",
              "Train classical SVM using K.",
              "Evaluate decision function on test samples.",
              "Compute classification metrics (accuracy, F1, AUC, etc.)."
            ],
            "advantages": [
              "Hybrid quantum-classical workflow appropriately handles NISQ limits.",
              "Allows quantum advantage without requiring full quantum classifiers."
            ]
          },
    
          "Financial_Input_Encoding": {
            "name": "Quantum Encoding of Financial Datasets",
            "purpose": "Transform raw financial data (prices, returns, volatility regimes) into quantum states.",
            "methods": {
              "angle_encoding": "Map each feature x_i to RY(x_i) rotation.",
              "amplitude_encoding": "Encode normalized feature vectors into state amplitudes.",
              "feature_map_structuring": "Use entangling circuits to capture correlations."
            },
            "procedure": [
              "Normalize financial features.",
              "Encode features into qubits using chosen feature map.",
              "Prepare entangled quantum states that represent correlated features."
            ],
            "advantages": [
              "Captures nonlinear dependencies in high-dimensional financial states.",
              "Supports regime classification, pattern detection, and anomaly detection."
            ]
          },
    
          "Kernel_Expressivity_and_Generalization": {
            "name": "Kernel Expressivity & Generalization Analysis",
            "purpose": "Analyze how well quantum kernels separate financial classes.",
            "metrics": {
              "kernel_alignment": "Measures how well quantum kernel correlates with labels.",
              "effective_dimension": "High ED indicates expressive quantum feature space.",
              "fidelity_spectrum": "Distribution of kernel eigenvalues affects generalization."
            },
            "findings": [
              "Quantum kernels outperform classical RBF in certain financial classification tasks.",
              "Feature map depth and entanglement significantly influence kernel quality.",
              "Noise can degrade performance unless circuits are shallow or error-mitigated."
            ]
          },
    
          "Model_Evaluation_Framework": {
            "name": "Evaluation Framework for Financial Quantum Kernel Models",
            "purpose": "Assess QSVM performance against classical baselines.",
            "methods": {
              "baselines": [
                "RBF-SVM",
                "Polynomial SVM",
                "Logistic Regression",
                "Random Forest"
              ],
              "performance_metrics": [
                "Accuracy",
                "Precision/Recall/F1",
                "AUC-ROC",
                "Confusion Matrix",
                "Margin Width",
                "Kernel Rank"
              ]
            },
            "findings": [
              "QSVM performs competitively or better on several noisy, nonlinear financial datasets.",
              "Quantum kernels show better separation of volatility regimes and price-movement labels.",
              "Shallow circuits provide best robustness on NISQ hardware."
            ]
          }
        }
      }
    },
    
    {
      "id": 32,
      "title": "An evolving objective function for improved variational quantum optimisation",
      "authors": "Kolotouros et al.",
      "year": 2021,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/2105.11766",
      "performance-metrics": [
        "Convergence Speed Under Objective Evolution",
        "Approximation Ratio (Energy Gap)",
        "Optimizer Stability",
        "Gradient Smoothness",
        "Cost Landscape Flatness Reduction",
        "Probability of Sampling Low-Energy States",
        "Noise Robustness of Optimization Process",
        "Performance Improvement vs Standard QAOA",
        "Circuit Depth vs Solution Quality",
        "Variational Parameter Sensitivity"
      ],
      "tools-used-in": [
        "Quantum-Approximate-Optimization-Algorithm-(QAOA)-for-CVaR-based-Portfolio-Optimization"
      ],
      "paper-details": {
        "algorithms": {
          "Evolving_Objective_Function": {
            "name": "Evolving Objective Function for Variational Quantum Optimization",
            "purpose": "Dynamically adjust the cost function during training to provide smoother optimization landscapes and improve convergence for variational algorithms like QAOA.",
            "components": {
              "cost_function_schedule": {
                "definition": "C(λ) = (1 − λ) C_simple + λ C_target, where λ increases over training.",
                "role": "Starts with an easier, smoother objective and gradually shifts to the true target objective.",
                "effect": "Improves optimizer navigation through barren plateaus and sharp cost landscapes."
              },
              "smooth_initial_objective": {
                "definition": "Simplified or relaxed Hamiltonian C_simple with fewer correlations.",
                "examples": [
                  "Weak coupling terms",
                  "Simplified QUBO penalty structure",
                  "Low-order interactions"
                ],
                "role": "Provides a well-behaved starting surface."
              },
              "target_objective": {
                "definition": "Full problem Hamiltonian C_target.",
                "role": "Ensures final solution accuracy.",
                "examples": [
                  "Portfolio variance Hamiltonian",
                  "Spin glass Hamiltonian",
                  "QUBO cost mapping"
                ]
              }
            },
            "procedure": [
              "Initialize QAOA parameters (γ, β) with λ = 0.",
              "Optimize using simplified objective C_simple.",
              "Gradually increase λ toward 1, updating parameters at each stage.",
              "Converge toward optimal solution of full Hamiltonian C_target.",
              "Sample circuit to obtain best-performing bitstring."
            ],
            "output": "Optimized variational parameters enabling better QAOA performance on hard optimization landscapes.",
            "advantages": [
              "Reduces barren plateau issues.",
              "Improves global optimization by guiding optimizer through smoother cost surfaces.",
              "Achieves higher-quality solutions vs standard QAOA."
            ]
          },
    
          "Application_to_QAOA": {
            "name": "Application of Evolving Objective to QAOA",
            "purpose": "Demonstrate improved performance on QAOA for combinatorial optimization problems, including financial portfolio optimization.",
            "components": {
              "qaoa_ansatz": {
                "definition": "Alternating application of problem Hamiltonian and mixer Hamiltonian.",
                "role": "Parameterizes search over quantum states.",
                "formula": "U(γ, β) = ∏_i e^{-iβ_i H_M} e^{-iγ_i H_C}"
              },
              "modified_cost_hamiltonian": {
                "definition": "H(λ) = (1 − λ) H_simple + λ H_target.",
                "role": "Used during early stages of training to reduce optimization difficulty."
              }
            },
            "procedure": [
              "Construct QAOA Hamiltonian using evolving objective schedule.",
              "Optimize parameters while λ increases gradually.",
              "Measure energy and evaluate improvement over standard QAOA.",
              "Use final parameters at λ = 1 for target solution."
            ],
            "advantages": [
              "Improved convergence behavior.",
              "Reduced optimizer sensitivity.",
              "Better approximation ratios in fewer iterations."
            ]
          },
    
          "Landscape_Analysis_and_Barren_Plateau_Mitigation": {
            "name": "Landscape Analysis & Barren Plateau Mitigation",
            "purpose": "Understand why evolving objectives ease optimization difficulties common in variational quantum algorithms.",
            "findings": [
              "Smooth objectives produce landscapes with larger gradients.",
              "Gradual shaping prevents optimizer from entering flat regions early.",
              "Improves trainability especially for deeper circuits or many-layer QAOA."
            ],
            "metrics": {
              "gradient_norm": "Measures vanishing gradient severity.",
              "Hessian_condition_number": "Indicates curvature smoothness.",
              "energy_landscape_roughness": "Quantifies ruggedness of C_target vs C_simple."
            }
          },
    
          "Parameter_Optimization_Framework": {
            "name": "Hybrid Optimization Framework With Evolving Objectives",
            "purpose": "Train variational parameters efficiently through classical optimization algorithms.",
            "methods": {
              "optimizers": [
                "COBYLA",
                "SPSA",
                "Nelder–Mead",
                "Gradient descent"
              ],
              "staged_training": "Reinitialize optimizer at different λ regions to avoid stagnation."
            },
            "procedure": [
              "Optimize parameters for current λ.",
              "Monitor gradient smoothness and energy values.",
              "Increase λ when convergence stalls or plateaus.",
              "Finalize training when λ = 1."
            ],
            "advantages": [
              "Stable parameter updates.",
              "Better final energy minima.",
              "Lower probability of getting stuck in local minima."
            ]
          },
    
          "Benchmarking_and_Performance_Comparison": {
            "name": "Benchmarking & Performance Comparison",
            "purpose": "Evaluate advantage of evolving objective over standard QAOA.",
            "metrics": {
              "approximation_ratio": "Energy of found solution vs optimal energy.",
              "convergence_rate": "Iterations to reach threshold energy.",
              "parameter_smoothness": "Variance of optimal parameters during training.",
              "sampling_probabilities": "Frequency of near-optimal solutions."
            },
            "findings": [
              "Evolving objectives outperform fixed-objective QAOA for multiple problem types.",
              "Major improvements seen in problems with rough cost landscapes.",
              "Technique generalizes well to other VQAs beyond QAOA."
            ]
          }
        }
      }
    },
    
    {
      "id": 33,
      "title": "Quantum RL with quantum circuits",
      "authors": "Jerbi et al.",
      "year": 2021,
      "source": "Quantum Sci. Technol.",
      "url": "https://iopscience.iop.org/article/10.1088/2058-9565/ac1d78",
      "performance-metrics": [
        "Cumulative Reward",
        "Policy Gradient Variance",
        "Sample Efficiency",
        "Quantum Circuit Fidelity",
        "Training Stability",
        "Return-to-Variance Ratio",
        "Action Selection Quality",
        "Convergence Speed",
        "Noise Robustness on NISQ Devices",
        "Expressivity of Quantum Policy Circuits"
      ],
      "tools-used-in": [
        "Quantum-Reinforcement-Learning-(QRL)"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Policy_Networks": {
            "name": "Quantum Policy Networks (QPNs)",
            "purpose": "Represent RL policies using parameterized quantum circuits that map classical observations to action probabilities.",
            "components": {
              "input_encoding": {
                "definition": "Embed environment states into quantum states via angle or amplitude encoding.",
                "role": "Convert classical observations into quantum representations."
              },
              "variational_circuit": {
                "definition": "Parameterized quantum gates that generate expressive policy states.",
                "examples": [
                  "Hardware-efficient ansatz",
                  "Entangling layers (CNOT/CZ)",
                  "Rotational gates RY, RZ, RX"
                ],
                "role": "Capture nonlinear decision boundaries."
              },
              "output_measurement": {
                "definition": "Measure qubits to obtain action probabilities P(a|s).",
                "role": "Enable sampling of stochastic actions used in RL."
              }
            },
            "procedure": [
              "Encode state s into qubits using chosen feature map.",
              "Apply variational quantum policy circuit.",
              "Measure qubits to sample an action.",
              "Interact with environment, observe reward.",
              "Update circuit parameters using quantum-policy gradient methods."
            ],
            "output": "Quantum-enhanced policy capable of reinforcement learning.",
            "advantages": [
              "High expressivity from quantum Hilbert space.",
              "Potential quantum advantage for complex environments.",
              "Structures enable trainability on NISQ devices."
            ]
          },
    
          "Quantum_Policy_Gradient_Method": {
            "name": "Quantum Policy Gradient RL",
            "purpose": "Train quantum policies using classical gradient estimators, similar to REINFORCE but with quantum circuits.",
            "components": {
              "policy_gradient": {
                "formula": "∇θ J = E[∇θ log πθ(a|s) R]",
                "definition": "Gradient of expected reward with respect to circuit parameters.",
                "role": "Core learning mechanism."
              },
              "parameter_shift_gradient": {
                "formula": "∂⟨O⟩/∂θ = [⟨O⟩(θ+π/2) - ⟨O⟩(θ−π/2)]/2",
                "role": "Allows exact gradient computation of quantum circuits."
              },
              "baseline_reduction": {
                "definition": "Subtract baseline b to reduce variance: ∇θ J = E[(R-b) ∇θ log π]",
                "role": "Improves sample efficiency."
              }
            },
            "procedure": [
              "Collect trajectories using quantum policy.",
              "Compute returns for each step.",
              "Evaluate gradient using parameter-shift rule.",
              "Update quantum circuit parameters via optimizer.",
              "Repeat until convergence."
            ],
            "advantages": [
              "Fully hybrid (quantum + classical) learning loop.",
              "Supports variational quantum policies on real hardware.",
              "Good convergence on small tasks even with noise."
            ]
          },
    
          "Quantum_Value_Function_and_Q_Circuits": {
            "name": "Quantum Q-function Estimation",
            "purpose": "Use quantum circuits to represent value functions or Q-functions.",
            "components": {
              "value_function_ansatz": {
                "definition": "Parameterized circuit approximating V(s) or Q(s,a).",
                "role": "Quantum analog of neural-network-based value estimators."
              },
              "temporal_difference_target": {
                "definition": "δ = r + γV(s') - V(s)",
                "role": "Used for updating parameters."
              }
            },
            "procedure": [
              "Encode state (and action) into quantum circuit.",
              "Measure circuit output to estimate value.",
              "Update via TD error or gradient steps."
            ],
            "advantages": [
              "Supports actor-critic quantum RL.",
              "Can reduce variance vs pure policy-gradient methods."
            ]
          },
    
          "Hybrid_Quantum_Classical_RL_Framework": {
            "name": "Hybrid RL Framework",
            "purpose": "Combine quantum policy/value circuits with classical RL components to allow training on realistic devices.",
            "components": {
              "classical_optimizer": [
                "Adam",
                "SGD",
                "COBYLA"
              ],
              "experience_buffer": "Optional replay memory for stabilizing learning.",
              "reward_processing": "Computing discounted returns, TD targets, baselines."
            },
            "procedure": [
              "Collect environment samples via quantum circuits.",
              "Process rewards with classical RL algorithms.",
              "Update quantum circuits using hybrid gradient rules.",
              "Iterate until desired performance."
            ],
            "advantages": [
              "Reduces quantum circuit calls.",
              "Allows scaling to more complex environments."
            ]
          },
    
          "Benchmarking_and_Performance_Analysis": {
            "name": "Performance Benchmarking for Quantum RL",
            "purpose": "Compare quantum-policy-based RL performance with classical RL baselines.",
            "metrics": {
              "reward_curve": "Cumulative reward over training episodes.",
              "sample_efficiency": "Reward improvement per trajectory.",
              "policy_quality": "Variance of selected actions; robustness under noise.",
              "hardware_metrics": "Gate depth, qubit count, error rates."
            },
            "findings": [
              "Quantum policies can outperform classical policies on some structured environments.",
              "Hybrid methods are more stable on NISQ hardware.",
              "Noise-mitigated circuits improve learning quality significantly."
            ]
          }
        }
      }
    },
    
    {
      "id": 34,
      "title": "ARIMA models - Forecasting: Principles and Practice",
      "authors": "Hyndman & Athanasopoulos",
      "year": 2021,
      "source": "OTexts",
      "url": "https://otexts.com/fpp3/arima.html",
      "performance-metrics": [
        "Mean Absolute Error (MAE)",
        "Mean Squared Error (MSE)",
        "Root Mean Squared Error (RMSE)",
        "Mean Absolute Percentage Error (MAPE)",
        "Akaike Information Criterion (AIC)",
        "Bayesian Information Criterion (BIC)",
        "Forecast bias",
        "Residual autocorrelation diagnostics",
        "Prediction interval coverage accuracy",
        "Convergence stability of parameter estimation"
      ],
      "tools-used-in": [
        "ARIMA-(AutoRegressive-Integrated-Moving-Average)"
      ],
      "paper-details": {
        "algorithms": {
          "ARIMA": {
            "name": "AutoRegressive Integrated Moving Average (ARIMA)",
            "purpose": "Perform time-series forecasting by modeling autocorrelation dynamics, differencing for stationarity, and moving average components.",
            "components": {
              "autoregressive_part": {
                "definition": "AR(p): Uses lagged observations X_{t-1}, X_{t-2}, ..., X_{t-p} to predict future values.",
                "role": "Captures momentum and autoregressive dependencies."
              },
              "integrated_part": {
                "definition": "I(d): Applies differencing ∇^d X_t to remove nonstationarity.",
                "role": "Transforms nonstationary time series to stationary."
              },
              "moving_average_part": {
                "definition": "MA(q): Models forecast error using past errors ε_{t-1}, ε_{t-2}, ..., ε_{t-q}.",
                "role": "Accounts for noise shocks in previous time steps."
              }
            },
            "procedure": [
              "Check for stationarity and apply differencing as needed.",
              "Identify optimal p, d, q using ACF/PACF plots and information criteria.",
              "Estimate model parameters using maximum likelihood or least squares.",
              "Fit ARIMA(p,d,q) model to time-series data.",
              "Validate residuals for randomness and no autocorrelation.",
              "Generate forecasts using model and compute prediction intervals."
            ],
            "output": "Future forecasted values with uncertainty intervals.",
            "advantages": [
              "Widely used and well-understood statistical forecasting method.",
              "Effective for linear temporal patterns.",
              "Good performance on low-noise time-series data."
            ],
            "limitations": [
              "Not suited for highly nonlinear or nonseasonal complex patterns.",
              "Sensitive to differencing and parameter selection.",
              "Performance degrades with high volatility or structural breaks.",
              "Requires stationarity assumption."
            ]
          },
    
          "Box_Jenkins_Methodology": {
            "name": "Box–Jenkins Methodology",
            "purpose": "Systematically identify, estimate, and validate ARIMA models.",
            "procedure": [
              "Model identification using ACF/PACF analysis.",
              "Estimation of model parameters.",
              "Diagnostic checking of residuals.",
              "Forecast generation and model refinement."
            ],
            "output": "Optimal ARIMA configuration for time-series forecasting."
          },
    
          "Seasonal_ARIMA_SARIMA": {
            "name": "Seasonal ARIMA (SARIMA)",
            "purpose": "Extend ARIMA to include seasonal components (P, D, Q).",
            "procedure": [
              "Apply seasonal differencing.",
              "Fit ARIMA parameters along with seasonal AR, I, MA terms.",
              "Validate using seasonal residual diagnostics."
            ],
            "output": "Enhanced forecasting for seasonal time-series."
          }
        }
      }
    },
    
    {
      "id": 35,
      "title": "QBM for financial regime detection",
      "authors": "Herr et al.",
      "year": 2021,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/2103.10953",
      "performance-metrics": [
        "Regime classification accuracy",
        "Log-likelihood of learned probability distribution",
        "Convergence rate of QBM-based unsupervised learning",
        "Stability of regime identification over time",
        "Detection sensitivity to transitions between market regimes",
        "Kullback–Leibler divergence between actual and learned distributions",
        "Robustness to market noise and data nonstationarity",
        "Quantum sampling efficiency vs classical methods",
        "Fidelity of state preparation for regime detection",
        "Clustering consistency across multiple runs"
      ],
      "tools-used-in": [
        "Quantum-Boltzmann-Machine-for-Market-Regime-Detection"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Boltzmann_Machine_for_Regime_Detection": {
            "name": "Quantum Boltzmann Machine (QBM) for Market Regime Detection",
            "purpose": "Model financial market states as quantum probability distributions to detect regime shifts using unsupervised quantum generative learning.",
            "components": {
              "hamiltonian_representation": {
                "definition": "H(θ) = Σ h_i Z_i + Σ J_{ij} Z_i Z_j, parameterized to learn data correlations.",
                "role": "Encodes energy distribution approximating market condition patterns."
              },
              "quantum_gibbs_sampling": {
                "definition": "ρ = e^{-βH} / Tr(e^{-βH})",
                "role": "Samples regime likelihoods through quantum thermal distribution."
              },
              "parameter_optimization": {
                "definition": "Gradient-based adjustment of Hamiltonian weights to fit observed data distributions.",
                "role": "Aligns the model structure with real market regime behavior."
              }
            },
            "procedure": [
              "Collect financial time-series data representing market indicators.",
              "Encode data distribution into basis for Hamiltonian parameter initialization.",
              "Prepare approximate quantum Gibbs state representing possible regimes.",
              "Use iterative sampling to update parameter set for distribution matching.",
              "Identify dominant probability clusters representing financial regimes.",
              "Track regime transitions across time using probability shifts."
            ],
            "output": "Detected market regimes and transition timing based on quantum probability structure.",
            "advantages": [
              "Captures complex nonlinear dependencies in regime dynamics.",
              "Potentially detects subtle shifts earlier than classical models.",
              "Improves sampling over classical Boltzmann approaches via quantum tunneling."
            ],
            "limitations": [
              "Hardware noise affects Gibbs state fidelity.",
              "High dependence on accurate qubit encoding and annealing schedule.",
              "Training convergence may be unstable for highly volatile financial series.",
              "Scalability limited by qubit number and connectivity constraints."
            ]
          },
    
          "Temporal_Regime_Transition_Modeling": {
            "name": "Temporal Modeling of Regime Transitions",
            "purpose": "Analyze sequential evolution of market states using QBM-based clustering.",
            "procedure": [
              "Map observed market data to QBM-generated regime probabilities.",
              "Track state likelihood changes over time.",
              "Identify regime transitions when probability shift exceeds threshold."
            ],
            "output": "Regime shift detection timeline based on probability change dynamics."
          },
    
          "Hybrid_Quantum_Classical_Training": {
            "name": "Hybrid Quantum-Classical Training for QBM",
            "purpose": "Train QBM with classical optimization using quantum-generated probability estimates.",
            "procedure": [
              "Generate quantum samples from Hamiltonian.",
              "Compute loss (negative log-likelihood).",
              "Use classical optimizer (e.g., gradient descent) to adjust parameters.",
              "Repeat until convergence for optimal regime classification."
            ],
            "output": "Optimized Hamiltonian parameters providing accurate regime learned distribution."
          }
        }
      }
    },
    
    {
      "id": 36,
      "title": "Multi-objective portfolio optimization on D-Wave",
      "authors": "Fung et al.",
      "year": 2021,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/2103.15912",
      "performance-metrics": [
        "Objective value improvement vs classical solver",
        "Trade-off efficiency between risk and return",
        "Constraint satisfaction rate (budget, diversification, regulatory)",
        "Solution feasibility under multi-objective formulation",
        "Time-to-solution using quantum annealing",
        "Stability of returned solutions across annealing runs",
        "Sampling probability of low-energy portfolio states",
        "Energy residual error post-annealing",
        "Impact of penalty parameter sensitivity on output quality",
        "Scalability with increasing number of assets"
      ],
      "tools-used-in": [
        "Quantum-Annealing-for-Multi-Objective-Portfolio-Construction"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Annealing_Multi_Objective_Portfolio": {
            "name": "Quantum Annealing for Multi-Objective Portfolio Optimization",
            "purpose": "Use D-Wave quantum annealer to optimize portfolio allocation by balancing multiple financial objectives (e.g., risk, return, diversification) encoded within the Hamiltonian.",
            "components": {
              "multi_objective_QUBO_mapping": {
                "definition": "Q = λ₁Σ - λ₂μ + λ₃Δ + penalty_terms, where Σ represents risk, μ return, and Δ diversification measure.",
                "role": "Balances different objectives through tunable weighting coefficients.",
                "importance": "Encodes multi-factor portfolio optimization into quadratic binary form."
              },
              "annealing_schedule": {
                "definition": "s(t) = (1 - t/T)H_init + (t/T)H_problem",
                "role": "Gradually shifts from trivial to problem Hamiltonian.",
                "advantage": "Utilizes quantum tunneling to explore trade-off frontier."
              }
            },
            "procedure": [
              "Define optimization objective combining risk, return, and additional constraints.",
              "Translate objective to QUBO matrix with tunable weight coefficients.",
              "Embed QUBO onto quantum annealing hardware (D-Wave).",
              "Run annealing with multiple shots to obtain candidate solutions.",
              "Postprocess results to filter feasible and Pareto-optimal portfolios.",
              "Assess trade-offs using classical financial metrics."
            ],
            "output": "Portfolio allocation vectors optimizing multiple financial criteria via quantum annealing.",
            "advantages": [
              "Allows simultaneous optimization of risk, return, and diversification.",
              "Efficiently explores combinatorial solution space using quantum tunneling.",
              "Natural formulation for constraint-inclusive optimization."
            ],
            "limitations": [
              "Performance highly dependent on penalty calibration.",
              "Hardware connectivity limits scalability for large portfolios.",
              "Quantum noise may affect solution feasibility.",
              "Requires extensive classical postprocessing to refine raw solution."
            ]
          },
    
          "Pareto_Front_Postprocessing": {
            "name": "Pareto Optimization Postprocessing",
            "purpose": "Identify non-dominated portfolio choices from quantum annealer outputs.",
            "procedure": [
              "Extract multiple solution candidates from annealer.",
              "Evaluate risk-return-diversification metrics for each.",
              "Eliminate dominated solutions falling outside Pareto frontier.",
              "Select optimal portfolio choices based on investor preferences."
            ],
            "output": "Set of Pareto-optimal portfolios derived from quantum-computed solutions."
          },
    
          "Hybrid_Quantum_Classical_Portfolio_Refinement": {
            "name": "Hybrid Quantum-Classical Refinement",
            "purpose": "Improve quantum-generated allocations using classical local optimization.",
            "procedure": [
              "Take best quantum candidate solution.",
              "Perform classical fine-tuning using gradient or heuristic search.",
              "Adjust continuous weights if necessary.",
              "Re-evaluate and validate against financial constraints."
            ],
            "output": "Refined portfolio allocation ready for deployment or simulation."
          }
        }
      }
    },
    
    {
      "id": 37,
      "title": "Scenario generation for market risk models using generative neural networks",
      "authors": "Flaig & Junike",
      "year": 2021,
      "source": "arXiv",
      "url": "https://arxiv.org/pdf/2109.10072",
      "performance-metrics": [
        "Distributional similarity to historical data (e.g., KL divergence, Wasserstein distance)",
        "Scenario realism evaluation via statistical tests (e.g., Jarque–Bera, Kolmogorov–Smirnov)",
        "Tail risk generation accuracy (extreme return coverage)",
        "Volatility clustering preservation",
        "Convergence stability of generative model training",
        "Backtesting accuracy for risk estimation models",
        "Scenario diversity and coverage of market regimes",
        "Prediction accuracy for stress-testing",
        "Time-to-convergence during model training",
        "Robustness against mode collapse in generative modeling"
      ],
      "tools-used-in": [
        "Quantum-Generative-Adversarial-Network-(QGAN)-for-Scenario-Generation"
      ],
      "paper-details": {
        "algorithms": {
          "Generative_Neural_Network_for_Scenario_Generation": {
            "name": "Generative Neural Network for Market Scenario Generation",
            "purpose": "Generate synthetic financial market scenarios for risk modeling and stress-testing by learning statistical patterns from historical data.",
            "components": {
              "generator_network": {
                "definition": "Transforms random noise into plausible financial scenarios.",
                "role": "Produces synthetic market states resembling historical distribution."
              },
              "discriminator_network": {
                "definition": "Classifies scenarios as real or synthetic.",
                "role": "Trains generator by providing adversarial feedback."
              },
              "loss_function": {
                "definition": "GAN-based adversarial loss or Wasserstein loss for improved training stability.",
                "role": "Encourages realistic scenario generation."
              }
            },
            "procedure": [
              "Collect and preprocess historical financial market data.",
              "Train generative model (e.g., GAN or VAE) using market indicators.",
              "Use discriminator to evaluate scenario authenticity.",
              "Iteratively refine generator using adversarial learning.",
              "Generate new market scenarios under normal and extreme conditions.",
              "Validate generated scenarios using statistical similarity and stress-testing frameworks."
            ],
            "output": "Synthetic market scenarios replicating realistic risk conditions.",
            "advantages": [
              "Can generate extreme and tail-risk scenarios beyond historical dataset.",
              "Captures nonlinear relationships and volatility patterns.",
              "Useful for risk projection and stress-testing in finance."
            ],
            "limitations": [
              "Susceptible to mode collapse during training.",
              "Requires large historical dataset.",
              "Generative modeling may produce statistically correct but economically implausible scenarios.",
              "Does not explicitly encode financial constraints unless incorporated manually."
            ]
          },
    
          "QGAN_for_Scenario_Generation": {
            "name": "Quantum Generative Adversarial Network (QGAN)",
            "purpose": "Quantum-inspired extension of GANs for scenario generation with potentially reduced sample complexity and enhanced expressiveness.",
            "procedure": [
              "Encode noise vector into quantum state.",
              "Apply variational quantum circuit to generate synthetic scenario representation.",
              "Measure quantum state and pass output into discriminator.",
              "Update quantum parameters using classical optimization."
            ],
            "output": "Quantum-enhanced synthetic risk scenarios for portfolio stress testing."
          },
    
          "Statistical_Validation_Framework": {
            "name": "Scenario Validation via Statistical Tests",
            "purpose": "Ensure generated scenarios align with historical and risk-based distribution profiles.",
            "procedure": [
              "Apply statistical similarity tests (e.g., Kolmogorov–Smirnov, Chi-square).",
              "Evaluate tail fit using Extreme Value Theory metrics.",
              "Backtest generated scenarios in risk models."
            ],
            "output": "Validated scenario set for integration in risk management models."
          }
        }
      }
    },
    
    {
      "id": 38,
      "title": "Mean-Variance-VaR portfolios: MIQP formulation and performance analysis",
      "authors": "Cesarone et al.",
      "year": 2021,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/2111.09773",
      "performance-metrics": [
        "Portfolio expected return",
        "Portfolio variance (risk)",
        "Value-at-Risk (VaR) at multiple confidence levels",
        "Efficient frontier positioning",
        "Trade-off evaluation between return, variance, and VaR",
        "Constraint satisfaction percentage for optimization (budget and allocation constraints)",
        "Solution time for MIQP solver",
        "Stability of optimal portfolio weights under stress scenarios",
        "Sensitivity of portfolio performance to VaR confidence changes",
        "Backtesting performance against historical datasets"
      ],
      "tools-used-in": [
        "Variance-Covariance-Value-at-Risk"
      ],
      "paper-details": {
        "algorithms": {
          "Mean_Variance_VaR_MIQP_Formulation": {
            "name": "Mean-Variance-VaR MIQP Optimization",
            "purpose": "Optimize portfolios using a mixed-integer quadratic programming (MIQP) framework that simultaneously considers expected return, variance, and Value-at-Risk.",
            "components": {
              "quadratic_objective": {
                "definition": "Minimize xᵀΣx (variance) or maximize μᵀx (return) subject to VaR constraint.",
                "role": "Models classic risk-return optimization."
              },
              "VaR_constraint": {
                "definition": "α·σ_p - μ_p ≤ VaR_threshold or mixed absolute deviation representation.",
                "role": "Ensures downside risk is controlled at target level."
              },
              "binary_variables": {
                "definition": "Binary selection for asset inclusion (integer variable formulation).",
                "role": "Allows buy/sell decisions and asset inclusion constraints."
              }
            },
            "procedure": [
              "Define expected returns and covariance matrix of assets.",
              "Specify VaR threshold and confidence level.",
              "Formulate quadratic objective with constraints as MIQP problem.",
              "Enforce investment constraints (budget, long-only, minimum allocation).",
              "Use integer solver to compute optimal allocation.",
              "Analyze trade-off among mean, variance, and VaR outcomes.",
              "Evaluate performance in historical and stressed market conditions."
            ],
            "output": "Optimal investment allocations balancing return, risk, and VaR-based downside protection.",
            "advantages": [
              "Explicitly incorporates tail risk measure (VaR).",
              "Enhances robustness compared to classical Markowitz models.",
              "Suitable for regulatory-compliant optimization."
            ],
            "limitations": [
              "Computationally intensive due to MIQP complexity.",
              "Requires accurate covariance and return estimation.",
              "VaR’s non-coherent properties may affect solution optimality.",
              "Binary variable formulation increases solver runtime."
            ]
          },
    
          "Variance_Covariance_Method_for_VaR": {
            "name": "Variance-Covariance Method for Value-at-Risk",
            "purpose": "Compute VaR using mean and standard deviation under normality assumption.",
            "procedure": [
              "Estimate expected return and volatility of portfolio.",
              "Use formula: VaR = Z_α · σ_p - μ_p.",
              "Integrate into MIQP optimization as constraint.",
              "Adjust weight allocation based on tail risk tolerance."
            ],
            "output": "Tail risk measure included in optimization model."
          },
    
          "MIQP_Solver_Optimization": {
            "name": "Mixed-Integer Quadratic Programming (MIQP) Solver",
            "purpose": "Solve combined risk-return-VaR objective with integer constraints.",
            "procedure": [
              "Define objective and constraints using quadratic form.",
              "Apply branch-and-bound or cutting-plane solver.",
              "Iteratively optimize weight allocation with feasibility checks."
            ],
            "output": "Precise allocation decisions respecting investment and risk limits."
          }
        }
      }
    },
    
    {
      "id": 39,
      "title": "Deep RL for quantitative trading: survey and results",
      "authors": "Zhang et al.",
      "year": 2020,
      "source": "AAAI",
      "url": "https://ojs.aaai.org/index.php/AAAI/article/view/6334",
      "performance-metrics": [
        "Cumulative strategy return",
        "Sharpe Ratio",
        "Maximum drawdown",
        "Sortino Ratio",
        "Calmar Ratio",
        "Annualized volatility",
        "Hit ratio (percentage of profitable trades)",
        "Training convergence rate",
        "Stability of cumulative reward during training",
        "Transaction cost-adjusted performance"
      ],
      "tools-used-in": [
        "Reinforcement-Learning-Trading-Agent"
      ],
      "paper-details": {
        "algorithms": {
          "Deep_RL_for_Trading": {
            "name": "Deep Reinforcement Learning Framework for Trading",
            "purpose": "Review and evaluate deep RL algorithms applied to quantitative trading, including portfolio management, market-making, and trade execution.",
            "components": {
              "state_space": {
                "definition": "Features capturing market information such as price history, technical indicators, and current portfolio position.",
                "role": "Represents environment at each time step for policy learning."
              },
              "action_space": {
                "definition": "Trading actions, such as buy, sell, hold, and allocation adjustment.",
                "role": "Determines execution decisions for the RL agent."
              },
              "reward_function": {
                "definition": "r_t = profit_t - λ * transaction_costs_t - risk_penalty_t",
                "role": "Trains agent to maximize long-term return while controlling risk and trading frequency."
              }
            },
            "procedure": [
              "Define market environment with state, action, and reward structure.",
              "Train RL agent using deep learning methods (e.g., DQN, PPO, A2C).",
              "Evaluate on historical backtesting data.",
              "Adjust hyperparameters to improve convergence and trading performance.",
              "Conduct sensitivity analysis for risk-adjusted return metrics.",
              "Compare performance vs benchmarks and classical financial strategies."
            ],
            "output": "Trained deep RL policy for automated trading or portfolio allocation.",
            "advantages": [
              "Learns adaptive trading strategies based on market behavior.",
              "Captures nonlinear and dynamic relationships.",
              "Can outperform rule-based and statistical models in certain conditions."
            ],
            "limitations": [
              "High risk of overfitting to historical market patterns.",
              "Sensitive to transaction costs and slippage.",
              "Requires robust market environment modeling and feature engineering.",
              "Policy performance may degrade in regime shifts."
            ]
          },
    
          "Policy_Gradient_and_Value_Based_Methods": {
            "name": "Policy Gradient and Value-Based RL Methods",
            "purpose": "Survey variations of RL models used for trading across multiple strategies.",
            "procedure": [
              "Apply DQN for discrete actions and value-based estimation.",
              "Use PPO or A3C for continuous action space allocation.",
              "Evaluate hybrid actor-critic architectures in trading context.",
              "Analyze learning performance against financial metrics."
            ],
            "output": "Comparative insights into RL algorithm suitability across trading tasks."
          },
    
          "Backtest_and_Performance_Evaluation": {
            "name": "Backtest and Risk-Adjusted Performance Evaluation",
            "purpose": "Test RL trading strategies over historical data using market simulation.",
            "procedure": [
              "Simulate trading with trained RL agent over market data.",
              "Compute risk-adjusted performance metrics.",
              "Evaluate robustness using transaction cost scenarios.",
              "Compare outcomes with buy-and-hold and momentum strategies."
            ],
            "output": "Performance evaluation of RL-based trading approaches."
          }
        }
      }
    },
    {
      "id": 40,
      "title": "Quantum algorithms for Monte Carlo pricing",
      "authors": "Stamatopoulos et al.",
      "year": 2020,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/2006.11382",
      "performance-metrics": [
        "Pricing estimation accuracy",
        "Quantum speedup factor vs classical Monte Carlo",
        "Variance reduction in expected payoff estimation",
        "Error bounds under amplitude estimation",
        "Confidence interval coverage accuracy",
        "Circuit depth performance comparison",
        "Sampling complexity reduction",
        "Sensitivity to underlying probability distribution",
        "Gate count efficiency vs classical simulation",
        "Convergence rate to true option price"
      ],
      "tools-used-in": [
        "Quantum-Amplitude-Estimation-for-Expected-Return"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Monte_Carlo_Pricing": {
            "name": "Quantum Monte Carlo Pricing with Amplitude Estimation",
            "purpose": "Use quantum amplitude estimation to compute expected financial payoffs (e.g., option pricing) with quadratic speedup compared to classical Monte Carlo.",
            "components": {
              "payoff_function_encoder": {
                "definition": "Encodes payoff function f(S) where S follows risk-neutral distribution.",
                "role": "Transforms payoff into quantum-measurable amplitude."
              },
              "quantum_amplitude_estimation": {
                "definition": "Estimates expectation ⟨f(S)⟩ using Grover-style iterations.",
                "role": "Achieves O(1/ε) sample complexity vs classical O(1/ε²)."
              },
              "oracle_and_reflection_operators": {
                "definition": "Unitary operators for marking and amplifying target amplitudes.",
                "role": "Improve precision of expectation estimation."
              }
            },
            "procedure": [
              "Define risk-neutral probability distribution for asset price S.",
              "Encode payoff function f(S) into quantum amplitude via quantum circuit.",
              "Apply quantum amplitude estimation (QAE) using iterative Grover-based sampling.",
              "Measure resulting quantum state and extract expected payoff estimate.",
              "Optionally, apply variance reduction techniques or classical postprocessing."
            ],
            "output": "Quantum-estimated expected payoff (e.g., option price).",
            "advantages": [
              "Quadratic speedup over classical Monte Carlo.",
              "Improved precision with fewer samples.",
              "Well-suited for high-dimensional risk pricing problems."
            ],
            "limitations": [
              "Requires deep quantum circuits with high qubit count.",
              "Sensitive to noise and circuit imperfections.",
              "Implementation constrained by current quantum hardware."
            ]
          },
    
          "QAE_Operator_Design": {
            "name": "Quantum Amplitude Estimation Operators",
            "purpose": "Implement Grover-style operators to enhance payoff amplitude precision.",
            "procedure": [
              "Construct unitary payoff oracle marking desirable payoff states.",
              "Build reflection operators around initial state.",
              "Apply repeated Grover iterations to amplify payoff probability."
            ],
            "output": "Amplified state with higher payoff amplitude for measurement."
          },
    
          "Hybrid_Quantum_Classical_Pricing_Procedure": {
            "name": "Hybrid Quantum-Classical Monte Carlo Pricing",
            "purpose": "Combine quantum amplitude estimation with classical Monte Carlo methods for practical implementation.",
            "procedure": [
              "Use classical Monte Carlo to simulate underlying asset.",
              "Apply quantum amplitude estimation for expected payoff refinement.",
              "Postprocess results classically for final pricing output."
            ],
            "output": "Hybrid pricing estimation with improved accuracy and scalability."
          }
        }
      }
    },
    
    {
      "id": 41,
      "title": "Quantum algorithms for Monte Carlo finance",
      "authors": "Stamatopoulos et al.",
      "year": 2020,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/2009.03842",
      "performance-metrics": [
        "Expected return estimation accuracy",
        "Risk (variance) estimation accuracy",
        "Quantum speedup over classical Monte Carlo (runtime comparison)",
        "Quadratic reduction in sampling complexity",
        "Error tolerance under amplitude estimation",
        "Gate depth and qubit requirement efficiency",
        "Robustness to noise and hardware imperfections",
        "Convergence rate to true financial metrics",
        "Simulation performance in high-dimensional asset models",
        "Hybrid integration effectiveness for risk and return computation"
      ],
      "tools-used-in": [
        "Quantum-Amplitude-Estimation-for-Expected-Return-and-Risk"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Monte_Carlo_Risk_Return": {
            "name": "Quantum Monte Carlo for Expected Return and Risk Estimation",
            "purpose": "Utilize amplitude estimation to simultaneously compute expected return and risk (variance) for financial assets using quantum simulation.",
            "components": {
              "quantum_distribution_sampler": {
                "definition": "Encodes asset price distribution into a quantum state |ψ⟩ using variational or probability loading techniques.",
                "role": "Basis for simulating financial random variables."
              },
              "payoff_encoding": {
                "definition": "Maps payoff f(S) and squared payoff f(S)² to amplitudes for expected return and risk calculation.",
                "role": "Allows simultaneous estimation of mean and variance."
              },
              "amplitude_estimation_for_risk_metrics": {
                "definition": "Quantum algorithm to estimate E[f(S)] and E[f(S)²] using amplitude estimation.",
                "role": "Computes risk (variance) via Var[f(S)] = E[f(S)²] - (E[f(S)])²."
              }
            },
            "procedure": [
              "Define risk-neutral probability distribution of asset prices.",
              "Encode payoff f(S) into quantum state via unitary transformation.",
              "Construct additional circuit encoding squared payoff for risk estimation.",
              "Apply amplitude estimation algorithm to obtain expected return and risk metrics.",
              "Optionally refine values using classical postprocessing (variance formula)."
            ],
            "output": "Quantum-estimated expected return and risk measures.",
            "advantages": [
              "Quadratic speedup over classical Monte Carlo.",
              "Simultaneous estimation of risk and return.",
              "Efficient for portfolio and option pricing with complex distributions."
            ],
            "limitations": [
              "Circuit depth and qubit requirements increase for multi-metric estimation.",
              "Noise and decoherence affect estimation precision.",
              "Requires approximation assumptions for distribution encoding."
            ]
          },
    
          "QAE_Multi_Metric_Computation": {
            "name": "Quantum Amplitude Estimation for Multiple Financial Metrics",
            "purpose": "Generalize amplitude estimation to calculate expectations for multiple risk-return components.",
            "procedure": [
              "Prepare quantum states encoding f(S) and auxiliary moments.",
              "Use controlled unitary transformations for respective encodings.",
              "Apply QAE iteratively with different measurement regimes.",
              "Derive combined risk and return metrics."
            ],
            "output": "Enhanced amplitude estimation for financial multi-objective analysis."
          },
    
          "Hybrid_Quantum_Classical_Finance_Workflow": {
            "name": "Hybrid Quantum-Classical Finance Workflow",
            "purpose": "Leverage classical Monte Carlo for distribution modeling and quantum circuits for precision enhancement via amplitude estimation.",
            "procedure": [
              "Simulate price paths or distribution using classical Monte Carlo.",
              "Feed statistical data into quantum circuit encoding.",
              "Execute quantum amplitude estimation for accuracy improvement.",
              "Combine outputs into final risk-return model."
            ],
            "output": "Hybrid financial analytics with reduced computational overhead."
          }
        }
      }
    }, 
    
    {
      "id": 42,
      "title": "Financial time series forecasting with deep learning: A systematic literature review",
      "authors": "Sezer et al.",
      "year": 2020,
      "source": "Expert Systems",
      "url": "https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.12302",
      "performance-metrics": [
        "Prediction accuracy (e.g., MAE, RMSE, MAPE)",
        "Directional accuracy (hit ratio)",
        "R-squared (R²) goodness of fit",
        "Sharpe Ratio of forecast-based trading strategies",
        "Annualized return based on predicted signals",
        "Volatility of strategy derived from forecasts",
        "Loss convergence rate during neural network training",
        "Stability across training-validation splits",
        "Computational efficiency (training time)",
        "Robustness of predictions under regime shifts"
      ],
      "tools-used-in": [
        "LSTM-(Long-Short-Term-Memory)-Neural-Network"
      ],
      "paper-details": {
        "algorithms": {
          "Deep_Learning_for_Time_Series_Forecasting": {
            "name": "Deep Learning Methods for Financial Time Series Forecasting",
            "purpose": "Analyze and summarize the use of deep learning techniques for predicting stock prices, returns, and other financial variables.",
            "components": {
              "data_preprocessing": {
                "definition": "Normalization, log returns computation, sliding window sequence generation.",
                "role": "Transforms raw financial data into model-ready input sequences."
              },
              "feature_engineering": {
                "definition": "Extraction of technical indicators and volatility features.",
                "role": "Enhances predictive power of the models."
              },
              "model_architectures": {
                "definition": "Includes CNNs, RNNs, LSTMs, GRUs, hybrid models, and attention mechanisms.",
                "role": "Enables learning temporal dependencies and nonlinear patterns."
              }
            },
            "procedure": [
              "Collect and preprocess financial time-series data.",
              "Generate sequential data format suitable for deep learning.",
              "Train selected neural architecture (e.g., LSTM) with target horizon forecasting.",
              "Evaluate predictive accuracy using statistical error metrics.",
              "Simulate trading strategy based on predictions and assess financial performance."
            ],
            "output": "Review-based conclusions on best-performing deep learning models for financial forecasting.",
            "advantages": [
              "Captures nonlinear and temporal dependencies better than classical models.",
              "High adaptability to different financial data structures.",
              "Effective with large historical data and high-dimensional inputs."
            ],
            "limitations": [
              "Risk of overfitting due to complex model structures.",
              "Less interpretability than statistical time-series models.",
              "Performance may deteriorate in unexpected market conditions.",
              "Requires extensive hyperparameter optimization."
            ]
          },
    
          "LSTM_for_Financial_Forecasting": {
            "name": "LSTM for Time Series Prediction",
            "purpose": "Use Long Short-Term Memory networks to capture long-range dependencies in financial time-series data.",
            "procedure": [
              "Feed sequential market data into LSTM cells.",
              "Compute cell state updates using input, forget, and output gates.",
              "Predict future price or return based on hidden state representation.",
              "Backpropagate errors using truncated backpropagation through time (TBPTT).",
              "Optimize using gradient descent."
            ],
            "output": "Predicted financial variables (returns, prices)."
          },
    
          "Hybrid_DL_Trading_Strategy_Analysis": {
            "name": "Hybrid Deep Learning and Trading Strategy Integration",
            "purpose": "Combine deep learning-based forecasts with trading rule-based decision systems.",
            "procedure": [
              "Generate buy/sell signals based on LSTM forecast output.",
              "Apply risk management constraints (stop-loss, max allocation).",
              "Backtest and evaluate performance using risk-adjusted metrics."
            ],
            "output": "Trading strategy derived from deep learning forecasts."
          }
        }
      }
    },

    {
      "id": 43,
      "title": "Practical limitations / hybrid approaches",
      "authors": "Schuld",
      "year": 2020,
      "source": "Quantum Inf. Process.",
      "url": "https://link.springer.com/article/10.1007/s11128-020-02974-x",
      "performance-metrics": [
        "Clustering accuracy using hybrid methods",
        "Circuit depth vs computational feasibility",
        "Quantum resource requirement (qubit count)",
        "Noise robustness in clustering results",
        "Scalability with number of data points",
        "Sample complexity efficiency",
        "Execution time vs classical clustering",
        "Impact of quantum–classical interface latency",
        "Stability of convergence under hardware noise",
        "Distance estimation precision in hybrid approach"
      ],
      "tools-used-in": [
        "Quantum-k-Means-Clustering"
      ],
      "paper-details": {
        "algorithms": {
          "Hybrid_Quantum_kMeans": {
            "name": "Hybrid Quantum–Classical k-Means Clustering",
            "purpose": "Address practical limitations in quantum clustering by combining quantum subroutines (e.g., distance estimation) with classical optimization to improve feasibility and efficiency.",
            "components": {
              "quantum_distance_estimator": {
                "definition": "Computes distances between data points using quantum inner product estimation.",
                "role": "Replaces classical distance calculation step.",
                "importance": "Reduces complexity for high-dimensional data."
              },
              "classical_assignment_step": {
                "definition": "Assigns data points to closest centroid using classical clustering logic.",
                "role": "Offloads heavy computational logic to classical processor.",
                "importance": "Ensures stability and avoids overhead."
              },
              "iterative_training_loop": {
                "definition": "Combines quantum distance output and classical centroid update iteratively.",
                "role": "Performs hybrid optimization for cluster convergence."
              }
            },
            "procedure": [
              "Encode data points onto quantum states.",
              "Use quantum subroutine to estimate distance between data and centroids.",
              "Assign clusters using classical logic.",
              "Update centroids classically.",
              "Iterate until convergence."
            ],
            "output": "Cluster assignments derived through hybrid quantum-classical workflow.",
            "advantages": [
              "Reduces quantum hardware dependency.",
              "Improves computational efficiency for high-dimensional data.",
              "Allows partial quantum advantage in clustering."
            ],
            "limitations": [
              "Quantum benefit limited due to classical overhead.",
              "Interface latency between quantum and classical phases.",
              "Error amplification due to quantum noise over iterations.",
              "Not fully scalable due to hybrid design constraints."
            ]
          },
    
          "Hybrid_Algorithmic_Design": {
            "name": "Hybrid Quantum Algorithm Design Framework",
            "purpose": "Propose architecture for performing partial quantum computations where classical algorithms remain superior for iterative steps.",
            "procedure": [
              "Identify algorithm stages suitable for quantum acceleration.",
              "Retain classical logic for control-heavy or data-intensive steps.",
              "Implement hybrid workflow for clustering or similar tasks.",
              "Benchmark hybrid vs purely classical approaches."
            ],
            "output": "Structured hybrid algorithm for improved performance and practicality."
          },
    
          "Quantum_Limitation_Analysis": {
            "name": "Analysis of Practical Quantum Limitations",
            "purpose": "Evaluate quantum algorithm constraints due to hardware and theoretical challenges.",
            "procedure": [
              "Analyze qubit requirement and noise threshold.",
              "Examine circuit depth impact on accuracy.",
              "Model influence of decoherence and gate errors.",
              "Study scalability using quantum hardware simulations."
            ],
            "output": "Understanding of limitations and guidance for hybrid-based solutions."
          }
        }
      }
    },

    {
      "id": 44,
      "title": "Transfer learning in hybrid quantum/classical models",
      "authors": "Mari et al.",
      "year": 2020,
      "source": "Quantum",
      "url": "https://quantum-journal.org/papers/q-2020-03-09-248/",
      "performance-metrics": [
        "Classification accuracy after transfer",
        "Reduction in training time compared to non-transfer learning",
        "Generalization performance on unseen target data",
        "Quantum circuit depth vs accuracy trade-off",
        "Gradient stability during fine-tuning phase",
        "Transfer efficiency (performance retained from source model)",
        "Resource usage optimization (number of trainable parameters)",
        "Noise robustness in transferred quantum circuits",
        "Fidelity of transferred quantum model state",
        "Convergence rate during target task fine-tuning"
      ],
      "tools-used-in": [
        "Quantum-Neural-Network-(QNN)"
      ],
      "paper-details": {
        "algorithms": {
          "Hybrid_Quantum_Classical_Transfer_Learning": {
            "name": "Hybrid Quantum-Classical Transfer Learning",
            "purpose": "Enable transfer of pre-trained classical model features into hybrid quantum-classical architectures to improve learning efficiency on new tasks.",
            "components": {
              "pretrained_classical_encoder": {
                "definition": "Classical neural network trained on a source dataset.",
                "role": "Extracts initial feature representation before quantum processing."
              },
              "quantum_feature_transformer": {
                "definition": "Variational quantum circuit applied to encoded feature vector.",
                "role": "Enhances feature mapping with quantum expressivity."
              },
              "fine_tuning_layer": {
                "definition": "Trainable parameters (quantum and/or classical) optimized for target dataset.",
                "role": "Adapts the hybrid model to new task characteristics."
              }
            },
            "procedure": [
              "Train classical neural network on source dataset.",
              "Freeze classical layers or partially fine-tune depending on transfer type.",
              "Embed intermediate representation into quantum circuit.",
              "Optimize hybrid model using variational methods on target dataset.",
              "Evaluate fine-tuned model performance via classification or regression metrics."
            ],
            "output": "Efficiently trained hybrid quantum-classical model adapted to target task using prior knowledge.",
            "advantages": [
              "Reduces training effort by leveraging pretrained classical models.",
              "Potentially improves performance on limited data scenarios.",
              "Enables early-layer classical processing to reduce quantum circuit complexity."
            ],
            "limitations": [
              "Transfer effectiveness depends on compatibility between source and target distributions.",
              "Quantum layers still constrained by depth and noise.",
              "Fine-tuning can introduce optimization instability (barren plateau risk)."
            ]
          },
    
          "QNN_Fine_Tuning": {
            "name": "Quantum Neural Network Fine-Tuning",
            "purpose": "Adjust parameters of pre-trained quantum circuit to improve target task performance.",
            "procedure": [
              "Initialize quantum circuit with transferred parameters or pre-trained weights.",
              "Apply variational parameter optimization using classical gradient methods.",
              "Use expectation value-based loss evaluation.",
              "Iteratively update parameters to minimize objective function."
            ],
            "output": "Fine-tuned QNN customized to target problem."
          },
    
          "Hybrid_Model_Optimization_Strategy": {
            "name": "Hybrid Optimization Strategy",
            "purpose": "Coordinate training of classical and quantum parameters in transfer learning.",
            "procedure": [
              "Assign learning rates separately for classical and quantum layers.",
              "Optimize classical layers first to stabilize representation.",
              "Gradually increase influence of quantum layers during training.",
              "Perform joint optimization in later training phase."
            ],
            "output": "Stable hybrid learning process with reduced risk of gradient vanishing."
          }
        }
      }
    },

    {
      "id": 45,
      "title": "Quantum-enhanced SARSA",
      "authors": "Lockwood & Meier",
      "year": 2020,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/2008.02823",
      "performance-metrics": [
        "Convergence rate of QRL-SARSA vs classical SARSA",
        "Expected cumulative reward across episodes",
        "Quantum speedup in Q-value update computation",
        "Policy stability under quantum-enhanced learning",
        "Variance reduction in temporal-difference error",
        "Sample efficiency (learning rate improvement)",
        "Action selection accuracy under quantum exploration",
        "Circuit depth vs learning performance trade-off",
        "Noise robustness of quantum-enhanced SARSA updates",
        "Generalization performance on stochastic environments"
      ],
      "tools-used-in": [
        "Quantum-Reinforcement-Learning-(QRL)"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Enhanced_SARSA": {
            "name": "Quantum-Enhanced SARSA",
            "purpose": "Integrate quantum computation into the SARSA reinforcement learning algorithm to accelerate temporal-difference learning and improve exploration.",
            "components": {
              "state_action_encoding": {
                "definition": "Encode state-action pairs into quantum states |s, a⟩.",
                "role": "Allows amplitude-based probability modeling for RL updates."
              },
              "quantum_value_estimator": {
                "definition": "Uses quantum superposition and amplitude encoding to approximate Q-values.",
                "role": "Speeds up computation of Q(s,a) predictions.",
                "advantage": "Reduces complexity of temporal-difference update steps."
              },
              "temporal_difference_update": {
                "definition": "Q(s, a) ← Q(s, a) + α[r + γQ(s′, a′) − Q(s, a)]",
                "role": "Quantum-enhanced classical update using fast Q-value approximation."
              }
            },
            "procedure": [
              "Initialize Q-values and policy π(s).",
              "Encode state-action pair into quantum register.",
              "Estimate Q(s, a) using quantum amplitude estimation or inner product.",
              "Perform action, observe reward r and next state-action pair.",
              "Compute temporal-difference error using quantum-supported computation.",
              "Update Q(s, a) using classical SARSA update rule.",
              "Iterate across episodes until convergence."
            ],
            "output": "Updated Q-table or policy with potential quantum-accelerated convergence.",
            "advantages": [
              "May provide computational speedup during Q-value prediction.",
              "Quantum-enhanced exploration using superposition of actions.",
              "Can improve SARSA convergence in complex environments."
            ],
            "limitations": [
              "NISQ hardware noise restricts practical performance.",
              "State-action encoding may introduce overhead.",
              "Quantum advantage depends on well-structured environments.",
              "Hybrid approach—requires both classical and quantum computation."
            ]
          },
    
          "Quantum_Policy_Exploration": {
            "name": "Quantum-Based Policy Exploration",
            "purpose": "Leverage superposition and measurement for enhanced exploration in reinforcement learning.",
            "procedure": [
              "Prepare superposition of possible actions for a given state.",
              "Measure action probabilities to derive exploration distribution.",
              "Use measurement result to sample next action.",
              "Update policy gradually using SARSA update logic."
            ],
            "output": "More diverse action selection improving exploration."
          },
    
          "Hybrid_QRL_Framework": {
            "name": "Hybrid Quantum Reinforcement Learning Framework",
            "purpose": "Combine quantum estimation techniques with classical RL algorithms.",
            "procedure": [
              "Use quantum circuits for efficient value estimation.",
              "Perform classical reward update rules.",
              "Cycle through decision–measurement–update steps.",
              "Compare hybrid agent performance vs classical approach."
            ],
            "output": "Hybrid RL agent with potentially reduced computation time and improved learning characteristics."
          }
        }
      }
    },

    {
      "id": 46,
      "title": "A Low Complexity Quantum Principal Component Analysis Algorithm",
      "authors": "He et al.",
      "year": 2020,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/2010.00831",
      "performance-metrics": [
        "Reduced circuit depth compared to conventional qPCA",
        "Computational complexity improvement (O(poly(log N))) vs standard qPCA",
        "Eigenvalue estimation accuracy",
        "Runtime efficiency for high-dimensional datasets",
        "Quantum resource utilization (qubit count reduction)",
        "Fidelity of approximate density matrix construction",
        "Stability of eigenvalue convergence under noise",
        "Comparison of reconstruction error vs classical PCA",
        "Scalability with increasing data dimensions",
        "Robustness to approximation errors during matrix exponentiation"
      ],
      "tools-used-in": [
        "Quantum-PCA-(qPCA)-for-Factor-Risk-Analysis"
      ],
      "paper-details": {
        "algorithms": {
          "Low_Complexity_qPCA": {
            "name": "Low Complexity Quantum Principal Component Analysis",
            "purpose": "Improve efficiency of qPCA by reducing the circuit depth and complexity, making it more suitable for implementation on NISQ hardware.",
            "components": {
              "approximate_density_matrix_encoding": {
                "definition": "Use low-rank approximation to encode covariance matrix in quantum form.",
                "role": "Avoids high-cost state preparation used in classical qPCA.",
                "advantage": "Reduces computational overhead significantly."
              },
              "hamiltonian_simulation_reduction": {
                "definition": "Efficient simulation of covariance matrix using low-complexity circuits.",
                "role": "Allows faster application of matrix exponentiation for eigenvalue extraction."
              },
              "eigenvalue_extraction": {
                "definition": "Phase estimation adapted to low-complexity qPCA framework.",
                "role": "Extracts principal eigenvalues representing factor loadings."
              }
            },
            "procedure": [
              "Preprocess classical data and generate approximate covariance matrix.",
              "Encode approximate covariance matrix into quantum state using low-rank methods.",
              "Apply simplified Hamiltonian simulation to approximate e^{-iΣt}.",
              "Use quantum phase estimation to retrieve principal eigenvalues.",
              "Project onto eigenvectors to obtain principal components.",
              "Extract reduced feature representation for analysis."
            ],
            "output": "Low-complexity quantum principal components approximating dominant variance factors.",
            "advantages": [
              "Reduced complexity compared to traditional qPCA approaches.",
              "More suitable for NISQ devices with limited qubit coherence.",
              "Lower circuit depth enhances practicality in current hardware.",
              "Maintains high accuracy for dominant eigenvalue extraction."
            ],
            "limitations": [
              "Accuracy depends on quality of low-rank approximation.",
              "Phase estimation remains sensitive to noise.",
              "Less effective when data covariance matrix is not well-approximated by low rank.",
              "Limited support for higher-order eigenstructure without additional cost."
            ]
          },
    
          "Approximate_Matrix_Exponentiation": {
            "name": "Approximate Matrix Exponentiation via Reduced Complexity Quantum Circuits",
            "purpose": "Accelerate qPCA by approximating matrix exponentials rather than computing exact Hamiltonian simulations.",
            "procedure": [
              "Express matrix exponential e^{-iΣt} using truncated series expansion.",
              "Construct quantum circuit implementing reduced simulation steps.",
              "Apply to approximate covariance state encoding.",
              "Extract eigenvalues via phase estimation methods."
            ],
            "output": "Fast approximation of covariance transformation for eigenvalue extraction."
          },
    
          "Quantum_Phase_Estimation_Adaptation": {
            "name": "Simplified Quantum Phase Estimation for Low-Complexity qPCA",
            "purpose": "Reduce number of controlled-U operations using an approximate algorithm suited for low-rank covariance representation.",
            "procedure": [
              "Prepare approximate eigenstate encoding.",
              "Apply reduced-depth phase estimation circuit.",
              "Measure phase corresponding to eigenvalues.",
              "Apply classical post-processing to determine principal components."
            ],
            "output": "Eigenvalue estimates with reduced circuit cost."
          }
        }
      }
    },

    {
      "id": 47,
      "title": "Empirical Asset Pricing via Machine Learning",
      "authors": "Gu et al.",
      "year": 2020,
      "source": "Review of Financial Studies",
      "url": "https://academic.oup.com/rfs/article/33/5/2223/5731311",
      "performance-metrics": [
        "Out-of-sample R² for return prediction",
        "Mean Squared Error (MSE) of forecasted returns",
        "Sharpe Ratio of ML-based portfolio strategies",
        "Variable importance ranking stability",
        "Directional accuracy of return prediction",
        "Cross-sectional predictive power (rank correlation)",
        "Economic value added vs traditional factor models",
        "Reduction in unexplained return variance",
        "Consistency of performance across time periods",
        "Return distribution properties (skewness, kurtosis) under ML forecasts"
      ],
      "tools-used-in": [
        "Random-Forest-Gradient-Boosted-Trees-(XGBoost)"
      ],
      "paper-details": {
        "algorithms": {
          "Machine_Learning_for_Asset_Pricing": {
            "name": "Machine Learning Framework for Asset Pricing",
            "purpose": "Use advanced ML algorithms (e.g., Random Forests, Gradient Boosting, Neural Networks) to predict cross-sectional stock returns using high-dimensional financial data.",
            "components": {
              "feature_construction": {
                "definition": "Use over 900 firm characteristics and transformations.",
                "role": "Provides rich feature inputs for model training.",
                "importance": "Captures nonlinear interactions beyond traditional linear models."
              },
              "tree_based_models": {
                "definition": "Random Forests and Gradient Boosted Trees (e.g., XGBoost).",
                "role": "Model nonlinear relationships and complex interactions.",
                "advantage": "Feature selection capability and robustness."
              },
              "regularization_and_validation": {
                "definition": "Regularized learning (e.g., shrinkage, early stopping) with cross-validation.",
                "role": "Prevent overfitting and improve out-of-sample generalization."
              }
            },
            "procedure": [
              "Collect high-dimensional firm-level financial data.",
              "Construct predictive features using past trading and fundamental indicators.",
              "Apply machine learning models (e.g., XGBoost, Random Forest) for return prediction.",
              "Use cross-validation for hyperparameter optimization.",
              "Rank assets based on predicted returns and form long–short portfolios.",
              "Evaluate model using statistical and economic performance metrics."
            ],
            "output": "ML-predicted stock returns and portfolio construction rules based on model outputs.",
            "advantages": [
              "Outperforms linear factor models (e.g., Fama–French) in predictive accuracy.",
              "Captures nonlinear factor interactions and conditional relationships.",
              "Provides robust predictions using high-dimensional data."
            ],
            "limitations": [
              "Interpretability is limited for deep ML models.",
              "May capture transient predictive signals leading to instability.",
              "Requires computationally heavy tuning and large datasets."
            ]
          },
    
          "XGBoost_for_Return_Prediction": {
            "name": "XGBoost-Based Return Prediction",
            "purpose": "Apply gradient-boosted decision trees to forecast asset returns and identify key predictive signals.",
            "procedure": [
              "Train XGBoost using return predictors and hyperparameter tuning.",
              "Use tree-based structures to model nonlinear dependencies.",
              "Generate forecasted return ranking for asset selection.",
              "Evaluate performance using predictive and economic metrics."
            ],
            "output": "Boosted ensemble model generating asset rankings based on expected returns."
          },
    
          "Portfolio_Construction_from_ML": {
            "name": "ML-Based Portfolio Construction",
            "purpose": "Translate model predictions into actionable trading strategies.",
            "procedure": [
              "Sort firms by predicted returns.",
              "Construct long–short portfolios based on return deciles.",
              "Backtest strategy under transaction cost assumptions.",
              "Evaluate Sharpe Ratio, information ratio, and drawdowns."
            ],
            "output": "Quantitative portfolio strategy informed by ML predictions."
          }
        }
      }
    },

    {
      "id": 48,
      "title": "Hybrid quantum-classical LSTM networks",
      "authors": "Chen et al.",
      "year": 2020,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/2009.01783",
      "performance-metrics": [
        "Prediction accuracy of Q-LSTM vs classical LSTM",
        "Mean Squared Error (MSE) on time-series forecasting",
        "Convergence speed during hybrid model training",
        "Parameter efficiency (weight count comparison)",
        "Quantum circuit depth impact on model performance",
        "Temporal dependency capture ability in sequential tasks",
        "Noise robustness of hybrid Q-LSTM",
        "Gradient stability and vanishing/exploding mitigation",
        "Cross-validation performance on test datasets",
        "Scalability with length of time-series input"
      ],
      "tools-used-in": [
        "Quantum-LSTM-(Q-LSTM)"
      ],
      "paper-details": {
        "algorithms": {
          "Hybrid_Quantum_Classical_LSTM": {
            "name": "Hybrid Quantum-Classical LSTM (Q-LSTM)",
            "purpose": "Integrate quantum computation into recurrent deep learning for improved sequential modeling and temporal pattern detection.",
            "components": {
              "quantum_cell_layer": {
                "definition": "Quantum variational circuit replacing or augmenting LSTM hidden state transformation.",
                "role": "Provides nonlinear entanglement-driven transformation in sequential data process.",
                "advantage": "Potentially increases expressiveness of memory representation."
              },
              "classical_lstm_cell": {
                "definition": "Standard LSTM structure with input, forget, output gates.",
                "role": "Maintains temporal memory using classical optimization.",
                "interaction": "Outputs passed into quantum feature transformation."
              },
              "quantum_feature_map": {
                "definition": "Encodes hidden states into a quantum state using rotation gates.",
                "role": "Enables quantum-enhanced state evolution.",
                "example": "Apply RY(h_t), RZ(h_t) for hidden state values h_t."
              }
            },
            "procedure": [
              "Input time-series data into classical LSTM pre-layer.",
              "Compute hidden and cell states using standard LSTM gating mechanisms.",
              "Encode hidden state into quantum register via parameterized rotations.",
              "Pass through variational quantum circuit to transform state representation.",
              "Measure qubits and decode outputs into classical format.",
              "Feed into next LSTM time-step or final prediction layer.",
              "Train hybrid architecture end-to-end using gradient-based optimization."
            ],
            "output": "Enhanced LSTM hidden state and final sequence predictions using quantum-driven nonlinear transformation.",
            "advantages": [
              "Potential to model complex temporal relationships more efficiently.",
              "Hybrid architecture permits partial deployment on NISQ hardware.",
              "Quantum variational circuit helps mitigate vanishing gradients.",
              "Can improve model expressiveness with fewer parameters."
            ],
            "limitations": [
              "Quantum layer performance highly dependent on circuit design.",
              "NISQ noise limits practical benefit in current hardware.",
              "Hybrid architecture adds training complexity.",
              "Scalability challenges when extending to long time-series data."
            ]
          },
    
          "Quantum_Feature_Transformation_for_Sequential_Data": {
            "name": "Quantum Feature Transformation for LSTM",
            "purpose": "Improve feature encoding using quantum state manipulation before recurrent processing.",
            "procedure": [
              "Encode input features or hidden states into qubits via rotation gates.",
              "Apply entangling and variational circuit transformations.",
              "Measure quantum state to retrieve high-dimensional mapped features.",
              "Inject into LSTM cell for improved temporal learning."
            ],
            "output": "Quantum-enhanced feature representation for sequential modeling."
          },
    
          "Hybrid_Training_Loop": {
            "name": "Hybrid Training of Q-LSTM",
            "purpose": "Train hybrid architecture using classical optimization with quantum circuit feedback.",
            "procedure": [
              "Forward propagate sequence through quantum-classical architecture.",
              "Compute loss function (e.g., MSE for prediction).",
              "Estimate gradients via parameter-shift rule for quantum components.",
              "Update both classical and quantum circuit parameters.",
              "Repeat until convergence."
            ],
            "output": "Optimized Q-LSTM model with integrated learning from quantum and classical resources."
          }
        }
      }
    },

    {
      "id": 49,
      "title": "QAOA for portfolio optimization on NISQ devices",
      "authors": "Barkoutsos et al.",
      "year": 2020,
      "source": "Quantum",
      "url": "https://quantum-journal.org/papers/q-2020-07-06-291/",
      "performance-metrics": [
        "Quantum vs classical portfolio optimization accuracy",
        "Convergence rate of QAOA",
        "Risk-adjusted return (Sharpe Ratio) of optimized portfolio",
        "Gate depth vs solution quality trade-off",
        "Energy objective reduction per QAOA iteration",
        "Noise robustness on NISQ devices",
        "Quantum resource utilization efficiency",
        "Comparison of optimization performance using different QAOA depths (p-parameter)",
        "Variance reduction from expected risk measure",
        "Sampling overhead and execution time on real quantum hardware"
      ],
      "tools-used-in": [
        "Quantum-Mean-Variance-Optimization-(QMV)"
      ],
      "paper-details": {
        "algorithms": {
          "QAOA_for_Portfolio_Optimization": {
            "name": "Quantum Approximate Optimization Algorithm (QAOA) for Portfolio Optimization",
            "purpose": "Solve the mean–variance portfolio optimization problem using QAOA on near-term quantum hardware by formulating it as a QUBO optimization task.",
            "components": {
              "qubo_formulation": {
                "definition": "Minimize f(x) = xᵀΣx − μᵀx subject to constraints formulated as quadratic penalty terms.",
                "role": "Represents risk–return tradeoff in QAOA Hamiltonian."
              },
              "cost_hamiltonian": {
                "definition": "H_C = Σ_{i,j} Σ_{ij} Z_i Z_j − Σ_i μ_i Z_i + penalty_terms",
                "role": "Encodes portfolio optimization objective.",
                "importance": "Ground state corresponds to optimal asset allocation."
              },
              "mixer_hamiltonian": {
                "definition": "H_M = Σ_i X_i",
                "role": "Induces transitions for solution exploration.",
                "advantage": "Supports continuous asset allocation search."
              }
            },
            "procedure": [
              "Formulate mean–variance optimization as QUBO problem.",
              "Map QUBO to quantum cost Hamiltonian H_C.",
              "Initialize equal superposition state.",
              "Apply alternating evolution under cost and mixer Hamiltonians using QAOA:",
              " |ψ(β, γ)⟩ = Π exp(-iβ_p H_M) exp(-iγ_p H_C) |+⟩ⁿ",
              "Optimize angles (β, γ) using classical gradient-based methods.",
              "Sample optimal bitstring from final quantum state to determine portfolio allocation."
            ],
            "output": "Optimized portfolio weights based on sampled quantum state minimizing risk and maximizing return.",
            "advantages": [
              "Compatible with NISQ devices using shallow circuits.",
              "Combines classical and quantum optimization for scalable solutions.",
              "Can approximate combinatorial portfolio solutions efficiently."
            ],
            "limitations": [
              "Solution quality depends heavily on ansatz design and parameter tuning.",
              "Quantum hardware noise impacts final portfolio reliability.",
              "QUBO discretization may oversimplify continuous weights.",
              "Scaling limited by qubit connectivity and coherence time."
            ]
          },
    
          "Hybrid_QAOA_Optimization_Loop": {
            "name": "Hybrid QAOA Optimization Loop",
            "purpose": "Tune QAOA parameters using classical optimization techniques.",
            "procedure": [
              "Initialize β and γ parameters.",
              "Simulate QAOA circuit and evaluate expectation of H_C.",
              "Use classical optimizer (e.g., COBYLA, Nelder–Mead) to update parameters.",
              "Repeat until convergence or time constraint limit."
            ],
            "output": "Optimized (β, γ) values providing lowest cost function result."
          },
    
          "Postprocessing_and_Allocation_Translation": {
            "name": "Quantum Solution Postprocessing",
            "purpose": "Convert binary solution from QAOA into actionable portfolio weights.",
            "procedure": [
              "Identify optimal sampled bitstring.",
              "Map binary values to asset selection indicators.",
              "Optionally apply classical continuous optimization refinement.",
              "Generate market-executable asset allocation weights."
            ],
            "output": "Investment-ready asset weight vector derived from quantum solution."
          }
        }
      }
    },

    {
      "id": 50,
      "title": "Improving Variational Quantum Optimization using CVaR",
      "authors": null,
      "year": 2020,
      "source": "Quantum",
      "url": "https://quantum-journal.org/papers/q-2020-04-20-256/",
      "performance-metrics": [
        "CVaR-based objective minimization accuracy",
        "Convergence speed of variational optimization under CVaR loss",
        "Improvement in optimized energy values vs standard VQE/QAOA",
        "Robustness to quantum measurement noise",
        "Probability of sampling near-optimal solutions",
        "Optimization stability across iterations",
        "Circuit depth impact on CVaR optimization quality",
        "Sensitivity of objective to risk threshold α",
        "Variance reduction in optimization outputs",
        "Resource scaling with number of qubits and QAOA depth"
      ],
      "tools-used-in": [
        "Quantum-Approximate-Optimization-Algorithm-(QAOA)-for-CVaR-based-Portfolio-Optimization"
      ],
      "paper-details": {
        "algorithms": {
          "CVaR_Based_Variational_Optimization": {
            "name": "CVaR-Based Variational Quantum Optimization",
            "purpose": "Improve optimization performance of quantum variational algorithms (e.g., QAOA, VQE) by focusing on Conditional Value-at-Risk (CVaR) rather than expected value.",
            "components": {
              "cvar_loss_function": {
                "definition": "CVaR_α = E[X | X ≤ VaR_α], where X is objective cost measurement.",
                "role": "Focuses optimization on worst-performing samples.",
                "advantage": "Provides stability and improved robustness in noisy quantum settings."
              },
              "quantile_based_sampling": {
                "definition": "Keep only the lowest α percentile of measurement results.",
                "role": "Concentrates optimization effort on high-quality solution samples."
              },
              "variational_ansatz": {
                "definition": "Parameterized quantum circuit (e.g., QAOA or VQE) with tunable parameters.",
                "role": "Generates quantum states evaluated under CVaR objective."
              }
            },
            "procedure": [
              "Prepare parameterized quantum state using QAOA/VQE circuit.",
              "Perform multiple measurements to obtain objective cost samples.",
              "Compute VaR threshold for risk level α.",
              "Filter results to retain only lowest α quantile of outcomes.",
              "Calculate CVaR-based cost function using selected measurements.",
              "Update circuit parameters using classical optimization.",
              "Repeat until convergence of CVaR objective."
            ],
            "output": "Optimized parameters for variational quantum circuit minimizing CVaR-based objective.",
            "advantages": [
              "More stable optimization in presence of noise.",
              "Better solution quality for risk-sensitive optimization tasks.",
              "Improves convergence vs expectation-based loss functions.",
              "Useful for portfolio optimization and high-risk applications."
            ],
            "limitations": [
              "Requires more measurement samples due to quantile filtering.",
              "Performance heavily depends on appropriate risk level selection (α).",
              "Increased classical optimization complexity.",
              "Circuit depth still constrained by NISQ hardware limitations."
            ]
          },
    
          "QAOA_for_CVaR_Optimization": {
            "name": "QAOA for CVaR-Based Portfolio Optimization",
            "purpose": "Integrate CVaR risk minimization into QAOA for solving portfolio optimization problems.",
            "procedure": [
              "Map risk-return objective to Hamiltonian H_C including CVaR penalties.",
              "Use QAOA ansatz with alternating mixer and cost Hamiltonians.",
              "Perform measurement sampling and calculate CVaR objective.",
              "Optimize QAOA parameters using CVaR-based variational feedback."
            ],
            "output": "Quantum-enhanced portfolio allocation strategy minimizing conditional risk."
          },
    
          "Hybrid_Variational_Quantum_Classical_Optimization": {
            "name": "Hybrid Variational Optimization using CVaR",
            "purpose": "Train quantum circuit parameters using classical gradient or heuristic optimizers with CVaR feedback loop.",
            "procedure": [
              "Run quantum circuit to generate solution distribution.",
              "Calculate CVaR objective from measurement samples.",
              "Use classical optimizer (e.g., COBYLA, gradient descent) for parameter updates.",
              "Iterate optimization until CVaR convergence."
            ],
            "output": "Refined Hamiltonian parameters minimizing expected conditional tail loss."
          }
        }
      }
    },

    {
      "id": 51,
      "title": "Quantum optimization for finance with QUBO mappings",
      "authors": "Venturelli et al.",
      "year": 2019,
      "source": "Front. Phys.",
      "url": "https://www.frontiersin.org/articles/10.3389/fphy.2019.00031/full",
      "performance-metrics": [
        "Solution quality vs classical solvers (objective value)",
        "Time-to-solution on quantum annealer",
        "Risk–return tradeoff optimization efficiency",
        "Constraint satisfaction ratio (budget, volatility, allocation bounds)",
        "Convergence behavior across annealing runs",
        "Impact of QUBO penalty weights on solution feasibility",
        "Sampling success rate of near-optimal portfolios",
        "Noise robustness of financial optimization under quantum annealing",
        "Energy residual error after annealing process",
        "Scalability with portfolio size and asset universe"
      ],
      "tools-used-in": [
        "Quantum-Annealing-for-Multi-Objective-Portfolio-Construction",
        "Quantum-Mean-Variance-Optimization-(QUBO-Formulation)"
      ],
      "paper-details": {
        "algorithms": {
          "QUBO_Mapping_for_Portfolio_Optimization": {
            "name": "QUBO Mapping for Financial Portfolio Optimization",
            "purpose": "Convert finance optimization problems (e.g., portfolio allocation, risk minimization) into Quadratic Unconstrained Binary Optimization (QUBO) format suitable for quantum annealing.",
            "components": {
              "objective_matrix": {
                "definition": "Q matrix encodes tradeoff between return maximization and risk minimization.",
                "role": "Transforms classical mean–variance optimization objective into binary quadratic form."
              },
              "penalty_terms": {
                "definition": "Quadratic penalties introduced for budget, diversification, and regulatory constraints.",
                "role": "Ensures feasibility of binary portfolio solutions."
              }
            },
            "procedure": [
              "Define portfolio optimization objective using risk-return formulation.",
              "Translate continuous weight variables into binary representation.",
              "Construct QUBO matrix including objective and constraint penalties.",
              "Embed QUBO into quantum annealing hardware or simulation solver.",
              "Execute annealing to explore solution space for optimal portfolio configurations.",
              "Interpret resulting bitstring to determine asset selections."
            ],
            "output": "Binary-encoded optimal or near-optimal solution representing portfolio structure.",
            "advantages": [
              "Compatible with quantum annealing hardware for fast exploration.",
              "Supports multi-objective optimization via penalty-based modeling.",
              "Natural framework for combinatorial portfolio selection problems."
            ],
            "limitations": [
              "Binary encoding may oversimplify continuous allocations.",
              "Requires careful calibration of penalty coefficients.",
              "Scalability still limited by current annealer qubit connectivity."
            ]
          },
    
          "Quantum_Annealing_for_Portfolio_Optimization": {
            "name": "Quantum Annealing-Based Portfolio Optimization",
            "purpose": "Use quantum annealing to minimize QUBO-encoded financial optimization objective.",
            "procedure": [
              "Embed QUBO matrix onto D-Wave or simulated annealing backend.",
              "Initialize annealing schedule and run multiple optimization cycles.",
              "Measure lowest energy solution candidates.",
              "Filter infeasible outputs using classical postprocessing."
            ],
            "output": "Quantum-annealed portfolio configuration aiming to optimize risk-return profile."
          },
    
          "Hybrid_Optimization_Postprocessing": {
            "name": "Hybrid Quantum-Classical Postprocessing",
            "purpose": "Refine raw quantum annealing solutions using classical optimization.",
            "procedure": [
              "Take best sample from annealer.",
              "Validate constraint satisfaction.",
              "Apply classical local search or gradient refinement for continuous adjustment.",
              "Translate solution into actionable asset weight vector."
            ],
            "output": "Optimally refined portfolio allocation combining quantum and classical methods."
          }
        }
      }
    },

    {
      "id": 52,
      "title": "Quantum feature maps for machine learning",
      "authors": "Schuld & Killoran",
      "year": 2019,
      "source": "PRL",
      "url": "https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.122.040504",
      "performance-metrics": [
        "Classification accuracy using quantum kernel",
        "Kernel matrix expressibility",
        "Fidelity of quantum state encoding",
        "Feature separability in Hilbert space",
        "Quantum circuit depth vs prediction performance",
        "Effectiveness of quantum kernel vs classical kernel methods",
        "Learning stability across noise-injected simulations",
        "Scalability with number of input features",
        "Generalization performance on test datasets",
        "Variance in kernel similarity estimation"
      ],
      "tools-used-in": [
        "Quantum-Support-Vector-Machine-(QSVM)"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Feature_Map": {
            "name": "Quantum Feature Map",
            "purpose": "Embed classical data into high-dimensional quantum Hilbert space using parameterized circuits to enable quantum-enhanced feature representation for learning tasks.",
            "components": {
              "data_encoding_layer": {
                "definition": "Unitary U(x) that encodes classical input x via rotation gates (e.g., RZ, RX, or phase gates).",
                "role": "Transforms classical features into quantum states |φ(x)⟩."
              },
              "entangling_layer": {
                "definition": "Applies controlled gates to introduce nonlinear interactions between encoded features.",
                "role": "Enhances expressive power of quantum feature mapping."
              },
              "quantum_kernel_estimation": {
                "definition": "K(xᵢ, xⱼ) = |⟨φ(xᵢ)|φ(xⱼ)⟩|²",
                "role": "Measures similarity in quantum feature space."
              }
            },
            "procedure": [
              "Normalize classical input data.",
              "Apply unitary gates to encode features into amplitudes or rotations.",
              "Introduce entanglement to capture high-order correlations.",
              "Generate quantum state |φ(x)⟩ per input sample.",
              "Compute pairwise similarities via fidelity estimation for kernel formation.",
              "Use kernel matrix in learning algorithm (e.g., QSVM)."
            ],
            "output": "Quantum-embedded feature representation suitable for kernel-based learning.",
            "advantages": [
              "Allows learning in exponentially large feature spaces.",
              "Enhances ability to capture nonlinear decision boundaries.",
              "Compatible with hybrid quantum–classical ML workflows."
            ],
            "limitations": [
              "Quantum advantage depends on circuit depth and proper feature mapping.",
              "Noise and decoherence affect kernel estimation accuracy.",
              "Scalability limited by current hardware constraints."
            ]
          },
    
          "QSVM_Using_Quantum_Kernels": {
            "name": "Quantum Support Vector Machine (QSVM) using Quantum Kernels",
            "purpose": "Construct decision boundaries using kernel-based SVM with quantum-computed similarity matrices.",
            "procedure": [
              "Compute quantum kernel matrix using feature maps.",
              "Pass kernel into classical SVM optimization routine.",
              "Train SVM model on quantum-enhanced feature space.",
              "Evaluate performance on validation data."
            ],
            "output": "QSVM classifier exploiting quantum-encoded features."
          },
    
          "Kernel_Similarity_Estimation": {
            "name": "Kernel Similarity Estimation using Quantum Circuits",
            "purpose": "Estimate inner product between quantum states for similarity measurement.",
            "procedure": [
              "Prepare states |φ(xᵢ)⟩ and |φ(xⱼ)⟩ using feature map.",
              "Apply inverse circuit to enable state overlap measurement.",
              "Measure output probability to approximate kernel value."
            ],
            "output": "Estimated similarity score forming quantum kernel entry."
          }
        }
      }
    },

    {
      "id": 53,
      "title": "Quantum circuits as universal approximators",
      "authors": "Schuld & Killoran",
      "year": 2019,
      "source": "PRL",
      "url": "https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.122.040801",
      "performance-metrics": [
        "Approximation error between target and learned function",
        "Expressibility of variational quantum circuits",
        "Convergence rate of parameter optimization",
        "Circuit depth vs approximation capability trade-off",
        "Generalization ability of quantum neural networks",
        "Robustness to quantum noise during learning",
        "Probability of successfully learning target function",
        "Gradient variance during parameter updates",
        "Performance comparison of QNNs vs classical neural networks",
        "Fidelity of output state relative to target distribution"
      ],
      "tools-used-in": [
        "Quantum-Neural-Network-(QNN)"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Circuit_Universal_Approximation": {
            "name": "Quantum Circuit as Universal Approximator",
            "purpose": "Demonstrate that variational quantum circuits (QNNs) can approximate arbitrary functions, similar to classical neural networks.",
            "components": {
              "variational_parameterized_circuit": {
                "definition": "U(θ) = Π exp(iθ_k H_k), a layered ansatz with trainable parameters.",
                "role": "Represents hypothesis space for function learning.",
                "importance": "Equivalent to neural network weights."
              },
              "feature_encoding": {
                "definition": "Encodes input x into quantum state using data-dependent gates.",
                "role": "Maps classical function domain into quantum Hilbert space."
              },
              "function_approximation_output": {
                "definition": "Measurement of quantum state expectation values to represent learned function.",
                "role": "Generates model output from circuit evaluation."
              }
            },
            "procedure": [
              "Encode classical input x into quantum state via rotation or amplitude encoding.",
              "Apply parametrized circuit layers to transform input state.",
              "Measure qubits and interpret measurement values as model output.",
              "Compute loss between predicted and target function values.",
              "Update circuit parameters using classical optimization.",
              "Iterate until function approximation error is minimized."
            ],
            "output": "Quantum circuit modeling target classical function.",
            "advantages": [
              "Quantum circuits can express functions beyond classical models due to large Hilbert space.",
              "Potential for exponential expressivity with fewer parameters.",
              "Links QNNs to classical universal approximation theory."
            ],
            "limitations": [
              "Training complexity increases with circuit depth.",
              "Barren plateaus can hinder optimization.",
              "Susceptible to quantum decoherence.",
              "Practical implementation limited by quantum hardware capability."
            ]
          },
    
          "Variational_Quantum_Circuit_Optimization": {
            "name": "Variational Quantum Circuit Optimization",
            "purpose": "Use quantum-classical hybrid learning to optimize circuit parameters.",
            "procedure": [
              "Initialize parameters randomly.",
              "Run circuit for input encoding and transformation.",
              "Measure output and compute loss function.",
              "Estimate gradients via parameter-shift rule or numerical methods.",
              "Update parameters using classical optimizers.",
              "Repeat until optimal approximation is reached."
            ],
            "output": "Optimized QNN parameters providing best function approximation."
          },
    
          "Quantum_Feature_Expressivity_Analysis": {
            "name": "Expressivity Analysis of Quantum Circuits",
            "purpose": "Study how quantum feature maps and variational structure contribute to approximation power.",
            "procedure": [
              "Construct random parameterized circuit instances.",
              "Determine expressibility using projected state distribution.",
              "Analyze universality with respect to classical function representation."
            ],
            "output": "Theoretical validation that QNNs act as universal function approximators."
          }
        }
      }
    },

    {
      "id": 54,
      "title": "Deep Reinforcement Learning for Portfolio Management (DeepTrader)",
      "authors": "Li et al.",
      "year": 2019,
      "source": "AAAI",
      "url": "https://aaai.org/ojs/index.php/AAAI/article/view/3809",
      "performance-metrics": [
        "Cumulative portfolio return",
        "Sharpe Ratio of RL-based trading strategy",
        "Maximum drawdown",
        "Sortino Ratio",
        "Annualized volatility",
        "Hit ratio (percentage of profitable trades)",
        "Training convergence speed",
        "Reward function stability across episodes",
        "Turnover rate and transaction cost impact",
        "Out-of-sample backtesting performance"
      ],
      "tools-used-in": [
        "Reinforcement-Learning-Trading-Agent"
      ],
      "paper-details": {
        "algorithms": {
          "DeepTrader_RL_Framework": {
            "name": "DeepTrader Reinforcement Learning Framework",
            "purpose": "Use deep reinforcement learning to optimize multi-asset portfolio allocation dynamically based on market conditions.",
            "components": {
              "state_representation": {
                "definition": "Market indicators, historical price data, and current portfolio weights.",
                "role": "Provides environment context for trading decisions."
              },
              "action_space": {
                "definition": "Continuous reallocation of asset weights or trade execution actions.",
                "role": "Determines portfolio rebalancing decisions."
              },
              "reward_function": {
                "definition": "r_t = log(V_t / V_{t-1}) adjusted for transaction costs.",
                "role": "Encourages profitable decision making while penalizing excessive trading."
              },
              "deep_q_network": {
                "definition": "Neural network estimating Q-values for actions.",
                "role": "Guides policy based on expected reward."
              }
            },
            "procedure": [
              "Initialize portfolio and RL framework.",
              "Observe market state from historical and technical indicators.",
              "Select portfolio allocation action based on DQN policy.",
              "Execute allocation and observe resulting returns.",
              "Update Q-network using temporal-difference learning.",
              "Iterate across trading periods for policy optimization.",
              "Backtest and evaluate using financial risk and return metrics."
            ],
            "output": "Trained deep RL policy for dynamic portfolio management.",
            "advantages": [
              "Captures nonlinear relationships and time dependencies.",
              "Adaptive to changing market conditions.",
              "Outperforms static strategies and traditional models."
            ],
            "limitations": [
              "High sensitivity to transaction costs.",
              "Requires large training datasets and computational resources.",
              "Risk of overfitting to historical data.",
              "Less interpretable than traditional financial models."
            ]
          },
    
          "DQN_Based_Portfolio_Optimization": {
            "name": "Deep Q-Network (DQN) for Portfolio Allocation",
            "purpose": "Approximate optimal action-value function for trading decisions using deep learning.",
            "procedure": [
              "Define Q(s, a) function approximated by neural network.",
              "Select actions using ε-greedy policy.",
              "Update Q-values using Bellman Equation.",
              "Iterate training to maximize long-term cumulative reward."
            ],
            "output": "Optimal Q-policy guiding dynamic asset allocation."
          },
    
          "Experience_Replay_and_Target_Network": {
            "name": "Experience Replay and Target Network Stabilization",
            "purpose": "Improve training stability and reduce correlation in RL updates.",
            "procedure": [
              "Store previous experiences in memory buffer.",
              "Randomly sample past experiences for training.",
              "Use target network for delayed Q-value updates.",
              "Periodically update target network parameters."
            ],
            "output": "More stable and efficient RL model training."
          }
        }
      }
    },

    {
      "id": 55,
      "title": "Quantum generative models for learning distributions",
      "authors": "Khoshaman et al.",
      "year": 2019,
      "source": "npj Quantum Info",
      "url": "https://www.nature.com/articles/s41534-019-0187-2",
      "performance-metrics": [
        "Log-likelihood of learned probability distribution",
        "Kullback–Leibler (KL) divergence between true and learned distributions",
        "Training convergence rate of quantum generative model",
        "Sampling fidelity between model-generated and target samples",
        "Expressibility of quantum circuit used for generative modeling",
        "Energy landscape smoothness for quantum Boltzmann training",
        "Reconstruction error of probability amplitudes",
        "Mode coverage and ability to avoid mode collapse",
        "Robustness of learning under noise and decoherence",
        "Comparison of quantum vs classical generative model performance"
      ],
      "tools-used-in": [
        "Quantum-Boltzmann-Machine-for-Market-Regime-Detection"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Generative_Model": {
            "name": "Quantum Generative Model (QGM)",
            "purpose": "Learn complex probability distributions using quantum states whose amplitudes encode the target distribution.",
            "components": {
              "quantum_state_encoding": {
                "definition": "|ψ⟩ = Σ_x √p(x) |x⟩",
                "role": "Represents the target probability distribution in amplitude form.",
                "importance": "Allows quantum circuits to naturally model complex correlations."
              },
              "variational_circuit": {
                "definition": "Parameterised quantum circuit U(θ)",
                "role": "Transforms initial state into distribution approximating target.",
                "advantage": "Enables expressive non-classical correlations."
              },
              "training_objective": {
                "definition": "Maximize overlap between model distribution and target distribution using contrastive divergence or gradient-based optimization.",
                "role": "Guides model parameters toward accurate distribution learning."
              }
            },
            "procedure": [
              "Initialize a parameterized quantum circuit with random parameters.",
              "Prepare quantum state and generate sample distribution from measurement outcomes.",
              "Compute training loss using divergence between target and learned distribution.",
              "Update quantum circuit parameters using gradient descent or hybrid optimization.",
              "Iterate sampling and training until convergence.",
              "Validate model using likelihood-based metrics or classical benchmarks."
            ],
            "output": "Quantum state whose measurement distribution approximates the target distribution.",
            "advantages": [
              "Quantum models can represent highly correlated probability structures compactly.",
              "Potential for exponential speedup in sampling over classical models.",
              "Naturally suited for generative tasks requiring entanglement.",
              "Integrates seamlessly with quantum Boltzmann machines."
            ],
            "limitations": [
              "Training requires many samples and may suffer from barren plateaus.",
              "Hardware noise limits performance on current quantum devices.",
              "Estimating likelihood is computationally expensive."
            ]
          },
    
          "Quantum_Boltzmann_Machine": {
            "name": "Quantum Boltzmann Machine (QBM)",
            "purpose": "Model probability distributions using quantum Hamiltonians, enabling richer probabilistic modeling than classical Boltzmann machines.",
            "components": {
              "hamiltonian": {
                "definition": "H(θ) = Σ_i h_i Z_i + Σ_{i,j} J_{ij} Z_i Z_j",
                "role": "Defines energy landscape of quantum distribution.",
                "importance": "Quantum interactions allow non-classical correlations."
              },
              "thermal_state": {
                "definition": "ρ = e^{-βH} / Tr[e^{-βH}]",
                "role": "Encodes probability distribution in quantum thermal equilibrium."
              }
            },
            "procedure": [
              "Define Hamiltonian structure based on model architecture.",
              "Prepare thermal or Gibbs state corresponding to Hamiltonian.",
              "Generate samples from quantum system via measurement.",
              "Compute gradients using quantum expectation values.",
              "Iteratively update Hamiltonian parameters via optimization.",
              "Compare learned distribution to target distribution."
            ],
            "output": "Quantum probabilistic model encoded in a thermal state.",
            "advantages": [
              "Represents multimodal distributions well.",
              "Less prone to mode collapse than classical generative models.",
              "Encodes complex correlations through entanglement."
            ],
            "limitations": [
              "Thermal state preparation is difficult on real hardware.",
              "Training gradients may be noisy or computationally expensive.",
              "Scaling to large qubit systems is challenging."
            ]
          },
    
          "Sampling_and_Learning_Framework": {
            "name": "Hybrid Quantum-Classical Sampling and Learning Framework",
            "purpose": "Combine classical optimization with quantum sampling to train generative models efficiently.",
            "procedure": [
              "Use quantum hardware to sample from parameterized distributions.",
              "Compute classical loss function using sample statistics.",
              "Perform parameter updates using classical optimizers.",
              "Iterate between quantum sampling and classical updates.",
              "Evaluate performance against target distribution using likelihood metrics."
            ],
            "output": "Hybrid-trained generative model approximating target distribution."
          }
        }
      }
    },
    
    {
      "id": 56,
      "title": "q-means: quantum algorithm for unsupervised learning",
      "authors": "Kerenidis et al.",
      "year": 2019,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/1904.02260",
      "performance-metrics": [
        "Quantum runtime complexity (polylogarithmic speedup vs classical k-means)",
        "Clustering accuracy compared to classical k-means",
        "Success probability of distance estimation subroutine",
        "Stability of cluster assignments across quantum runs",
        "Fidelity of quantum state preparation for data encoding",
        "Error bounds on quantum distance estimation",
        "Convergence rate of q-means iterations",
        "Comparison of cluster center update accuracy vs classical algorithm",
        "Robustness of clustering under quantum noise",
        "End-to-end runtime reduction on large and high-dimensional datasets"
      ],
      "tools-used-in": [
        "Quantum-k-Means-Clustering"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_k_Means": {
            "name": "q-means (Quantum k-Means Algorithm)",
            "purpose": "Provide a quantum-accelerated version of k-means clustering using quantum state preparation, distance estimation, and amplitude techniques.",
            "components": {
              "quantum_data_encoding": {
                "definition": "Encode each data vector xᵢ into a normalized quantum state |xᵢ⟩ using QRAM or amplitude encoding.",
                "role": "Enables efficient computation of inner products and distances."
              },
              "quantum_distance_estimation": {
                "definition": "Use the swap test or related quantum subroutines to estimate ||xᵢ − μ_k||².",
                "role": "Assigns points to clusters by estimating distances to centroids.",
                "advantage": "Provides possible polynomial or exponential speedup in high dimensions."
              },
              "cluster_assignment": {
                "definition": "Assign data points to nearest centroid based on quantum-estimated distances.",
                "role": "Equivalent to E-step in classical k-means."
              },
              "centroid_update": {
                "definition": "Recompute cluster centers using weighted superpositions of assigned states.",
                "role": "Equivalent to M-step using quantum-assisted averaging."
              }
            },
            "procedure": [
              "Encode all data points into quantum states via QRAM or amplitude encoding.",
              "Initialize cluster centroids as quantum states.",
              "Estimate distances between each data point and each centroid using quantum distance estimation.",
              "Assign each point to the nearest cluster (quantum-assisted E-step).",
              "Update cluster centroids using quantum arithmetic and amplitude amplification.",
              "Repeat until stabilization of cluster assignments.",
              "Decode centroids if classical interpretation is required."
            ],
            "output": "Cluster assignments and quantum-derived centroids approximating classical k-means results.",
            "advantages": [
              "Potential exponential speedup in distance computation and centroid updates.",
              "Efficient for high-dimensional and large datasets.",
              "Uses amplitude encoding to compactly represent large datasets.",
              "Reduces overall runtime complexity significantly compared to classical k-means."
            ],
            "limitations": [
              "Requires QRAM for efficient data loading, which is not yet practical.",
              "Sensitive to quantum noise and decoherence during distance estimation.",
              "Centroid decoding may require classical post-processing.",
              "Speedup depends heavily on structure of dataset and quantum hardware conditions."
            ]
          },
    
          "Quantum_Distance_Estimation": {
            "name": "Quantum Distance Estimation Subroutine",
            "purpose": "Efficiently compute squared Euclidean distance between data vectors and centroids using quantum amplitude and overlap measurements.",
            "components": {
              "swap_test": {
                "definition": "Quantum algorithm that estimates inner product ⟨x | μ⟩.",
                "role": "Used to compute ||x − μ||² = ||x||² + ||μ||² − 2⟨x | μ⟩."
              }
            },
            "procedure": [
              "Prepare states |xᵢ⟩ and |μ_k⟩.",
              "Execute swap test to measure overlap ⟨xᵢ | μ_k⟩.",
              "Compute distance using quantum arithmetic circuits.",
              "Feed resulting distances into cluster assignment step."
            ],
            "output": "Estimated squared distance between point and centroid."
          },
    
          "Quantum_Centroid_Update": {
            "name": "Quantum Centroid Update Mechanism",
            "purpose": "Update cluster centroids using quantum superpositions and amplitude techniques.",
            "procedure": [
              "Generate superposition of all points assigned to each cluster.",
              "Apply quantum averaging routines to compute mean vector.",
              "Encode resulting centroid back into quantum state form.",
              "Proceed to next iteration until convergence."
            ],
            "output": "Updated quantum cluster centroids."
          }
        }
      }
    },
    
    {
      "id": 57,
      "title": "Portfolio rebalancing with QAOA",
      "authors": "Hodson et al.",
      "year": 2019,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/1911.06259",
      "performance-metrics": [
        "Approximation ratio of QAOA-generated portfolio vs classical optimum",
        "Convergence of QAOA energy minimization across iterations",
        "Portfolio variance reduction compared to classical solutions",
        "Expected return accuracy under QUBO encoding",
        "QAOA circuit depth vs solution quality trade-off",
        "Sampling fidelity for portfolio bitstring solutions",
        "Robustness of QAOA results under shot noise",
        "Time-to-solution comparison against classical optimizers",
        "Constraint satisfaction rate (budget, turnover constraints)",
        "Out-of-sample performance of QAOA-selected portfolios"
      ],
      "tools-used-in": [
        "Quantum-Mean-Variance-Optimization-(QMV)",
        "Quantum-Mean-Variance-Optimization-(QUBO-Formulation)"
      ],
      "paper-details": {
        "algorithms": {
          "QAOA_for_Portfolio_Rebalancing": {
            "name": "Quantum Approximate Optimization Algorithm (QAOA) for Portfolio Rebalancing",
            "purpose": "Solve portfolio optimization problems by encoding mean–variance objective into a QUBO Hamiltonian and using QAOA to approximate the optimal allocation.",
            "components": {
              "qubo_encoding": {
                "definition": "Encode portfolio weights as binary decision variables x ∈ {0,1} mapped into QUBO cost function.",
                "role": "Transforms portfolio selection into combinatorial optimization.",
                "importance": "Makes the problem compatible with quantum optimization methods."
              },
              "cost_hamiltonian": {
                "definition": "H_C = xᵀ Q x, where Q encodes return, variance, and constraints.",
                "role": "Represents the optimization objective in quantum form."
              },
              "mixer_hamiltonian": {
                "definition": "H_M = Σ_i X_i",
                "role": "Explores the feasible solution space by inducing bit flips."
              },
              "angle_parameters": {
                "definition": "γ (cost angles) and β (mixer angles)",
                "role": "Variational parameters optimized to minimize expected cost."
              }
            },
            "procedure": [
              "Formulate mean–variance optimization as a QUBO problem.",
              "Encode QUBO matrix Q into cost Hamiltonian H_C.",
              "Apply p-layer QAOA using alternating unitaries e^{-iγH_C} and e^{-iβH_M}.",
              "Sample output bitstrings representing possible portfolios.",
              "Evaluate classical performance of sampled portfolios.",
              "Optimize γ, β using classical machine learning or gradient-based updates.",
              "Select portfolio with lowest quantum energy (best cost)."
            ],
            "output": "Quantum-generated portfolio allocations approximating mean–variance optimal weights.",
            "advantages": [
              "Quantum-friendly formulation of portfolio optimization.",
              "Potential speedup for large-scale combinatorial financial problems.",
              "Flexible encoding allows handling constraints such as turnover and sector limits.",
              "QAOA offers near-term feasibility on NISQ hardware."
            ],
            "limitations": [
              "Performance depends strongly on QAOA depth p.",
              "Quantum noise and decoherence degrade approximation quality.",
              "Requires extensive classical optimization of variational parameters.",
              "Binary encoding may limit resolution of portfolio weights."
            ]
          },
    
          "QUBO_Portfolio_Encoding": {
            "name": "QUBO Formulation for Mean–Variance Optimization",
            "purpose": "Translate Markowitz mean–variance problem into binary optimization form suitable for quantum algorithms.",
            "components": {
              "binary_weights": {
                "definition": "Represent each asset’s weight via multiple binary variables (bit-depth encoding).",
                "role": "Allows discrete approximation of continuous portfolio weights."
              },
              "constraint_penalties": {
                "definition": "Penalty terms added to QUBO to enforce budget and allocation constraints.",
                "role": "Ensures feasible portfolio solutions."
              }
            },
            "procedure": [
              "Discretize portfolio weights into binary representations.",
              "Construct QUBO matrix with mean–variance objective and penalty terms.",
              "Pass QUBO to QAOA for quantum optimization.",
              "Decode best-performing bitstrings back into portfolio weights."
            ],
            "output": "Binary-encoded portfolio optimization structure ready for QAOA."
          },
    
          "Quantum_Mean_Variance_Optimization": {
            "name": "Quantum Mean–Variance Optimization (QMV)",
            "purpose": "Use quantum optimization techniques to minimize portfolio variance while maximizing expected return.",
            "procedure": [
              "Define Markowitz objective function in QUBO format.",
              "Apply QAOA to explore feasible weight combinations.",
              "Collect sample distributions and classical scoring metrics.",
              "Identify portfolio with optimal quantum-evaluated objective value."
            ],
            "output": "Quantum-optimized portfolio allocation via QUBO and QAOA."
          }
        }
      }
    },
    
    {
      "id": 58,
      "title": "Supervised learning with quantum-enhanced feature spaces",
      "authors": "Havlíček et al.",
      "year": 2019,
      "source": "Nature",
      "url": "https://www.nature.com/articles/s41586-019-0980-2",
      "performance-metrics": [
        "Classification accuracy using quantum kernel",
        "Quantum kernel fidelity estimation error",
        "Generalization performance vs classical kernels (e.g., RBF, polynomial)",
        "Kernel matrix condition number (numerical stability)",
        "Separation margins in quantum feature space",
        "Training convergence under quantum-enhanced SVM",
        "Noise robustness of quantum feature map circuits",
        "Sampling overhead for kernel estimation",
        "Performance gap between simulated and hardware-executed QSVM",
        "Scaling behavior of quantum kernel evaluation"
      ],
      "tools-used-in": [
        "Quantum-Support-Vector-Machine-(QSVM)"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Enhanced_Feature_Map": {
            "name": "Quantum-Enhanced Feature Map",
            "purpose": "Embed classical data into high-dimensional Hilbert space using parameterized quantum circuits to induce class-separating nonlinearities.",
            "components": {
              "feature_encoding": {
                "definition": "|φ(x)⟩ = U_φ(x)|0⟩",
                "role": "Maps classical vector x into quantum state through entangling gates.",
                "importance": "Produces complex, non-classical decision boundaries."
              },
              "entangling_structure": {
                "definition": "Layers of controlled-Z gates applied after data-dependent rotations.",
                "role": "Creates highly expressive quantum feature correlations."
              },
              "kernel_estimation": {
                "definition": "K(x_i, x_j) = |⟨φ(x_i)|φ(x_j)⟩|²",
                "role": "Provides kernel values required for QSVM."
              }
            },
            "procedure": [
              "Encode classical data using rotation- and entanglement-based quantum circuits.",
              "Prepare feature states |φ(x)⟩ for each sample.",
              "Estimate pairwise kernel values via quantum overlaps.",
              "Construct quantum kernel matrix.",
              "Train classical SVM using quantum kernel.",
              "Evaluate classification performance."
            ],
            "output": "Quantum kernel matrix enabling nonlinear classification.",
            "advantages": [
              "Potential exponential advantage over classical kernels for certain data distributions.",
              "Naturally incorporates entanglement to build complex features.",
              "Compatible with existing classical SVM solvers."
            ],
            "limitations": [
              "Kernel estimation costs scale with number of samples.",
              "Highly sensitive to noise in NISQ hardware.",
              "Expressive circuits may suffer from barren plateaus."
            ]
          },
    
          "Quantum_SVM": {
            "name": "Quantum Support Vector Machine (QSVM)",
            "purpose": "Apply classical SVM optimization using a kernel derived from quantum feature maps.",
            "components": {
              "quantum_kernel_matrix": {
                "definition": "Matrix of overlaps between encoded quantum states.",
                "role": "Acts as nonlinear similarity measure in SVM dual optimization."
              }
            },
            "procedure": [
              "Compute quantum kernel values for training samples.",
              "Use kernel matrix as input to classical SVM solver.",
              "Find optimal separating hyperplane in quantum-enhanced feature space.",
              "Predict labels for test samples using quantum kernel."
            ],
            "output": "SVM classifier that leverages quantum-induced nonlinear feature transformations.",
            "advantages": [
              "Provides better separation in complex datasets where classical kernels fail.",
              "Flexible integration with classical ML pipelines.",
              "Leverages quantum circuits without requiring full quantum optimization."
            ]
          },
    
          "Kernel_Overlap_Estimation": {
            "name": "Quantum Kernel Overlap Estimation",
            "purpose": "Compute fidelity between quantum feature states for constructing kernel matrix.",
            "procedure": [
              "Prepare states |φ(x_i)⟩ and |φ(x_j)⟩.",
              "Apply Hadamard test or SWAP test to estimate inner product.",
              "Square the magnitude to obtain kernel value.",
              "Repeat sampling to reduce statistical error."
            ],
            "output": "Estimated kernel similarity K(x_i, x_j) for QSVM."
          }
        }
      }
    },
    
    {
      "id": 59,
      "title": "Parameterized quantum circuits as generative models",
      "authors": "Benedetti et al.",
      "year": 2019,
      "source": "Quantum Sci. Technol.",
      "url": "https://iopscience.iop.org/article/10.1088/2058-9565/ab4eb5",
      "performance-metrics": [
        "Log-likelihood of generated distribution",
        "Kullback–Leibler (KL) divergence between target and generated distributions",
        "Mode coverage and avoidance of mode collapse",
        "Training convergence stability across circuit depths",
        "Expressibility and entanglement capacity of parameterized circuits",
        "Sampling fidelity relative to target distribution",
        "Robustness to hardware noise and decoherence",
        "Gradient variance and susceptibility to barren plateaus",
        "Approximation error of probability amplitudes",
        "Comparison of quantum vs classical generative modeling performance"
      ],
      "tools-used-in": [
        "Quantum-Boltzmann-Machine-(QBM)",
        "Quantum-Boltzmann-Machine-for-Market-Regime-Detection"
      ],
      "paper-details": {
        "algorithms": {
          "Parameterized_Quantum_Circuit_Generative_Model": {
            "name": "Parameterized Quantum Circuit (PQC) Generative Model",
            "purpose": "Use trainable quantum circuits to approximate probability distributions by encoding them into quantum states whose measurement statistics reproduce the target distribution.",
            "components": {
              "quantum_ansatz": {
                "definition": "Parameterized unitary U(θ) built from rotation and entangling gates.",
                "role": "Represents learnable transformation from initial state to probability distribution.",
                "importance": "Controls expressibility of the generative model."
              },
              "data_encoding": {
                "definition": "Amplitude or basis encoding depending on modeling task.",
                "role": "Maps classical data into quantum state format for training."
              },
              "training_objective": {
                "definition": "Minimize divergence D(p_target || p_model) using classical optimization.",
                "role": "Guides parameter updates for distribution matching."
              }
            },
            "procedure": [
              "Initialize PQC parameters θ randomly.",
              "Prepare initial state |0⟩ and apply U(θ) to obtain quantum model state.",
              "Generate samples via measurement to estimate model distribution.",
              "Compute loss function (KL divergence or negative log-likelihood).",
              "Update circuit parameters using hybrid classical optimization.",
              "Iterate sampling and optimization until convergence.",
              "Evaluate distribution quality using likelihood-based metrics."
            ],
            "output": "Quantum state whose measurement distribution approximates the target distribution.",
            "advantages": [
              "Can represent complex, non-classical correlations.",
              "Potentially more expressive than classical generative models.",
              "Compatible with NISQ architectures using shallow circuits.",
              "Integrates naturally with quantum Boltzmann machine frameworks."
            ],
            "limitations": [
              "Training can be unstable due to barren plateaus.",
              "Requires large numbers of circuit evaluations for gradient estimation.",
              "Noise on current devices reduces sampling fidelity.",
              "Likelihood estimation may be computationally expensive."
            ]
          },
    
          "Quantum_Boltzmann_Machine_Training": {
            "name": "Quantum Boltzmann Machine (QBM) Training with PQCs",
            "purpose": "Use parameterized quantum circuits to approximate thermal states of Ising Hamiltonians for probabilistic modeling.",
            "components": {
              "energy_hamiltonian": {
                "definition": "H(θ) = Σ h_i Z_i + Σ J_{ij} Z_i Z_j",
                "role": "Defines probability landscape via Boltzmann distribution."
              },
              "thermal_state_approximation": {
                "definition": "ρ ≈ U(θ)|0⟩⟨0|U†(θ)",
                "role": "Use PQC to approximate thermal distributions without explicit Gibbs sampling."
              }
            },
            "procedure": [
              "Define QBM Hamiltonian structure.",
              "Use PQC to approximate thermal distribution over states.",
              "Generate samples and compute log-likelihood gradient estimates.",
              "Perform hybrid classical–quantum parameter optimization.",
              "Evaluate equilibrium distribution accuracy via divergence metrics."
            ],
            "output": "Approximate quantum Boltzmann distribution encoded in PQC outputs."
          },
    
          "Hybrid_Quantum_Classical_Training": {
            "name": "Hybrid Training Framework for PQC Generative Models",
            "purpose": "Optimize quantum generative models using classical optimizers and quantum sampling loops.",
            "procedure": [
              "Sample bitstrings from PQC to estimate p_model(x).",
              "Compute loss function comparing p_model(x) with data distribution.",
              "Use classical optimizer (SGD, Adam, CMA-ES) to update parameters.",
              "Iterate until convergence.",
              "Evaluate out-of-sample distribution quality."
            ],
            "output": "Hybrid-trained PQC generative model capable of modeling complex distributions."
          }
        }
      }
    },
    
    {
      "id": 60,
      "title": "Quantum risk analysis",
      "authors": null,
      "year": 2019,
      "source": "IBM Research",
      "url": "https://research.ibm.com/publications/quantum-risk-analysis",
      "performance-metrics": [
        "Variance reduction in risk estimation compared to classical Monte Carlo",
        "Speedup factor in expected-loss computation using QAE",
        "Accuracy of CVaR and VaR estimation under quantum amplitude estimation",
        "Sampling complexity reduction (O(1/ε) vs O(1/ε²))",
        "Fidelity of quantum probability distribution encoding",
        "Estimation error bounds for quantum risk measures",
        "Robustness of quantum circuits to noise during amplitude estimation",
        "Confidence interval tightness for quantum-estimated risk metrics",
        "Bias–variance tradeoff in QAE under finite sampling",
        "Comparison of quantum vs classical risk analytics performance"
      ],
      "tools-used-in": [
        "Quantum-Amplitude-Estimation-(QAE)-for-CVaR"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Amplitude_Estimation_for_Risk": {
            "name": "Quantum Amplitude Estimation (QAE) for Risk Measures",
            "purpose": "Compute financial risk metrics such as expected loss, Value-at-Risk (VaR), and Conditional Value-at-Risk (CVaR) more efficiently than classical Monte Carlo via quantum amplitude amplification.",
            "components": {
              "quantum_state_preparation": {
                "definition": "Prepare |ψ⟩ = Σ_i √p_i |i⟩ encoding probability distribution of losses.",
                "role": "Allows quantum amplitudes to represent event probabilities."
              },
              "oracle_operator": {
                "definition": "O |i⟩ = (-1)^{f(i)} |i⟩ where f(i) flags loss events above threshold.",
                "role": "Marks states contributing to VaR and CVaR estimation."
              },
              "grover_operator": {
                "definition": "Q = Uψ O Uψ† O₀",
                "role": "Amplifies amplitudes associated with loss events."
              }
            },
            "procedure": [
              "Encode probability distribution of losses into quantum amplitudes.",
              "Apply amplitude estimation to compute expectation E[L].",
              "For VaR: use an oracle that thresholds losses above α-quantile.",
              "For CVaR: estimate conditional expectation conditioned on threshold exceedances.",
              "Repeat to refine quantum estimation accuracy.",
              "Classically post-process amplitudes to obtain numerical risk values."
            ],
            "output": "Quantum-estimated values for expected loss, VaR, and CVaR.",
            "advantages": [
              "Quadratic speedup over Monte Carlo (O(1/ε) vs O(1/ε²)).",
              "Efficient for complex financial distributions.",
              "Reduces sampling cost for tail-risk estimation.",
              "Integrates naturally with QAE-based quantum finance workflows."
            ],
            "limitations": [
              "Requires coherent state preparation of loss distribution.",
              "Sensitive to quantum hardware noise.",
              "Depth of Grover-like operators can exceed NISQ capabilities.",
              "Amplitude estimation requires phase estimation circuits, which are deep."
            ]
          },
    
          "Quantum_Loss_Distribution_Encoding": {
            "name": "Quantum Encoding of Financial Loss Distributions",
            "purpose": "Represent financial loss scenarios using quantum amplitudes for efficient expectation and tail-risk computation.",
            "components": {
              "loss_register": {
                "definition": "Quantum register that stores discretized loss values.",
                "role": "Forms basis for evaluating loss probability distribution."
              },
              "probability_amplitudes": {
                "definition": "√p_i encoding of probabilities.",
                "role": "Enables amplitude estimation to compute expectations efficiently."
              }
            },
            "procedure": [
              "Discretize loss variable L into bins.",
              "Assign amplitudes proportional to √p(L).",
              "Normalize quantum state.",
              "Use this encoded state as input to QAE-based algorithms."
            ],
            "output": "Quantum state representing loss distribution."
          },
    
          "Quantum_CVaR_Optimization": {
            "name": "Quantum CVaR Computation",
            "purpose": "Compute Conditional Value-at-Risk using quantum amplitude estimation for enhanced efficiency.",
            "procedure": [
              "Identify VaR threshold using QAE-based quantile estimation.",
              "Construct oracle to flag losses exceeding VaR.",
              "Apply amplitude estimation to compute E[L | L > VaR].",
              "Combine VaR and CVaR results into final risk metric."
            ],
            "output": "Efficient quantum-estimated CVaR for financial risk analysis."
          }
        }
      }
    },
    
    {
      "id": 61,
      "title": "Quantum algorithms for portfolio optimization",
      "authors": "Rebentrost et al.",
      "year": 2018,
      "source": "Phys. Rev. A",
      "url": "https://journals.aps.org/pra/abstract/10.1103/PhysRevA.98.022321",
      "performance-metrics": [
        "Quantum runtime complexity vs classical quadratic programming",
        "Error bounds for quantum linear system solver in portfolio weights",
        "Fidelity of quantum-prepared mean and covariance matrices",
        "Quality of optimal weights under quantum state tomography noise",
        "Scaling of portfolio optimization accuracy with qubit count",
        "Robustness of quantum solution under condition number variations",
        "Approximation error from Hamiltonian simulation in optimization",
        "Variance of quantum-estimated risk measures",
        "Quantum resource requirements (gate depth, qubit count)",
        "Comparison of portfolio return–risk tradeoff vs classical solver"
      ],
      "tools-used-in": [
        "Quantum-Mean-Variance-Optimization-(QMV)"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Mean_Variance_Optimization": {
            "name": "Quantum Mean–Variance Optimization (QMV)",
            "purpose": "Use quantum algorithms—primarily quantum linear system solvers (QLS)—to solve the Markowitz mean–variance optimization problem more efficiently than classical methods.",
            "components": {
              "quantum_state_preparation": {
                "definition": "Encode mean return vector μ and covariance matrix Σ into quantum states.",
                "role": "Allows quantum processing of financial inputs using amplitude encoding."
              },
              "quantum_linear_solver": {
                "definition": "Use HHL algorithm to solve Σ w = λ μ.",
                "role": "Computes optimal portfolio weights using quantum subroutines.",
                "advantage": "Provides exponential speedup under well-conditioned assumptions."
              },
              "hamiltonian_simulation": {
                "definition": "Simulate Σ efficiently using sparse or low-rank structures.",
                "role": "Required for implementing QLS and matrix inversion."
              }
            },
            "procedure": [
              "Encode expected returns and covariance matrix as quantum states.",
              "Construct Hamiltonian representation of covariance matrix.",
              "Use quantum linear solver (HHL-style) to compute w ∝ Σ^{-1} μ.",
              "Measure output state to extract classical approximation of optimal weights.",
              "Calculate portfolio variance and expected return using quantum inner products.",
              "Validate and refine weights via classical post-processing."
            ],
            "output": "Quantum-generated approximation of Markowitz optimal portfolio weights.",
            "advantages": [
              "Potential exponential speedup for large portfolios under ideal assumptions.",
              "Efficient handling of high-dimensional covariance matrices.",
              "Integrates well with quantum amplitude estimation for risk metrics."
            ],
            "limitations": [
              "Requires well-conditioned covariance matrix for HHL to perform efficiently.",
              "Noise and decoherence limit effectiveness on NISQ devices.",
              "Extracting classical weights requires repeated measurements.",
              "State preparation of Σ and μ is nontrivial and resource-intensive."
            ]
          },
    
          "Covariance_Matrix_State_Preparation": {
            "name": "Quantum Covariance Matrix Encoding",
            "purpose": "Represent the covariance matrix Σ as a quantum operator or state to enable matrix inversion and inner-product calculations.",
            "components": {
              "sparse_encoding": {
                "definition": "Encode Σ as a sparse Hamiltonian for simulation.",
                "role": "Allows efficient Hamiltonian simulation if Σ is sparse or structured."
              },
              "density_matrix_encoding": {
                "definition": "Use purification to encode Σ/Tr(Σ) as density matrix.",
                "role": "Useful for expectation-value estimation and matrix exponentiation."
              }
            },
            "procedure": [
              "Estimate covariance matrix from historical returns.",
              "Normalize or scale Σ for spectral consistency.",
              "Encode Σ into a quantum-usable form via sparse or low-rank decomposition.",
              "Verify encoding correctness using quantum tomography or indirect overlap tests."
            ],
            "output": "Quantum representation of Σ enabling QMV operations."
          },
    
          "Quantum_Inner_Product_Computation": {
            "name": "Quantum Estimation of Return and Risk",
            "purpose": "Use quantum measurement techniques to compute expected return and variance of the optimized portfolio.",
            "procedure": [
              "Prepare quantum state encoding weight vector w.",
              "Compute μᵀ w using inner-product estimation circuits.",
              "Compute wᵀ Σ w using Hamiltonian expectation estimation.",
              "Classically aggregate results to evaluate objective function."
            ],
            "output": "Quantum-evaluated risk and return metrics for optimized portfolio."
          }
        }
      }
    },
    
    {
      "id": 62,
      "title": "Deep learning with long short-term memory networks for financial market predictions",
      "authors": "Fischer & Krauss",
      "year": 2018,
      "source": "Journal of Forecasting",
      "url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/for.2473",
      "performance-metrics": [
        "Directional accuracy of predictions (up/down forecasting)",
        "Mean Absolute Error (MAE)",
        "Root Mean Squared Error (RMSE)",
        "Sharpe Ratio of LSTM-based trading strategy",
        "Return distribution characteristics under LSTM predictions",
        "Confusion matrix metrics (precision, recall, F1-score)",
        "Area Under ROC Curve (AUC)",
        "Profitability vs. benchmark strategies (Buy-and-Hold, Logistic Regression)",
        "Stability of predictions across rolling windows",
        "Out-of-sample generalization performance"
      ],
      "tools-used-in": [
        "LSTM-(Long-Short-Term-Memory)-Neural-Network"
      ],
      "paper-details": {
        "algorithms": {
          "LSTM_Model_for_Market_Prediction": {
            "name": "LSTM Neural Network for Financial Time-Series Prediction",
            "purpose": "Capture long-term temporal dependencies in financial time-series to forecast market movements and improve trading strategies.",
            "components": {
              "lstm_cell": {
                "definition": "A recurrent unit with input, output, and forget gates.",
                "role": "Controls which information to retain or discard across time steps.",
                "advantage": "Solves vanishing gradient problem of classical RNNs."
              },
              "feature_windowing": {
                "definition": "Use historical price and technical indicators as input sequences.",
                "role": "Provides time-dependent context for predictions."
              },
              "dense_output_layer": {
                "definition": "Feedforward layer mapping LSTM output to prediction.",
                "role": "Generates probability or regression output (direction or return)."
              }
            },
            "procedure": [
              "Collect and preprocess financial time-series (scaling, normalization).",
              "Form training sequences using sliding windows.",
              "Train LSTM network using backpropagation-through-time (BPTT).",
              "Validate model on holdout set and tune hyperparameters.",
              "Predict market direction or returns for test period.",
              "Evaluate accuracy and economic performance metrics.",
              "Construct trading strategy and compare against benchmarks."
            ],
            "output": "Predicted future returns or directional movements for financial assets.",
            "advantages": [
              "Effectively models nonlinear temporal dependencies.",
              "High predictive accuracy compared to linear models.",
              "Adaptable to large feature sets (technical, fundamental).",
              "Improves economic performance when converted into trading signals."
            ],
            "limitations": [
              "Requires careful tuning and large training datasets.",
              "Susceptible to overfitting without proper regularization.",
              "Computationally intensive for long sequences.",
              "Interpretability of hidden states is limited."
            ]
          },
    
          "Trading_Strategy_from_LSTM": {
            "name": "LSTM-Based Trading Strategy",
            "purpose": "Convert LSTM model predictions into executable trading decisions and evaluate economic performance.",
            "procedure": [
              "Use LSTM outputs to classify next-day direction (long/short).",
              "Construct portfolio using long-only or long–short signals.",
              "Backtest strategy over out-of-sample period.",
              "Compute Sharpe Ratio, cumulative returns, and drawdowns.",
              "Benchmark against classical machine-learning and linear models."
            ],
            "output": "Performance metrics of a trading strategy driven by LSTM forecasts."
          },
    
          "Feature_Engineering_and_Preprocessing": {
            "name": "Feature Engineering for LSTM Financial Models",
            "purpose": "Prepare structured input sequences suitable for LSTM consumption.",
            "procedure": [
              "Normalize raw pricing and volume features.",
              "Compute technical indicators (RSI, moving averages, momentum).",
              "Generate lagged features forming sequential time windows.",
              "Split dataset into training, validation, and test sets."
            ],
            "output": "Clean, temporally structured datasets for LSTM prediction pipeline."
          }
        }
      }
    },
    
    {
      "id": 63,
      "title": "Reinforcement Learning in finance: challenges and practical considerations",
      "authors": "Fischer",
      "year": 2018,
      "source": "SSRN",
      "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3128246",
      "performance-metrics": [
        "Cumulative return of RL policy",
        "Sharpe Ratio of learned trading strategy",
        "Maximum drawdown during evaluation period",
        "Stability and variance of policy performance across episodes",
        "Sample efficiency (reward improvement per training sample)",
        "Volatility of returns generated by the agent",
        "Reward convergence speed during training",
        "Action distribution entropy (exploration vs exploitation)",
        "Transaction cost–adjusted profitability",
        "Out-of-sample generalization of RL strategy"
      ],
      "tools-used-in": [
        "Reinforcement-Learning-Trading-Agent"
      ],
      "paper-details": {
        "algorithms": {
          "Reinforcement_Learning_Framework_for_Finance": {
            "name": "Reinforcement Learning Framework for Financial Trading",
            "purpose": "Apply RL techniques to develop trading agents capable of learning optimal policies from historical or simulated market interaction.",
            "components": {
              "state_representation": {
                "definition": "Vector of price-based features, indicators, and portfolio positions.",
                "role": "Provides situational awareness to the agent.",
                "importance": "Determines informativeness of the RL environment."
              },
              "action_space": {
                "definition": "Discrete (buy/hold/sell) or continuous (target weight, leverage).",
                "role": "Defines how the agent interacts with the market."
              },
              "reward_function": {
                "definition": "r_t = f(returns, risk, transaction_costs)",
                "role": "Encodes objective: profit-maximization with risk constraints."
              },
              "policy_function": {
                "definition": "π(a | s; θ)",
                "role": "Maps states to actions via learnable parameters (neural networks)."
              }
            },
            "procedure": [
              "Define RL environment using market data and trading constraints.",
              "Initialize agent with random or pre-trained policy network.",
              "Simulate episodes where agent takes actions and receives rewards.",
              "Update policy using chosen RL algorithm (e.g., DQN, PPO, REINFORCE).",
              "Evaluate agent on validation and test periods.",
              "Incorporate transaction costs and slippage modeling.",
              "Assess economic and statistical performance of the learned strategy."
            ],
            "output": "Trained RL policy capable of producing trading signals based on learned patterns.",
            "advantages": [
              "Learns dynamic, nonlinear trading rules directly from data.",
              "Adapts to changing market regimes with continuous learning.",
              "Handles sequential decision-making better than supervised learners."
            ],
            "limitations": [
              "High sensitivity to reward shaping and state design.",
              "Prone to overfitting due to nonstationary financial data.",
              "Requires large computational resources for training.",
              "Difficult to guarantee stability and robustness."
            ]
          },
    
          "Policy_Gradient_Methods": {
            "name": "Policy Gradient–Based Reinforcement Learning",
            "purpose": "Directly optimize trading policies by maximizing expected cumulative reward.",
            "procedure": [
              "Sample trajectories using current policy π(a|s).",
              "Estimate gradient of expected reward via REINFORCE or actor–critic methods.",
              "Update policy parameters in gradient ascent direction.",
              "Incorporate entropy bonuses to encourage exploration.",
              "Evaluate policy stability under stochastic returns."
            ],
            "output": "Improved trading policy with reinforced reward maximization."
          },
    
          "Value_Based_Methods": {
            "name": "Value-Based RL (e.g., DQN) in Finance",
            "purpose": "Approximate value function to select the optimal action for each state.",
            "procedure": [
              "Use neural networks to approximate Q(s, a).",
              "Collect experience tuples (s, a, r, s').",
              "Update network parameters using Bellman error minimization.",
              "Employ replay buffer and target networks for training stability.",
              "Convert value estimates into trading actions."
            ],
            "output": "Action-value function enabling rule-based trading decisions."
          },
    
          "Risk_Aware_RL": {
            "name": "Risk-Aware Reinforcement Learning",
            "purpose": "Incorporate risk metrics like volatility, CVaR, or drawdowns into the reward or optimization process.",
            "procedure": [
              "Define augmented reward function penalizing risk exposures.",
              "Apply CVaR-based objective or risk-sensitive policy gradients.",
              "Train agent to balance profitability and risk constraints.",
              "Evaluate performance under extreme market conditions."
            ],
            "output": "Trading policies optimized not only for return but also for risk-adjusted performance."
          }
        }
      }
    },
    
    {
      "id": 64,
      "title": "Classification with quantum neural networks",
      "authors": "Farhi & Neven",
      "year": 2018,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/1802.06002",
      "performance-metrics": [
        "Classification accuracy on benchmark datasets",
        "Convergence rate of QNN training",
        "Expressibility of quantum circuit architecture",
        "Gradient vanishing effects (barren plateaus)",
        "Robustness to hardware noise during training/inference",
        "Generalization capability vs classical neural networks",
        "Quantum circuit depth vs classification performance",
        "Training loss reduction across iterations",
        "Fidelity of state preparation in QNN",
        "Training stability under hybrid quantum–classical optimization"
      ],
      "tools-used-in": [
        "Quantum-Neural-Network-(QNN)"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Neural_Network": {
            "name": "Quantum Neural Network (QNN)",
            "purpose": "Perform classification tasks using a parameterized quantum circuit trained via hybrid quantum–classical optimization.",
            "components": {
              "quantum_feature_encoding": {
                "definition": "Encode classical input vector x into quantum state |ψ(x)⟩ via data-dependent rotations.",
                "role": "Transforms input data into a quantum-representable format."
              },
              "variational_quantum_circuit": {
                "definition": "Parameterized unitary U(θ) composed of rotation and entangling gates.",
                "role": "Acts as the neural network layer mapping inputs to outputs.",
                "importance": "Weights represented as quantum gate parameters."
              },
              "measurement_layer": {
                "definition": "Measure qubits in computational basis or Pauli-Z.",
                "role": "Returns probabilities used for classification decision."
              }
            },
            "procedure": [
              "Encode classical features into quantum states using rotation gates.",
              "Apply parameterized quantum circuit to transform state.",
              "Measure output qubits to obtain classification probabilities.",
              "Compute loss function (cross-entropy or MSE) based on labels.",
              "Iteratively update circuit parameters using classical optimizers (e.g., gradient descent).",
              "Repeat until convergence or performance plateau."
            ],
            "output": "Predicted class labels based on quantum circuit measurements.",
            "advantages": [
              "Potentially more expressive hypothesis space than classical models.",
              "Leverages entanglement and superposition for nonlinear classification.",
              "Suitable for NISQ hardware using shallow circuits."
            ],
            "limitations": [
              "Susceptible to barren plateau issues for deep circuits.",
              "Requires careful architecture design to maintain trainability.",
              "Noise-sensitive and hardware-limited on current devices.",
              "Data re-uploading is often needed to overcome limited expressiveness."
            ]
          },
    
          "Hybrid_Quantum_Classical_Optimization": {
            "name": "Hybrid Optimization Framework for QNN",
            "purpose": "Train QNN using classical optimizers in a closed-loop quantum-classical feedback setting.",
            "procedure": [
              "Initialize parameters of quantum circuit.",
              "Run quantum circuit, measure outputs, compute cost function.",
              "Backpropagate gradient estimate using classical computation (parameter shift rule or finite difference).",
              "Update quantum gate parameters.",
              "Repeat until satisfactory accuracy is reached."
            ],
            "output": "Optimized QNN model parameters."
          },
    
          "Parameter_Shift_Gradient_Estimation": {
            "name": "Parameter-Shift Rule",
            "purpose": "Estimate gradients of QNN parameters without requiring classical backpropagation.",
            "procedure": [
              "Shift parameter θ by ±π/2.",
              "Evaluate cost function at both shifted values.",
              "Compute gradient using (C(θ+π/2) − C(θ−π/2)) / 2.",
              "Use gradient for parameter update."
            ],
            "output": "Gradient estimate compatible with quantum computation."
          }
        }
      }
    },
    
    {
      "id": 65,
      "title": "Quantum Boltzmann machine learning",
      "authors": "Amin et al.",
      "year": 2018,
      "source": "PRX",
      "url": "https://journals.aps.org/prx/abstract/10.1103/PhysRevX.8.021050",
      "performance-metrics": [
        "Log-likelihood of learned distribution",
        "Kullback–Leibler divergence between target and model distributions",
        "Convergence rate of QBM parameter optimization",
        "Quantum Gibbs state fidelity",
        "Energy expectation error during training",
        "Robustness of optimization under quantum noise",
        "Sampling efficiency compared to classical Boltzmann machines",
        "Expressivity of quantum Boltzmann Hamiltonian",
        "Gradient variance during parameter updates",
        "Performance comparison of QBM vs classical RBM"
      ],
      "tools-used-in": [
        "Quantum-Boltzmann-Machine-(QBM)"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Boltzmann_Machine": {
            "name": "Quantum Boltzmann Machine (QBM)",
            "purpose": "Learn probability distributions via quantum Gibbs states generated from a Hamiltonian encoding interactions among variables.",
            "components": {
              "boltzmann_hamiltonian": {
                "definition": "H(θ) = ∑ h_i Z_i + ∑ J_{ij} Z_i Z_j + ∑ K_{ijk} Z_i Z_j Z_k (extended form).",
                "role": "Defines the energy function of the model.",
                "importance": "Captures complex correlations using quantum interactions."
              },
              "quantum_gibbs_state": {
                "definition": "ρ = e^{-βH} / Tr(e^{-βH})",
                "role": "Represents the model distribution using quantum thermal state."
              },
              "parameter_set": {
                "definition": "θ = {h_i, J_{ij}, …} defining the Hamiltonian.",
                "role": "Learnable parameters updated during training."
              }
            },
            "procedure": [
              "Initialize Hamiltonian parameters for the quantum Boltzmann distribution.",
              "Prepare approximate quantum Gibbs state using annealing or Hamiltonian simulation.",
              "Sample from distribution via quantum measurement.",
              "Compute loss using gradient of log-likelihood.",
              "Update Hamiltonian parameters with classical optimizers.",
              "Repeat until convergence of negative log-likelihood or stability of Boltzmann energy."
            ],
            "output": "Quantum model approximating target probability distribution.",
            "advantages": [
              "Quantum tunneling may escape local minima more efficiently.",
              "Can learn highly correlated and structured patterns.",
              "Potentially more expressive than classical RBMs.",
              "Compatible with quantum annealing and gate-based hardware."
            ],
            "limitations": [
              "Gibbs state preparation is computationally difficult on near-term hardware.",
              "Gradient estimation may be noisy.",
              "Scalability depends on qubit connectivity and coherence time.",
              "Requires precise Hamiltonian implementation, challenging under NISQ constraints."
            ]
          },
    
          "Quantum_Annealing_for_Training": {
            "name": "Quantum Annealing-Based Training",
            "purpose": "Use quantum annealers to approximate thermal distribution sampling for QBM parameter updates.",
            "procedure": [
              "Encode Hamiltonian parameters into annealer configuration.",
              "Perform quantum annealing to sample low-energy states.",
              "Compute statistics needed for gradient-based training.",
              "Update parameters to reduce model loss.",
              "Iterate until distribution converges."
            ],
            "output": "Boltzmann energy-based training using annealing hardware."
          },
    
          "Hybrid_Quantum_Classical_Optimization": {
            "name": "Hybrid Optimization for QBM",
            "purpose": "Use classical optimizers (e.g., Adam, SGD) to update Hamiltonian parameters based on quantum-generated samples.",
            "procedure": [
              "Generate samples via quantum simulation or hardware.",
              "Estimate expectation values and loss gradients.",
              "Update parameters using classical gradient descent.",
              "Repeat with feedback loops until convergence."
            ],
            "output": "Optimized Hamiltonian parameters improving model distribution quality."
          }
        }
      }
    },
    
    {
      "id": 66,
      "title": "Deep RL for multi-step cryptocurrency portfolio allocation",
      "authors": "Jiang et al.",
      "year": 2017,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/1706.10059",
      "performance-metrics": [
        "Portfolio cumulative return",
        "Sharpe Ratio of RL-based portfolio strategy",
        "Maximum drawdown during test period",
        "Volatility of portfolio returns",
        "Calmar Ratio (return-to-drawdown efficiency)",
        "Convergence rate of RL training",
        "Policy stability across consecutive episodes",
        "Turnover rate and transaction cost impact",
        "Out-of-sample performance against benchmarks",
        "Hit ratio (percentage of profitable allocation decisions)"
      ],
      "tools-used-in": [
        "Reinforcement-Learning-Trading-Agent"
      ],
      "paper-details": {
        "algorithms": {
          "Deep_RL_Portfolio_Allocation": {
            "name": "Deep Reinforcement Learning for Multi-Step Portfolio Allocation",
            "purpose": "Optimize portfolio weights dynamically using a reinforcement learning agent trained on cryptocurrency market data.",
            "components": {
              "state_representation": {
                "definition": "Historical cryptocurrency price matrix and current portfolio weights.",
                "role": "Provides time-dependent input features for decision making.",
                "importance": "Captures market dynamics and past performance."
              },
              "action_space": {
                "definition": "Continuous or normalized portfolio weight allocations.",
                "role": "Represents desired allocation across cryptocurrencies."
              },
              "reward_function": {
                "definition": "r_t = log(Portfolio_Return_t) adjusted for transaction costs.",
                "role": "Encourages long-term profit maximization.",
                "importance": "Uses log returns to handle compounding."
              },
              "policy_network": {
                "definition": "Deep convolutional or recurrent network mapping states to allocation decisions.",
                "role": "Learns optimal trading policy parameters."
              }
            },
            "procedure": [
              "Construct state tensor from historical cryptocurrency prices and current allocations.",
              "Define reward signal based on logarithmic portfolio return.",
              "Initialize RL policy network and simulate market interaction.",
              "Use deep RL algorithm (e.g., policy gradient or actor–critic) to update parameters.",
              "Iteratively improve strategy through episodes till convergence.",
              "Backtest agent performance on unseen data."
            ],
            "output": "Optimized dynamic cryptocurrency allocation strategy.",
            "advantages": [
              "Adapts to nonlinear and nonstationary market patterns.",
              "Captures multi-step dependencies and trading impact.",
              "Outperforms passive benchmarks in volatile crypto markets."
            ],
            "limitations": [
              "Highly sensitive to hyperparameters and market fluctuations.",
              "Susceptible to overfitting on limited data.",
              "Large training data and computational power required.",
              "Transaction costs drastically impact performance."
            ]
          },
    
          "Policy_Gradient_RL": {
            "name": "Policy Gradient Methods",
            "purpose": "Optimize portfolio allocation using gradient ascent on cumulative reward.",
            "procedure": [
              "Sample policy rollouts using current allocation strategy.",
              "Compute returns for each trajectory.",
              "Estimate gradient of expected return with respect to network parameters.",
              "Update parameters in direction of gradient ascent.",
              "Repeat across multiple episodes."
            ],
            "output": "Refined policy network with improved reward performance."
          },
    
          "CNN_Based_State_Encoder": {
            "name": "Convolutional Neural Network (CNN) for Market State Encoding",
            "purpose": "Extract spatial–temporal patterns from cryptocurrency price time-series.",
            "procedure": [
              "Input normalized cryptocurrency price sequence.",
              "Apply convolutional filters to detect market features.",
              "Flatten and feed into policy network for decision making."
            ],
            "output": "High-level features used by RL agent to determine allocations."
          }
        }
      }
    },
    
    {
      "id": 67,
      "title": "Portfolio rebalancing on D-Wave",
      "authors": "Rosenberg et al.",
      "year": 2016,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/1604.05718",
      "performance-metrics": [
        "Quantum annealer solution quality vs classical solver",
        "Time-to-solution comparison (annealing vs classical optimization)",
        "Objective function value (risk–return tradeoff) post-rebalancing",
        "Constraint violation rate (budget, leverage, maximum holdings)",
        "Portfolio turnover and trade optimization efficiency",
        "Annealing convergence success probability",
        "Energy landscape exploration effectiveness",
        "Sensitivity of results to penalty coefficients in QUBO",
        "Robustness to annealer noise and thermal fluctuations",
        "Performance gap between simulated annealing and D-Wave hardware"
      ],
      "tools-used-in": [
        "Quantum-Annealing-for-Multi-Objective-Portfolio-Construction",
        "Quantum-Mean-Variance-Optimization-(QUBO-Formulation)"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Annealing_for_Portfolio_Rebalancing": {
            "name": "Quantum Annealing for Multi-Objective Portfolio Construction",
            "purpose": "Perform portfolio rebalancing by solving a QUBO formulation of a multi-objective optimization problem using a quantum annealer.",
            "components": {
              "qubo_formulation": {
                "definition": "Minimize xᵀ Q x where Q encodes return, risk, and penalty constraints.",
                "role": "Transforms portfolio optimization into binary decision problem.",
                "importance": "Allows compatibility with quantum annealing hardware."
              },
              "annealing_schedule": {
                "definition": "s(t) = (1 - t/T) H_initial + (t/T) H_problem",
                "role": "Gradually transitions from easy-to-solve Hamiltonian to portfolio optimization Hamiltonian.",
                "advantage": "Uses quantum tunneling to escape local minima."
              }
            },
            "procedure": [
              "Define binary decision variables representing buy/hold/sell actions or discrete allocation levels.",
              "Construct QUBO matrix incorporating expected return, risk (variance), and rebalancing penalties.",
              "Embed QUBO onto D-Wave’s qubit connectivity using minor embedding.",
              "Set annealing schedule and hyperparameters (duration, number of reads).",
              "Execute quantum annealing to search for optimal allocation.",
              "Postprocess annealer solutions to recover portfolio weights.",
              "Validate objective improvements and constraint satisfaction."
            ],
            "output": "Set of candidate rebalanced portfolios obtained via quantum annealing.",
            "advantages": [
              "Can efficiently explore large combinatorial solution spaces.",
              "Quantum tunneling improves ability to escape local minima.",
              "Supports solving multi-objective financial optimization problems."
            ],
            "limitations": [
              "Embedding overhead increases with portfolio size.",
              "Results sensitive to annealing parameters and penalty weights.",
              "Hardware constraints (qubit count, connectivity) limit scalability.",
              "Annealing does not guarantee global optimum under noise."
            ]
          },
    
          "QUBO_Formulation_for_Portfolio_Optimization": {
            "name": "QUBO Formulation for Portfolio Optimization",
            "purpose": "Define a binary quadratic optimization form suitable for both quantum annealing and QAOA-based quantum mean–variance optimization.",
            "components": {
              "objective_matrix": {
                "definition": "Q = λΣ − μμᵀ + penalty_terms",
                "role": "Balances risk minimization and return maximization."
              },
              "constraint_penalties": {
                "definition": "Quadratic terms penalizing budget, leverage or holding size violations.",
                "role": "Enforces regulatory or trading constraints."
              }
            },
            "procedure": [
              "Discretize portfolio weights into binary variable representation.",
              "Construct QUBO matrix including variance (risk), return, and penalty terms.",
              "Bind matrix to annealer input format.",
              "Run optimization and interpret resulting bitstrings."
            ],
            "output": "Binary encoded formulation of portfolio rebalancing problem."
          },
    
          "Postprocessing_and_Solution_Filtering": {
            "name": "Postprocessing QUBO Solutions",
            "purpose": "Refine raw annealer outputs into valid portfolio allocations.",
            "procedure": [
              "Collect multiple solution samples from annealer.",
              "Filter out constraint-violating solutions.",
              "Classically rescale or average binary outputs into usable weights.",
              "Perform backtesting or risk analysis on refined portfolio."
            ],
            "output": "Constraint-compliant, risk-adjusted rebalanced portfolio."
          }
        }
      }
    },
    
    {
      "id": 68,
      "title": "XGBoost: A Scalable Tree Boosting System",
      "authors": "Chen & Guestrin",
      "year": 2016,
      "source": "KDD",
      "url": "https://dl.acm.org/doi/10.1145/2939672.2939785",
      "performance-metrics": [
        "Classification accuracy (for supervised learning tasks)",
        "Area Under ROC Curve (AUC)",
        "Root Mean Squared Error (RMSE) for regression tasks",
        "Training time efficiency",
        "Inference latency",
        "Feature importance ranking stability",
        "Model scalability across large datasets",
        "Overfitting resistance via regularization",
        "Memory usage and computational efficiency",
        "Lift over baseline models (e.g., Random Forest)"
      ],
      "tools-used-in": [
        "Random-Forest-Gradient-Boosted-Trees-(XGBoost)"
      ],
      "paper-details": {
        "algorithms": {
          "XGBoost": {
            "name": "XGBoost (Extreme Gradient Boosting)",
            "purpose": "Improve computational efficiency and predictive accuracy of gradient-boosted decision trees using optimized system design and algorithmic enhancements.",
            "components": {
              "gradient_boosting_framework": {
                "definition": "Sequentially train decision trees to minimize loss with respect to residuals.",
                "role": "Improves model by correcting previous errors using gradient descent.",
                "advantage": "Leads to strong, high-performing ensemble learners."
              },
              "regularized_objective": {
                "definition": "Loss(θ) = Σ l(yᵢ, ŷᵢ) + Σ (α‖w‖₁ + λ‖w‖₂²)",
                "role": "Penalizes complexity to reduce overfitting.",
                "advantage": "Improves generalization compared to classical boosting."
              },
              "sparsity_aware_training": {
                "definition": "Optimized split-finding for missing or sparse data.",
                "purpose": "Achieves high efficiency on sparse datasets."
              },
              "approximate_split_finding": {
                "definition": "Histogram-based split estimation.",
                "role": "Reduces computation time on large datasets."
              }
            },
            "procedure": [
              "Define loss function and gradient-based optimization objective.",
              "Initialize model with constant prediction.",
              "Iteratively train decision trees to minimize residual errors.",
              "Apply second-order optimization using gradient and hessian.",
              "Use regularization terms to control tree complexity.",
              "Perform split optimization using approximate methods for efficiency.",
              "Aggregate trees for final prediction.",
              "Evaluate model using validation datasets and metrics."
            ],
            "output": "Trained gradient-boosted tree model optimizing accuracy and computational performance.",
            "advantages": [
              "Highly efficient and scalable implementation.",
              "Support for both classification and regression tasks.",
              "Built-in regularization improves generalization.",
              "Handles missing data and sparsity effectively.",
              "Parallelization and distributed computing support."
            ],
            "limitations": [
              "Parameter tuning can be complex.",
              "Less interpretable than simpler tree-based models.",
              "Gradients can lead to overfitting on noisy datasets if improperly regularized.",
              "Computational cost increases with tree depth and number of trees."
            ]
          },
    
          "Second_Order_Gradient_Optimization": {
            "name": "Second-Order Gradient Optimization",
            "purpose": "Enhance boosting efficiency using both first and second derivatives of loss function.",
            "procedure": [
              "Compute gradient and hessian for each data instance.",
              "Use them to approximate change in loss for potential splits.",
              "Update model parameters accordingly.",
              "Repeat for each boosting iteration."
            ],
            "output": "More precise optimization leading to faster convergence and improved model performance."
          },
    
          "Sparsity_Aware_Split_Finding": {
            "name": "Sparsity-Aware Split Finding",
            "purpose": "Efficiently handle missing and sparse data without imputation.",
            "procedure": [
              "Identify missing values as a separate category.",
              "Assign optimal default split direction.",
              "Evaluate split gain using available non-missing values only."
            ],
            "output": "Optimized tree structure that maintains prediction accuracy with sparse inputs."
          }
        }
      }
    },
    
    {
      "id": 69,
      "title": "Quantum speedup of Monte Carlo methods",
      "authors": "Montanaro",
      "year": 2015,
      "source": "Proc. Roy. Soc. A",
      "url": "https://royalsocietypublishing.org/doi/10.1098/rspa.2015.0301",
      "performance-metrics": [
        "Quantum Monte Carlo convergence rate (O(1/ε) vs classical O(1/ε²))",
        "Estimation error bounds under quantum amplitude estimation",
        "Variance reduction in expectation estimation",
        "Speedup factor in Monte Carlo sampling tasks",
        "Sampling complexity improvements",
        "Quantum vs classical runtime comparison",
        "Fidelity of probability encoding in quantum states",
        "Robustness to noise during amplitude amplification",
        "Error propagation in quantum-accelerated Monte Carlo",
        "Performance scaling with dimension and number of samples"
      ],
      "tools-used-in": [
        "Quantum-Amplitude-Estimation-for-Expected-Return",
        "Quantum-Amplitude-Estimation-for-Expected-Return-and-Risk"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Monte_Carlo_Speedup": {
            "name": "Quantum Monte Carlo Speedup via Amplitude Estimation",
            "purpose": "Achieve quadratic improvement in Monte Carlo simulation efficiency using quantum amplitude estimation and amplification techniques.",
            "components": {
              "state_preparation": {
                "definition": "Prepare quantum state |ψ⟩ where amplitudes encode probability distribution of Monte Carlo outcomes.",
                "role": "Forms basis for expectation estimation via amplitude measurement."
              },
              "oracle_operator": {
                "definition": "Oracle O marks states corresponding to favorable Monte Carlo outcomes.",
                "role": "Enables selective amplification of probabilities."
              },
              "grover_operator": {
                "definition": "Q = Uψ O Uψ† O0",
                "role": "Amplifies amplitude corresponding to desired probabilities."
              }
            },
            "procedure": [
              "Define Monte Carlo simulation variable X with probabilistic distribution.",
              "Encode distribution into quantum amplitudes using state preparation.",
              "Apply amplitude estimation to determine E[X] with error O(1/M).",
              "Use Grover iteration to amplify relevant amplitudes where necessary.",
              "Measure qubits repeatedly to estimate desired expectation value.",
              "Post-process outcomes to retrieve classical risk or return metrics."
            ],
            "output": "Estimated expectation values of simulated distributions using fewer samples than classical Monte Carlo.",
            "advantages": [
              "Quadratic speedup over classical Monte Carlo convergence.",
              "Suitable for high-dimensional risk evaluation problems.",
              "Integrates with financial risk and portfolio optimization algorithms.",
              "Reduces sampling overhead significantly."
            ],
            "limitations": [
              "Requires precise state preparation and coherent quantum execution.",
              "Sensitive to noise and decoherence in quantum hardware.",
              "Phase estimation-based QAE circuits are deep and costly.",
              "Quantum advantage is asymptotic—practical benefits depend on hardware."
            ]
          },
    
          "Quantum_Amplitude_Estimation": {
            "name": "Quantum Amplitude Estimation (QAE)",
            "purpose": "Efficiently estimate expected returns and risks by using amplitude amplification-based variance reduction.",
            "components": {
              "amplitude_encoding": {
                "definition": "Encode expectations as amplitudes of quantum states.",
                "role": "Directly relates measurement outcomes to expected value."
              }
            },
            "procedure": [
              "Initialize state encoding probability-weighted outcomes.",
              "Apply QAE to compute expected value more efficiently.",
              "Use iterative phase estimation or maximum likelihood approximation if hardware-limited.",
              "Decode amplitude estimates into classical expected return or risk."
            ],
            "output": "Expected value estimate with quadratically reduced sample complexity."
          },
    
          "Application_to_Financial_Risk": {
            "name": "Quantum Monte Carlo for Expected-Return and Risk Estimation",
            "purpose": "Apply quantum-accelerated Monte Carlo to compute portfolio expected-return and risk metrics.",
            "procedure": [
              "Define financial loss/return distribution.",
              "Encode distribution as quantum amplitudes.",
              "Run QAE-based Monte Carlo simulation.",
              "Estimate expected return and variance/correlation.",
              "Use results in portfolio/risk management frameworks."
            ],
            "output": "Quantum-accelerated computation of financial expectations and risk distributions."
          }
        }
      }
    },

    {
      "id": 70,
      "title": "Quantum principal component analysis",
      "authors": "Lloyd, Mohseni & Rebentrost",
      "year": 2014,
      "source": "Nature Physics",
      "url": "https://www.nature.com/articles/nphys3029",
      "performance-metrics": [
        "Eigenvalue estimation accuracy",
        "State fidelity for reconstructed principal components",
        "Runtime complexity comparison (quantum vs classical PCA: exponential vs polynomial)",
        "Sample complexity for density matrix exponentiation",
        "Scalability with respect to system dimension (n-qubit density matrix)",
        "Sparsity and low-rank sensitivity of input density matrix"
      ],
      "tools-used-in": [
        "Quantum-PCA-(qPCA)-for-Factor-Risk-Analysis"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_PCA": {
            "name": "Quantum Principal Component Analysis (qPCA)",
            "purpose": "Extract principal components of a quantum density matrix using quantum operations, achieving an exponential speedup in cases where density matrix exponentiation is efficient.",
            "components": {
              "density_matrix_exponentiation": {
                "method": "Simulate e^{-iρt} using many copies of ρ",
                "effect": "Allows the system to undergo unitary evolution proportional to the density matrix itself.",
                "requirement": "A large (polynomial) number of identical copies of ρ."
              },
              "quantum_phase_estimation": {
                "role": "Estimates eigenvalues of ρ",
                "operation": "Applies controlled-unitary operations using U = e^{-iρt}",
                "output": "Eigenvalue λ_k encoded in a quantum register."
              },
              "eigenvector_extraction": {
                "role": "Obtains eigenvectors of the density matrix",
                "effect": "Input state collapses onto eigenvector |v_k⟩ proportional to its overlap with eigenvectors of ρ."
              }
            },
            "procedure": [
              "Prepare multiple identical copies of the quantum density matrix ρ.",
              "Implement density matrix exponentiation to construct U = e^{-iρt}.",
              "Apply quantum phase estimation using U to obtain eigenvalues λ_k.",
              "The system collapses to eigenvectors |v_k⟩ with probability proportional to their contribution in ρ.",
              "Collect principal components (largest λ_k) to reconstruct reduced representation of ρ."
            ],
            "output": "Eigenvalues and eigenvectors of ρ, corresponding to principal components of the quantum system.",
            "advantages": [
              "Potential exponential speedup for low-rank or structured density matrices.",
              "Direct extraction of eigenvectors without constructing full covariance matrix.",
              "Efficient for quantum data where classical PCA is impractical."
            ],
            "limitations": [
              "Requires many identical copies of ρ (state tomography-like scaling).",
              "Density matrix exponentiation feasible only for certain structured states.",
              "Hardware noise may corrupt eigenvalue estimation."
            ]
          },
          "Density_Matrix_Exponentiation": {
            "name": "Density Matrix Exponentiation",
            "purpose": "Enable unitary operations based on a density matrix without explicitly computing the matrix.",
            "procedure": [
              "Use many copies of density matrix ρ as a resource.",
              "Apply SWAP operations between ancilla and system.",
              "Simulate infinitesimal evolution e^{-iρΔt}.",
              "Concatenate small steps to approximate e^{-iρt}."
            ],
            "output": "Unitary operator e^{-iρt} enabling quantum PCA.",
            "role_in_qpca": "Core mechanism enabling exponential speedup through implicit access to ρ."
          },
          "Quantum_Phase_Estimation": {
            "name": "Quantum Phase Estimation (QPE)",
            "purpose": "Estimate eigenvalues of e^{-iρt}, which encode eigenvalues of ρ.",
            "procedure": [
              "Prepare eigenstate-superposition input.",
              "Apply controlled-e^{-iρt} operations.",
              "Perform inverse QFT on phase register.",
              "Measure phase register to obtain eigenvalue estimate."
            ],
            "output": "Eigenvalues λ_k allowing extraction of principal components."
          }
        }
      }
    },

    {
      "id": 71,
      "title": "Quantum algorithms for clustering via distance estimation",
      "authors": "Lloyd et al.",
      "year": 2013,
      "source": "arXiv",
      "url": "https://arxiv.org/abs/1304.7827",
      "performance-metrics": [
        "Quantum distance estimation error (inner-product fidelity)",
        "Success probability of cluster assignment",
        "Runtime scaling comparison (quantum vs classical: O(log N) vs O(N))",
        "State preparation complexity",
        "Cluster separation performance under exponential speedup regime",
        "Sample complexity for accurate distance estimation"
      ],
      "tools-used-in": [
        "Quantum-k-Means-Clustering"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Distance_Estimation": {
            "name": "Quantum Distance Estimation Algorithm",
            "purpose": "Efficiently compute Euclidean distance or inner products between high-dimensional vectors using quantum amplitude estimation.",
            "components": {
              "state_preparation": {
                "method": "Prepare |v⟩ and |w⟩ as normalized quantum states from classical vectors v and w",
                "role": "Enables encoding of high-dimensional vectors into logarithmic qubit space."
              },
              "swap_test": {
                "operation": "Apply Hadamard gate, controlled-SWAP, final Hadamard measurement",
                "output": "Estimate inner product ⟨v|w⟩ with quantum advantage.",
                "effect": "Allows distance computation: ||v - w||² = 2 - 2⟨v|w⟩"
              },
              "amplitude_estimation": {
                "role": "Boosts precision of distance estimation",
                "advantage": "Achieves quadratic speedup compared to classical sampling."
              }
            },
            "procedure": [
              "Encode classical vectors v and w into quantum states |v⟩ and |w⟩.",
              "Apply the SWAP test to estimate their inner product.",
              "Convert inner product into Euclidean distance.",
              "Use amplitude estimation to refine accuracy.",
              "Feed estimated distances into clustering routine."
            ],
            "output": "Accurate inner-product or distance estimates enabling quantum clustering.",
            "advantages": [
              "Distance computation in O(log N) time instead of O(N).",
              "Quadratic improvement in precision via amplitude estimation.",
              "Supports large-scale clustering on quantum hardware."
            ]
          },
    
          "Quantum_k_Means": {
            "name": "Quantum k-Means Clustering",
            "purpose": "Assign points to clusters using quantum-estimated distances, offering exponential speedup in high-dimensional clustering tasks.",
            "components": {
              "distance_oracle": {
                "role": "Provides quantum-estimated distances to centroids",
                "operation": "Leverages inner-product calculations and state encoding of centroids."
              },
              "cluster_assignment": {
                "method": "Pick the nearest centroid based on estimated distance",
                "output": "Quantum-enhanced label assignment for each data point."
              },
              "centroid_update": {
                "role": "Recompute cluster means (classical post-processing)",
                "note": "Paper focuses on accelerating distance computations, not centroid recomputation."
              }
            },
            "procedure": [
              "Prepare quantum states for data point and cluster centroids.",
              "Estimate distances to each centroid using quantum distance estimation.",
              "Assign data point to closest centroid.",
              "Recompute centroids (typically classical).",
              "Repeat steps for convergence."
            ],
            "output": "Cluster labels determined using quantum distance estimation.",
            "advantages": [
              "Exponential speedup in nearest-centroid distance computation.",
              "Suitable for high-dimensional and large datasets.",
              "Reduces bottleneck of classical k-means algorithm."
            ],
            "limitations": [
              "Requires efficient state preparation, which may be costly.",
              "Centroid update step remains classical.",
              "Sensitive to noise in inner-product estimation."
            ]
          },
    
          "Swap_Test": {
            "name": "Swap Test",
            "purpose": "Measure similarity between two quantum states |v⟩ and |w⟩.",
            "procedure": [
              "Prepare ancilla qubit in |0⟩ state.",
              "Apply Hadamard gate to ancilla.",
              "Apply controlled-SWAP between |v⟩ and |w⟩.",
              "Apply Hadamard to ancilla and measure.",
              "Probability of measuring |0⟩ gives (1 + |⟨v|w⟩|²)/2."
            ],
            "output": "Inner product estimate enabling distance computation."
          }
        }
      }
    },
    
    {
      "id": 72,
      "title": "Value and Momentum Everywhere",
      "authors": "Asness et al.",
      "year": 2013,
      "source": "Journal of Finance",
      "url": "https://onlinelibrary.wiley.com/doi/abs/10.1111/jofi.12068",
      "performance-metrics": [
        "Sharpe Ratio (cross-asset)",
        "Information Ratio",
        "Average Excess Return (Value and Momentum premiums)",
        "t-statistics for factor significance",
        "Correlation between Value and Momentum returns",
        "Drawdown and crash sensitivity metrics",
        "Volatility of factor returns",
        "Factor diversification benefit (cross-market correlation reduction)",
        "Skewness and kurtosis of Value & Momentum factor returns"
      ],
      "tools-used-in": [
        "Momentum-Strategy"
      ],
      "paper-details": {
        "algorithms": {
          "Value_Factor": {
            "name": "Value Factor Construction",
            "purpose": "Identify undervalued securities across multiple asset classes using valuation ratios.",
            "components": {
              "value_signals": {
                "examples": [
                  "Book-to-Market (B/M)",
                  "Earnings-to-Price (E/P)",
                  "Dividend-to-Price (D/P)",
                  "Cash-flow-to-Price (CF/P)"
                ],
                "role": "Rank assets based on relative cheapness."
              },
              "cross_asset_normalization": {
                "operation": "Standardize valuation measures within each asset class.",
                "effect": "Allows fair comparison of value signals across heterogeneous markets."
              }
            },
            "procedure": [
              "Collect valuation ratios for each asset.",
              "Normalize signals to align scales across markets.",
              "Rank assets and select those with highest value scores.",
              "Construct long-short value portfolio (long cheap, short expensive)."
            ],
            "output": "Cross-sectional value portfolio with persistent positive returns.",
            "advantages": [
              "Simple and scalable across asset classes.",
              "Historically robust long-term positive premium.",
              "Low correlation with momentum factor, enhancing diversification."
            ]
          },
    
          "Momentum_Factor": {
            "name": "Momentum Factor Construction",
            "purpose": "Exploit continuation in asset returns across global markets.",
            "components": {
              "momentum_signal": {
                "definition": "Past 12-month return excluding most recent month (12–1 momentum).",
                "effect": "Captures trend-following behavior observed across asset classes."
              },
              "ranking_mechanism": {
                "role": "Sort assets by past performance.",
                "operation": "Select winners (top quantile) and losers (bottom quantile)."
              }
            },
            "procedure": [
              "Compute 12–1 momentum for each asset.",
              "Rank assets within each asset class.",
              "Go long winners and short losers.",
              "Rebalance at fixed monthly intervals."
            ],
            "output": "Momentum portfolio capturing trend persistence.",
            "advantages": [
              "Strong positive returns across equities, bonds, commodities, and currencies.",
              "Complementary to value due to low correlation.",
              "Historically robust even after adjusting for risk factors."
            ],
            "limitations": [
              "Susceptible to momentum crashes in market reversals.",
              "Drawdown risk during sharp regime shifts."
            ]
          },
    
          "Combined_Value_Momentum_Strategy": {
            "name": "Value + Momentum Integrated Portfolio",
            "purpose": "Enhance risk-adjusted returns through diversification between negatively correlated factors.",
            "components": {
              "portfolio_weighting": {
                "operation": "Blend value and momentum factor returns to minimize volatility.",
                "effect": "Reduces tail risk and increases Sharpe ratio."
              },
              "cross_asset_implementation": {
                "markets": [
                  "Equities",
                  "Government bonds",
                  "Currencies",
                  "Commodities"
                ],
                "role": "Ensures robustness across geographies and asset types."
              }
            },
            "procedure": [
              "Construct standalone value and momentum factor portfolios.",
              "Measure historical correlations between the factors.",
              "Allocate capital to achieve diversification benefits.",
              "Rebalance periodically."
            ],
            "output": "Combined portfolio with stronger Sharpe ratio and lower volatility.",
            "advantages": [
              "Value and momentum premiums persist globally.",
              "Their low correlation enables consistent performance.",
              "Improves downside protection vs individual factors."
            ]
          }
        }
      }
    },
    
    {
      "id": 73,
      "title": "Time series momentum",
      "authors": "Moskowitz et al.",
      "year": 2012,
      "source": "Journal of Financial Economics",
      "url": "https://www.sciencedirect.com/science/article/pii/S0304407612000286",
      "performance-metrics": [
        "Sharpe Ratio of time-series momentum portfolios",
        "Hit-rate (percentage of assets with positive autocorrelation sign)",
        "t-statistics for significance of time-series predictability",
        "Volatility-adjusted return",
        "Cross-asset autocorrelation strength",
        "Performance persistence across asset classes",
        "Drawdown analysis and crash sensitivity",
        "Turnover and transaction-cost adjusted returns",
        "Correlation with traditional momentum and value strategies",
        "Risk premium stability across sample periods"
      ],
      "tools-used-in": [
        "Momentum-Strategy"
      ],
      "paper-details": {
        "algorithms": {
          "Time_Series_Momentum": {
            "name": "Time-Series Momentum (Trend Following)",
            "purpose": "Determine whether an asset's own past return predicts its future return direction and construct long/short strategies accordingly.",
            "components": {
              "lookback_returns": {
                "definition": "Past 12-month excess return used as the momentum signal.",
                "role": "Determines whether the asset has been trending up or down.",
                "property": "Positive returns → long; negative returns → short."
              },
              "volatility_scaling": {
                "method": "Scale position size inversely to recent volatility.",
                "effect": "Stabilizes risk across assets and time.",
                "advantage": "Reduces impact of extreme volatility spikes."
              },
              "cross_asset_universality": {
                "markets": [
                  "Equities",
                  "Government bonds",
                  "Currencies",
                  "Commodities",
                  "Equity indices"
                ],
                "role": "Shows time-series momentum is a global phenomenon."
              }
            },
            "procedure": [
              "Compute 12-month past return for each asset.",
              "Assign long position if return > 0 and short if return < 0.",
              "Scale each position by inverse volatility.",
              "Construct equal-weight or risk-parity portfolio across assets.",
              "Rebalance periodically (typically monthly)."
            ],
            "output": "Long/short momentum portfolio that profits from continuation in asset trends.",
            "advantages": [
              "Strong performance across many decades and asset classes.",
              "Continues to work after accounting for transaction costs.",
              "Low correlation with cross-sectional momentum and value factors.",
              "Risk-balanced implementation improves Sharpe ratio significantly."
            ],
            "limitations": [
              "Susceptible to reversals during fast trend breaks.",
              "Performance deteriorates during choppy, range-bound markets.",
              "Short-term mean reversion can offset long-term trend signals."
            ]
          },
    
          "Volatility_Scaling": {
            "name": "Volatility Scaling of Momentum Signals",
            "purpose": "Reduce portfolio risk and improve Sharpe ratio by normalizing exposure based on recent volatility.",
            "procedure": [
              "Compute rolling volatility for each asset (e.g., 1-month or 3-month).",
              "Set position size = target risk / recent volatility.",
              "Adjust long or short exposure based on time-series momentum signal.",
              "Rebalance positions to maintain stable portfolio risk."
            ],
            "output": "Risk-adjusted trend-following portfolio with enhanced stability.",
            "advantages": [
              "Dampens portfolio drawdowns.",
              "Improves signal consistency across assets of different risk levels."
            ]
          },
    
          "Trend_Signal_Construction": {
            "name": "Trend Direction Determination",
            "purpose": "Identify whether price series exhibits positive or negative trend.",
            "procedure": [
              "Measure past return of asset over a fixed lookback window.",
              "Compare sign of return to threshold (zero).",
              "Assign directional exposure: long if positive, short if negative.",
              "Repeat monthly for all assets."
            ],
            "output": "Binary directional signals that drive time-series momentum."
          }
        }
      }
    },
    
    {
      "id": 74,
      "title": "On the efficiency of risk measures for funds of hedge funds",
      "authors": "Laube et al.",
      "year": 2011,
      "source": "SpringerLink",
      "url": "https://link.springer.com/article/10.1057/jdhf.2011.3",
      "performance-metrics": [
        "Value-at-Risk (VaR) estimation accuracy",
        "Expected Shortfall (ES) estimation quality",
        "Backtesting exceedance frequency",
        "Kupiec LR test statistic for VaR violations",
        "Christoffersen independence test statistic",
        "Volatility clustering sensitivity (via GARCH modeling)",
        "Tail-risk sensitivity under non-normal returns",
        "Comparative portfolio drawdown performance",
        "Correlation sensitivity for diversified hedge fund portfolios"
      ],
      "tools-used-in": [
        "Variance-Covariance-Value-at-Risk"
      ],
      "paper-details": {
        "algorithms": {
          "Variance_Covariance_VaR": {
            "name": "Variance–Covariance Value-at-Risk (Parametric VaR)",
            "purpose": "Evaluate risk of hedge fund portfolios under the assumption of normally distributed returns using mean–variance estimation.",
            "components": {
              "portfolio_variance": {
                "definition": "σ_p² = wᵀ Σ w",
                "role": "Captures co-movement between hedge fund strategies.",
                "effect": "Determines VaR magnitude under Gaussian assumption."
              },
              "parametric_var_formula": {
                "definition": "VaR = μ_p + z_α · σ_p",
                "role": "Measures potential loss at confidence level α.",
                "note": "Assumes normal distribution, often violated in hedge funds."
              },
              "covariance_matrix_estimation": {
                "methods": [
                  "Sample covariance",
                  "Shrinkage estimators",
                  "GARCH-based conditional covariance"
                ],
                "role": "Addresses instability in hedge fund return correlations."
              }
            },
            "procedure": [
              "Collect return series for hedge fund indices or FoHF (fund of hedge funds).",
              "Estimate mean vector and covariance matrix of returns.",
              "Compute portfolio variance using σ_p² = wᵀ Σ w.",
              "Use normal quantile z_α to compute VaR.",
              "Backtest VaR violations using statistical tests."
            ],
            "output": "Parametric VaR estimate for diversified hedge fund portfolios.",
            "advantages": [
              "Simple and computationally efficient.",
              "Useful baseline model for diversified portfolios.",
              "Facilitates cross-strategy comparison of hedge fund risks."
            ],
            "limitations": [
              "Normality assumption fails for hedge funds with fat tails.",
              "Ignores nonlinear payoffs typical in FoHF strategies.",
              "Underestimates risk during periods of high volatility clustering."
            ]
          },
    
          "Expected_Shortfall": {
            "name": "Expected Shortfall (ES)",
            "purpose": "Provide a coherent risk measure capturing tail losses beyond VaR threshold.",
            "components": {
              "tail_expectation": {
                "definition": "ES = E[L | L > VaR]",
                "role": "Accounts for magnitude of extreme losses.",
                "advantage": "More sensitive to tail risk than VaR."
              }
            },
            "procedure": [
              "Estimate VaR threshold at chosen confidence level.",
              "Compute conditional average of returns worse than VaR.",
              "Compare ES and VaR across hedge fund portfolio configurations."
            ],
            "output": "Tail-risk estimate superior to VaR for heavy-tailed hedge fund returns.",
            "advantages": [
              "Coherent risk measure satisfying subadditivity.",
              "Better suited for hedge funds with nonlinear payoff structures."
            ]
          },
    
          "Backtesting_Framework": {
            "name": "VaR Backtesting Framework",
            "purpose": "Evaluate statistical reliability of VaR estimates for hedge fund portfolios.",
            "procedure": [
              "Count number of VaR exceedances.",
              "Apply Kupiec’s unconditional coverage test.",
              "Apply Christoffersen’s independence test.",
              "Evaluate whether violations occur randomly over time.",
              "Assess model adequacy and compare risk measure efficiency."
            ],
            "output": "Quantitative assessment of VaR accuracy and reliability."
          }
        }
      }
    },
    
    {
      "id": 75,
      "title": "The Properties of Equally Weighted Risk Contribution Portfolios",
      "authors": "Maillard et al.",
      "year": 2010,
      "source": "Journal of Portfolio Management",
      "url": "https://jpm.pm-research.com/content/36/4/60",
      "performance-metrics": [
        "Risk contribution balance (RC_i / Total Risk)",
        "Portfolio volatility",
        "Sharpe Ratio",
        "Diversification ratio",
        "Turnover and transaction-cost impact",
        "Stability of weights over time",
        "Correlation sensitivity",
        "Tracking error vs minimum variance and equal-weight portfolios",
        "Out-of-sample performance consistency"
      ],
      "tools-used-in": [
        "Risk-Parity-ERC"
      ],
      "paper-details": {
        "algorithms": {
          "Equal_Risk_Contribution": {
            "name": "Equally Weighted Risk Contribution (ERC) Portfolio",
            "purpose": "Allocate capital such that each asset contributes equally to overall portfolio risk.",
            "components": {
              "marginal_risk_contribution": {
                "definition": "MRC_i = (w_i · (Σw)_i) / σ_p",
                "role": "Measures how each asset affects total portfolio volatility.",
                "effect": "Used to equalize risk contributions across assets."
              },
              "risk_contribution": {
                "definition": "RC_i = w_i · MRC_i",
                "role": "Determines share of total risk assigned to each asset."
              },
              "covariance_matrix": {
                "role": "Measures co-movement between assets.",
                "importance": "ERC relies heavily on covariance stability for proper risk allocation."
              }
            },
            "procedure": [
              "Estimate covariance matrix Σ for all assets.",
              "Define marginal risk contributions for each asset.",
              "Set constraint RC_1 = RC_2 = ... = RC_n (equal risk contribution).",
              "Solve optimization problem: minimize ∑ (RC_i - RC_avg)² subject to weight and risk constraints.",
              "Normalize weights to sum to 1 and rebalance periodically."
            ],
            "output": "ERC weights that equalize risk across portfolio constituents.",
            "advantages": [
              "Diversification without requiring return forecasts.",
              "More stable than minimum variance and less concentrated.",
              "Balances exposure across uncorrelated and volatile assets."
            ],
            "limitations": [
              "Requires accurate covariance estimates.",
              "Sensitive to unstable correlation regimes.",
              "Computationally intensive relative to naive equal-weight portfolios."
            ]
          },
    
          "Diversification_Ratio": {
            "name": "Diversification Ratio",
            "purpose": "Evaluate how diversified an ERC portfolio is relative to its components.",
            "components": {
              "formula": {
                "definition": "DR = (wᵀσ) / σ_p",
                "role": "Compares weighted average asset volatility to portfolio volatility."
              }
            },
            "procedure": [
              "Compute individual asset volatilities and portfolio volatility.",
              "Calculate weighted average of asset volatilities.",
              "Divide by portfolio volatility to obtain diversification ratio.",
              "Compare DR across ERC, minimum variance, and equal-weight strategies."
            ],
            "output": "Measure of diversification efficiency.",
            "advantages": [
              "Shows how ERC improves diversification vs standard strategies."
            ]
          },
    
          "Risk_Contribution_Analysis": {
            "name": "Risk Contribution Decomposition",
            "purpose": "Break down total portfolio volatility into contributions from each asset.",
            "procedure": [
              "Compute MRC_i for all assets.",
              "Multiply by weights to obtain RC_i.",
              "Verify equal contribution across assets in ERC.",
              "Compare RC distribution against equal-weight and minimum-variance portfolios."
            ],
            "output": "Risk decomposition confirming equal risk contributions."
          }
        }
      }
    },
    
    {
      "id": 76,
      "title": "Pairs Trading: Performance of a Relative-Value Arbitrage Rule",
      "authors": "Gatev et al.",
      "year": 2006,
      "source": "Review of Financial Studies",
      "url": "https://academic.oup.com/rfs/article/19/3/797/1599809",
      "performance-metrics": [
        "Average excess return (monthly and annualized)",
        "Sharpe Ratio of pairs trading strategy",
        "Profit per trade (PPT)",
        "Dollar-neutral return profile",
        "Maximum drawdown of the pairs portfolio",
        "Hit-rate (percentage of profitable trades)",
        "Half-life of mean reversion",
        "Standard deviation of spread returns",
        "t-statistics for profitability significance",
        "Rolling-window out-of-sample performance stability"
      ],
      "tools-used-in": [
        "Pairs-Trading-(Statistical-Arbitrage)"
      ],
      "paper-details": {
        "algorithms": {
          "Pairs_Trading_Strategy": {
            "name": "Distance-Based Pairs Trading",
            "purpose": "Identify pairs of stocks with historically similar price movements and exploit temporary divergences through mean reversion.",
            "components": {
              "pair_selection": {
                "method": "Find stock pairs with minimum sum-of-squared price differences over formation period.",
                "effect": "Ensures stable long-term price relationships.",
                "role": "Core mechanism for identifying arbitrage opportunities."
              },
              "spread_definition": {
                "definition": "Spread_t = Price_A(t) – Price_B(t)",
                "role": "Tracks divergence between paired stocks.",
                "importance": "Determines when to initiate long–short positions."
              },
              "trading_thresholds": {
                "entry_rule": "Enter trade when spread widens beyond historical mean by predefined threshold.",
                "exit_rule": "Close trade when spread reverts to mean.",
                "advantage": "Captures mean-reversion profits while limiting noise."
              }
            },
            "procedure": [
              "Select trading universe (e.g., S&P 500 stocks).",
              "Form historical pairs by minimizing distance metric over prior 12 months.",
              "Monitor spread during trading period.",
              "Go long the underpriced stock and short the overpriced stock when spread exceeds threshold.",
              "Close position once spread reverts.",
              "Compute profits net of transaction costs."
            ],
            "output": "Long–short relative-value strategy generating statistically significant abnormal returns.",
            "advantages": [
              "Strong profitability with low market correlation.",
              "Robust across formation and trading horizons.",
              "Provides market-neutral exposure (beta close to zero).",
              "Works well in various market regimes."
            ],
            "limitations": [
              "Profitability decreases under high transaction costs.",
              "Strategy can fail during structural breaks or changes in fundamentals.",
              "Requires careful selection of formation period to avoid overfitting."
            ]
          },
    
          "Mean_Reversion_Model": {
            "name": "Mean Reversion of Spread",
            "purpose": "Model divergence and convergence behavior of paired stocks.",
            "components": {
              "ou_process": {
                "definition": "dS_t = θ(μ – S_t)dt + σ dW_t",
                "role": "Represents speed and variability of spread reversion.",
                "importance": "Key to determining holding period and profitability."
              }
            },
            "procedure": [
              "Estimate long-term mean of spread (μ).",
              "Estimate speed of reversion (θ) and volatility (σ).",
              "Determine expected convergence horizon.",
              "Align trading thresholds with reversion characteristics."
            ],
            "output": "Framework for modeling and predicting spread convergence."
          },
    
          "Pair_Selection_Distance_Metric": {
            "name": "Distance Metric for Pair Formation",
            "purpose": "Identify pairs with tightly linked historical price paths.",
            "procedure": [
              "Compute sum of squared daily price differences over formation window.",
              "Rank all stock pairs by distance score.",
              "Select lowest-distance pairs as trading candidates."
            ],
            "output": "Stable candidate pairs with high probability of mean reversion."
          }
        }
      }
    },
    
    {
      "id": 77,
      "title": "Using ARIMA model to forecast stock price",
      "authors": "Pai & Lin",
      "year": 2005,
      "source": "ScienceDirect",
      "url": "https://www.sciencedirect.com/science/article/pii/S0305054805000081",
      "performance-metrics": [
        "Mean Absolute Error (MAE)",
        "Mean Squared Error (MSE)",
        "Root Mean Squared Error (RMSE)",
        "Mean Absolute Percentage Error (MAPE)",
        "Directional Accuracy (up/down prediction accuracy)",
        "Residual autocorrelation diagnostics (Ljung–Box Q-statistic)",
        "AIC (Akaike Information Criterion) for model selection",
        "BIC (Bayesian Information Criterion)",
        "Forecast error variance",
        "In-sample vs out-of-sample predictive performance"
      ],
      "tools-used-in": [
        "ARIMA-(AutoRegressive-Integrated-Moving-Average)"
      ],
      "paper-details": {
        "algorithms": {
          "ARIMA_Model": {
            "name": "Autoregressive Integrated Moving Average (ARIMA)",
            "purpose": "Forecast stock prices by modeling autocorrelations in time series data using autoregression, differencing, and moving-average components.",
            "components": {
              "autoregressive_part": {
                "definition": "AR(p): y_t = φ₁y_{t-1} + … + φ_py_{t-p} + ε_t",
                "role": "Captures momentum and persistence in stock price movements."
              },
              "differencing": {
                "definition": "I(d): Apply differencing d times to remove non-stationarity.",
                "purpose": "Convert stock price series into a stationary form before modeling."
              },
              "moving_average_part": {
                "definition": "MA(q): ε_t = θ₁e_{t-1} + … + θ_q e_{t-q}",
                "role": "Captures shock propagation and noise structure."
              }
            },
            "procedure": [
              "Test stock price series for stationarity using ADF or KPSS tests.",
              "Difference the series until stationarity is achieved.",
              "Identify p, d, q parameters using ACF and PACF plots.",
              "Estimate ARIMA(p, d, q) parameters using maximum likelihood.",
              "Diagnose residuals using autocorrelation and Ljung–Box tests.",
              "Generate forecasts and evaluate forecasting errors."
            ],
            "output": "Short-term stock price forecasts based on the identified ARIMA model.",
            "advantages": [
              "Effective for linear and short-memory time series.",
              "Interpretable parameters and strong statistical foundation.",
              "Well-suited for univariate forecasting."
            ],
            "limitations": [
              "Cannot capture nonlinear patterns in financial time series.",
              "Performance degrades under high volatility or structural shifts.",
              "Assumes linear relationships and stationary residuals."
            ]
          },
    
          "Model_Selection_Criteria": {
            "name": "ARIMA Model Selection via Information Criteria",
            "purpose": "Choose optimal (p, d, q) configuration that balances fit and complexity.",
            "procedure": [
              "Estimate ARIMA models for various (p, q) pairs.",
              "Compute AIC and BIC for each candidate model.",
              "Select the model with minimum AIC/BIC score.",
              "Validate using out-of-sample forecast accuracy."
            ],
            "output": "Best-fitting ARIMA configuration for forecasting."
          },
    
          "Residual_Diagnostics": {
            "name": "Residual Autocorrelation Analysis",
            "purpose": "Ensure ARIMA residuals behave like white noise.",
            "procedure": [
              "Compute residual autocorrelation function (ACF).",
              "Perform Ljung–Box test across multiple lags.",
              "Check for remaining patterns or seasonality.",
              "Verify homoskedasticity and independence."
            ],
            "output": "Validated residuals suitable for forecasting."
          }
        }
      }
    },
    
    {
      "id": 78,
      "title": "Pairs Trading",
      "authors": "Elliott et al.",
      "year": 2005,
      "source": "Quantitative Finance",
      "url": "https://www.tandfonline.com/doi/abs/10.1080/14697680500149370",
      "performance-metrics": [
        "Average excess return from pairs trades",
        "Sharpe Ratio of spread-based trading strategy",
        "Profit per trade (PPT)",
        "Signal-to-noise ratio of spread process",
        "Half-life of mean reversion estimated via OU process",
        "Maximum drawdown of the pairs portfolio",
        "t-statistics for spread predictability",
        "RMSE of spread model fit",
        "Hit-rate of profitable trades",
        "Out-of-sample performance stability under regime changes"
      ],
      "tools-used-in": [
        "Pairs-Trading-(Statistical-Arbitrage)"
      ],
      "paper-details": {
        "algorithms": {
          "Stochastic_Spread_Model": {
            "name": "Ornstein–Uhlenbeck (OU) Process Based Spread Model",
            "purpose": "Model the price spread between paired assets as a mean-reverting stochastic process to generate trading signals.",
            "components": {
              "ou_process": {
                "definition": "dS_t = κ(μ − S_t) dt + σ dW_t",
                "parameters": {
                  "κ": "Speed of mean reversion",
                  "μ": "Long-run equilibrium spread",
                  "σ": "Volatility of spread innovations"
                },
                "role": "Determines trade entry and exit timing based on deviations from μ."
              },
              "state_space_representation": {
                "definition": "Spread modeled using hidden-state Kalman filtering",
                "role": "Improves estimation of spread dynamics in noisy financial data.",
                "benefit": "Allows dynamic updating of spread model parameters."
              }
            },
            "procedure": [
              "Estimate OU process parameters from historical spread data.",
              "Apply Kalman filter to infer hidden spread states.",
              "Compute standardized deviations (z-scores) from equilibrium.",
              "Enter long–short positions when z-score exceeds threshold.",
              "Exit when spread reverts to equilibrium value.",
              "Evaluate profitability net of transaction costs."
            ],
            "output": "Dynamic, mean-reversion–based spread trading signals.",
            "advantages": [
              "More rigorous than simple distance-based methods.",
              "Incorporates noise filtering for smoother signals.",
              "Explicitly models mean reversion strength and volatility.",
              "Adapts over time via Kalman filtering."
            ],
            "limitations": [
              "Requires strong statistical assumptions.",
              "Performance sensitive to parameter estimation errors.",
              "OU-based trading may fail in non-stationary environments."
            ]
          },
    
          "Kalman_Filter_Estimation": {
            "name": "Kalman Filter for Spread and Parameter Estimation",
            "purpose": "Provide optimal recursive estimation of hidden spread dynamics and adjust to new market information.",
            "components": {
              "state_equation": {
                "definition": "S_t = μ + e_t, with transition governed by OU dynamics",
                "role": "Defines evolution of true (unobserved) spread."
              },
              "measurement_equation": {
                "definition": "Observed spread = S_t + noise",
                "role": "Models observed spread series with observational errors."
              }
            },
            "procedure": [
              "Initialize state and covariance estimates.",
              "Use prediction step to estimate next state using OU parameters.",
              "Update estimate using new spread observations.",
              "Iteratively refine spread and deviation estimates."
            ],
            "output": "Filtered spread series used for robust signal generation.",
            "advantages": [
              "Reduces noise and improves signal reliability.",
              "Allows real-time adaptive estimation."
            ]
          },
    
          "Trading_Rule": {
            "name": "Z-Score Based Trading Rule",
            "purpose": "Determine when to enter or exit positions based on deviation of filtered spread from equilibrium.",
            "procedure": [
              "Compute z-score: (Spread_t − μ) / σ.",
              "Enter long–short trade when |z| exceeds threshold.",
              "Reverse or close positions once spread reverts to mean.",
              "Monitor profit trajectory and risk metrics."
            ],
            "output": "Systematic entry and exit signals for pairs trading strategy."
          }
        }
      }
    },
    
    {
      "id": 79,
      "title": "Honey, I Shrunk the Sample Covariance Matrix",
      "authors": "Ledoit & Wolf",
      "year": 2004,
      "source": "Journal of Portfolio Management",
      "url": "https://www.jstor.org/stable/4485060",
      "performance-metrics": [
        "Out-of-sample portfolio variance",
        "Stability of covariance matrix estimates",
        "Condition number of covariance matrix",
        "Portfolio turnover",
        "Sharpe Ratio improvement versus sample covariance",
        "Mean–variance efficiency (frontier curvature)",
        "Estimation error reduction (Frobenius norm distance)",
        "Eigenvalue dispersion and shrinkage effectiveness",
        "Tracking error of optimized portfolios",
        "Risk-concentration metrics across portfolio weights"
      ],
      "tools-used-in": [
        "Markowitz-Mean–Variance"
      ],
      "paper-details": {
        "algorithms": {
          "Shrinkage_Covariance_Estimator": {
            "name": "Ledoit–Wolf Shrinkage Estimator",
            "purpose": "Stabilize covariance matrix estimation by shrinking the noisy sample covariance matrix toward a structured target matrix.",
            "components": {
              "sample_covariance_matrix": {
                "definition": "S = (1/n) Σ (x_t − μ)(x_t − μ)ᵀ",
                "role": "Baseline estimator but highly unstable when n is small relative to number of assets."
              },
              "shrinkage_target": {
                "definition": "T = scalar multiple of identity or constant correlation matrix",
                "role": "Provides a stable, low-variance target structure.",
                "benefit": "Reduces noise and extreme eigenvalue behavior."
              },
              "shrinkage_intensity": {
                "definition": "α ∈ [0, 1]",
                "formula": "Σ̂ = αT + (1 − α)S",
                "role": "Controls trade-off between bias and variance.",
                "advantage": "Optimal α minimizes mean-squared error between true and estimated covariance."
              }
            },
            "procedure": [
              "Compute sample covariance matrix S from asset returns.",
              "Define shrinkage target matrix T based on identity or constant correlation.",
              "Estimate optimal shrinkage intensity α using Ledoit–Wolf closed-form formula.",
              "Construct shrunk covariance estimator Σ̂ = αT + (1 − α)S.",
              "Use Σ̂ in Markowitz mean–variance portfolio optimization."
            ],
            "output": "Shrunk covariance matrix Σ̂ with improved stability and lower estimation error.",
            "advantages": [
              "Significantly reduces estimation error in high-dimensional settings.",
              "Produces well-conditioned covariance matrices suitable for optimization.",
              "Enhances out-of-sample portfolio performance versus naive covariance estimators.",
              "Simple to compute and robust across asset classes."
            ],
            "limitations": [
              "Shrinkage target may oversimplify true correlation structure.",
              "Does not capture nonlinear or regime-shifting relationships.",
              "Optimal α depends on assumptions about stationarity of returns."
            ]
          },
    
          "Markowitz_Optimization_with_Shrinkage": {
            "name": "Mean–Variance Optimization using Shrunk Covariance Matrix",
            "purpose": "Construct portfolios with lower realized risk by using the more stable Ledoit–Wolf covariance estimator.",
            "components": {
              "objective": {
                "definition": "Minimize wᵀΣ̂w for given target return or maximize Sharpe Ratio.",
                "role": "Uses shrunk covariance to avoid extreme weight concentration."
              },
              "constraints": {
                "examples": [
                  "Weights sum to 1",
                  "No short-selling (optional)",
                  "Target-return requirement"
                ]
              }
            },
            "procedure": [
              "Replace S with Σ̂ in the Markowitz optimization problem.",
              "Solve quadratic optimization to obtain optimal portfolio weights.",
              "Evaluate out-of-sample performance using shrunk covariance matrix."
            ],
            "output": "Portfolio with improved stability and diversification compared to sample covariance optimization.",
            "advantages": [
              "Avoids overfitting to noisy covariance estimates.",
              "Produces more diversified, stable portfolios."
            ]
          },
    
          "Shrinkage_Intensity_Estimation": {
            "name": "Optimal Shrinkage Intensity Calculation",
            "purpose": "Determine optimal α that minimizes estimation error in covariance matrix.",
            "procedure": [
              "Estimate variance of sample covariance elements.",
              "Estimate covariance between sample elements and true covariance.",
              "Derive closed-form solution for optimal α.",
              "Plug α into Σ̂ = αT + (1 − α)S."
            ],
            "output": "Statistically optimal intensity parameter α for shrinkage."
          }
        }
      }
    },
    {
      "id": 80,
      "title": "Conditional value-at-risk for general loss distributions",
      "authors": "Krokhmal et al.",
      "year": 2002,
      "source": "Journal of Banking & Finance",
      "url": "https://www.sciencedirect.com/science/article/pii/S0378426602001042",
      "performance-metrics": [
        "CVaR estimation accuracy under non-normal loss distributions",
        "VaR backtesting violation frequency",
        "Tail-risk sensitivity metrics",
        "Minimized portfolio tail loss (optimized CVaR objective)",
        "Convergence performance of linear programming formulation",
        "Out-of-sample tail-loss stability",
        "Scenario-based risk estimation error",
        "Robustness to heavy-tailed and skewed return distributions",
        "Comparison of CVaR vs VaR under extreme-loss events"
      ],
      "tools-used-in": [
        "CVaR-Optimization"
      ],
      "paper-details": {
        "algorithms": {
          "CVaR_Definition_and_Estimation": {
            "name": "Conditional Value-at-Risk (CVaR) Estimation Framework",
            "purpose": "Define and compute Conditional Value-at-Risk for general — including heavy-tailed — loss distributions, improving tail-risk measurement beyond VaR.",
            "components": {
              "var_threshold": {
                "definition": "VaRα = inf { x : P(Loss > x) ≤ 1 − α }",
                "role": "Initial threshold determining tail region for CVaR computation."
              },
              "cvar_formula": {
                "definition": "CVaRα = E[ Loss | Loss ≥ VaRα ]",
                "role": "Represents expected loss in the worst (1 − α)% scenarios.",
                "advantage": "Coherent risk measure capturing tail severity, unlike VaR."
              },
              "scenario_representation": {
                "method": "Represent loss distribution using discrete scenarios Li with probabilities pi",
                "effect": "Enables optimization using linear programming."
              }
            },
            "procedure": [
              "Identify α confidence level (e.g., 95%, 99%).",
              "Compute VaRα threshold from loss distribution or scenarios.",
              "Aggregate losses exceeding VaRα.",
              "Estimate conditional expectation to compute CVaR.",
              "Use CVaR estimator in optimization or stress-testing."
            ],
            "output": "Tail-risk estimate reflecting magnitude of extreme losses.",
            "advantages": [
              "Works for arbitrary (non-normal, fat-tailed, multimodal) distributions.",
              "Coherent risk measure satisfying monotonicity and subadditivity.",
              "Better suited for portfolio optimization under uncertainty."
            ]
          },
    
          "CVaR_Optimization_Algorithm": {
            "name": "Rockafellar–Uryasev CVaR Optimization Method",
            "purpose": "Optimize portfolios using CVaR as the objective function through a tractable linear programming formulation.",
            "components": {
              "auxiliary_variable_eta": {
                "definition": "η approximates VaRα threshold and is optimized jointly with weights.",
                "role": "Ensures tail events are correctly captured in LP formulation."
              },
              "loss_exceedance_variables": {
                "definition": "ξ_i = max{0, L_i(w) − η}",
                "role": "Measures how much each scenario exceeds VaRα.",
                "importance": "Allows piecewise-linear representation of tail losses."
              },
              "linear_program_formulation": {
                "objective": "Minimize η + (1 / ((1 − α)N)) Σ ξ_i",
                "role": "Convex optimization replacing non-convex VaR minimization.",
                "benefit": "Guaranteed global optimum."
              }
            },
            "procedure": [
              "Represent portfolio loss L_i(w) across N scenarios.",
              "Introduce η as VaR proxy and ξ_i as exceedance variables.",
              "Formulate convex linear program minimizing CVaR objective.",
              "Solve LP to obtain optimal weights and η.",
              "Compute resulting CVaR and evaluate portfolio tail-risk."
            ],
            "output": "Portfolio weight vector minimizing tail losses under CVaR definition.",
            "advantages": [
              "Transforms difficult VaR optimization into solvable convex LP.",
              "Handles arbitrary loss distributions without assuming normality.",
              "Ensures global optimum due to convexity."
            ],
            "limitations": [
              "Scenario generation quality strongly impacts results.",
              "May require large number of scenarios for accuracy.",
              "Computational intensity grows with scenario dimension."
            ]
          },
    
          "Scenario_Based_Loss_Model": {
            "name": "Scenario-Based Loss Representation",
            "purpose": "Approximate arbitrary loss distributions for CVaR estimation and optimization.",
            "procedure": [
              "Generate or sample scenario losses from historical or simulated data.",
              "Assign probability weights to each scenario.",
              "Use scenario set to compute VaR and CVaR.",
              "Plug scenarios directly into CVaR optimization LP."
            ],
            "output": "Discrete loss distribution enabling LP-based CVaR optimization."
          }
        }
      }
    },
    
    {
      "id": 81,
      "title": "Principal Component Analysis",
      "authors": "Jolliffe",
      "year": 2002,
      "source": "Springer",
      "url": "https://link.springer.com/book/10.1007/978-1-4757-1904-8",
      "performance-metrics": [
        "Explained variance ratio",
        "Cumulative explained variance",
        "Reconstruction error",
        "Eigenvalue magnitude and decay rate",
        "Principal component loadings stability",
        "Sensitivity to noise and multicollinearity",
        "Out-of-sample projection error",
        "Correlation between original variables and components",
        "Dimensionality reduction effectiveness",
        "Clustering performance improvement when combined with k-means"
      ],
      "tools-used-in": [
        "PCA-K-Means"
      ],
      "paper-details": {
        "algorithms": {
          "Principal_Component_Analysis": {
            "name": "Principal Component Analysis (PCA)",
            "purpose": "Reduce dimensionality by transforming correlated variables into a smaller set of uncorrelated principal components that capture maximal variance.",
            "components": {
              "standardization": {
                "definition": "Transform each variable to zero mean and unit variance.",
                "role": "Ensures PCA is not dominated by high-variance variables."
              },
              "covariance_matrix": {
                "definition": "Σ = (1/n) XᵀX after centering",
                "role": "Captures linear relationships between variables."
              },
              "eigendecomposition": {
                "definition": "Solve Σv = λv",
                "role": "Eigenvalues λ represent variance explained; eigenvectors v define components."
              },
              "principal_components": {
                "definition": "PC_k = X · v_k",
                "role": "Projection of data onto eigenvector directions."
              }
            },
            "procedure": [
              "Standardize original variables.",
              "Compute covariance (or correlation) matrix.",
              "Perform eigendecomposition to obtain eigenvalues and eigenvectors.",
              "Sort eigenvalues in descending order and select top k components.",
              "Project original data onto selected eigenvectors.",
              "Analyze variance explained and interpret loadings."
            ],
            "output": "Reduced-dimensional representation using orthogonal principal components.",
            "advantages": [
              "Reduces noise and redundancy.",
              "Improves computational efficiency for high-dimensional tasks.",
              "Enhances clustering quality when used before k-means.",
              "Transforms correlated variables into uncorrelated components."
            ],
            "limitations": [
              "Assumes linearity and Gaussian structure.",
              "Components may be difficult to interpret.",
              "Sensitive to scaling and outliers.",
              "Does not capture nonlinear patterns (unlike kernel PCA)."
            ]
          },
    
          "Scree_Plot_Selection": {
            "name": "Eigenvalue Scree Plot Analysis",
            "purpose": "Select the optimal number of principal components based on eigenvalue decay.",
            "procedure": [
              "Sort eigenvalues in decreasing order.",
              "Plot eigenvalues against component indices.",
              "Identify ‘elbow point’ where marginal gain in variance decreases sharply.",
              "Choose number of PCs based on elbow or variance threshold (e.g., 90%)."
            ],
            "output": "Optimal number of principal components for dimensional reduction."
          },
    
          "PCA_KMeans_Pipeline": {
            "name": "PCA + k-Means Clustering Pipeline",
            "purpose": "Use PCA-transformed data to improve clustering performance by reducing noise and redundancy.",
            "components": {
              "dimensionality_reduction": {
                "role": "Compress data into most informative features.",
                "effect": "Improves separability of clusters in reduced space."
              },
              "k_means_clustering": {
                "definition": "Partition reduced data into k clusters by minimizing within-cluster variance.",
                "role": "Extracts cluster structure after PCA cleanup."
              }
            },
            "procedure": [
              "Apply PCA and select top k components.",
              "Normalize PCA outputs if required.",
              "Apply k-means clustering to reduced data.",
              "Evaluate cluster quality via silhouette scores or inertia."
            ],
            "output": "Improved clustering structure due to PCA denoising.",
            "advantages": [
              "Reduces computational burden of k-means.",
              "Mitigates curse of dimensionality.",
              "Improves cluster separation and stability."
            ]
          }
        }
      }
    },
    
    {
      "id": 82,
      "title": "Original amplitude estimation algorithm",
      "authors": "Brassard et al.",
      "year": 2002,
      "source": "Contemporary Mathematics",
      "url": "https://arxiv.org/abs/quant-ph/0005055",
      "performance-metrics": [
        "Quadratic speedup in estimation accuracy (O(1/ε) classical vs O(1/√ε) quantum)",
        "Estimation error of amplitude a (|â − a|)",
        "Success probability of measurement outcomes",
        "Number of Grover iterations required for target precision",
        "Variance of amplitude estimator",
        "Runtime complexity in terms of oracle calls",
        "Robustness under noise in unitary operations",
        "Scaling performance for large dataset encodings"
      ],
      "tools-used-in": [
        "Quantum-Amplitude-Estimation-for-Expected-Return"
      ],
      "paper-details": {
        "algorithms": {
          "Amplitude_Estimation": {
            "name": "Quantum Amplitude Estimation (Original Brassard et al. Algorithm)",
            "purpose": "Estimate the amplitude a of a target quantum state with quadratic speedup over classical Monte Carlo sampling.",
            "components": {
              "state_preparation_operator": {
                "definition": "A|0⟩ = √(1−a)|ψ_0⟩ + √a|ψ_1⟩",
                "role": "Encodes probability amplitude a into a quantum state."
              },
              "grover_operator": {
                "definition": "Q = −A S_0 A† S_χ",
                "role": "Amplifies the amplitude of the target state through Grover rotations.",
                "effect": "Implements unitary evolution U(a) whose phase encodes the amplitude."
              },
              "phase_estimation": {
                "definition": "Quantum Phase Estimation applied to Q",
                "role": "Extracts phase θ such that a = sin²(θ).",
                "advantage": "Achieves O(1/√ε) sampling complexity."
              }
            },
            "procedure": [
              "Prepare initial state through unitary A encoding amplitude a.",
              "Construct Grover operator Q = −A S_0 A† S_χ.",
              "Apply Quantum Phase Estimation using controlled-Q operations.",
              "Measure phase register to estimate θ.",
              "Compute amplitude estimate â = sin²(θ)."
            ],
            "output": "Amplitude estimate â with quadratic improvement in precision.",
            "advantages": [
              "Provides quadratic speedup over classical Monte Carlo methods.",
              "Achieves high precision with fewer oracle queries.",
              "Forms the basis for quantum algorithms in finance, machine learning, and optimization."
            ],
            "limitations": [
              "Requires deep quantum circuits and many controlled operations.",
              "Highly sensitive to noise and gate errors.",
              "Phase estimation overhead can be large for near-term devices."
            ]
          },
    
          "Grover_Operator": {
            "name": "Grover Operator Construction for Amplitude Amplification",
            "purpose": "Build a unitary transformation that rotates the system state in a 2D subspace to encode amplitude information into a measurable phase.",
            "procedure": [
              "Prepare A|0⟩ as the superposition encoding amplitude a.",
              "Apply selective phase shifts S_χ to mark desired states.",
              "Apply S_0 to flip phase of |0⟩.",
              "Construct Q = −A S_0 A† S_χ.",
              "Repeatedly apply Q to amplify target amplitude as needed."
            ],
            "output": "Unitary operator used by amplitude estimation to encode amplitude into phase."
          },
    
          "Phase_Estimation": {
            "name": "Quantum Phase Estimation Subroutine",
            "purpose": "Estimate the phase θ of the Grover operator eigenvalues, enabling recovery of amplitude a = sin²(θ).",
            "procedure": [
              "Use m control qubits for resolution 1/2^m.",
              "Apply controlled-Q operations for powers Q^(2^k).",
              "Perform inverse QFT on control qubits.",
              "Measure to obtain estimate of θ."
            ],
            "output": "Phase estimate needed for recovering the amplitude."
          }
        }
      }
    },
    
    {
      "id": 83,
      "title": "International Asset Allocation with Regime Shifts",
      "authors": "Ang & Bekaert",
      "year": 2002,
      "source": "Review of Financial Studies",
      "url": "https://academic.oup.com/rfs/article/15/4/1137/1586418",
      "performance-metrics": [
        "Regime classification accuracy",
        "Log-likelihood of the estimated hidden Markov model",
        "Filtered and smoothed state probability stability",
        "Out-of-sample portfolio return performance",
        "Out-of-sample volatility reduction",
        "Sharpe Ratio improvement from regime-aware allocation",
        "Downside risk (expected shortfall) across regimes",
        "Transition probability stability across sample windows",
        "Predictive accuracy of regime-dependent excess returns",
        "Portfolio drawdown behavior in high-volatility vs low-volatility regimes"
      ],
      "tools-used-in": [
        "Hidden-Markov-Model-(HMM)"
      ],
      "paper-details": {
        "algorithms": {
          "Hidden_Markov_Model": {
            "name": "Hidden Markov Model (Regime-Switching Asset Allocation)",
            "purpose": "Capture time-varying return dynamics and volatility using latent market regimes that follow a Markov process.",
            "components": {
              "state_space": {
                "definition": "Two- or multi-state regimes representing low-volatility and high-volatility markets.",
                "role": "Each regime has its own mean return and covariance structure."
              },
              "transition_matrix": {
                "definition": "P(i→j) gives probability of moving from regime i to j.",
                "properties": [
                  "Rows sum to 1",
                  "Captures persistence of market conditions"
                ],
                "importance": "Regime persistence allows prediction of future states."
              },
              "emission_distributions": {
                "definition": "Regime-dependent normal distributions N(μ_s, Σ_s)",
                "role": "Model observed asset returns conditional on the hidden state."
              }
            },
            "procedure": [
              "Specify number of regimes (typically 2: calm and turbulent).",
              "Estimate regime-dependent means μ_s and covariances Σ_s.",
              "Estimate transition probabilities using maximum likelihood.",
              "Compute filtered and smoothed state probabilities via the forward–backward algorithm.",
              "Use predicted regime probabilities to allocate assets dynamically.",
              "Backtest portfolio across differing regime conditions."
            ],
            "output": "Regime-aware forecasts of returns, volatilities, and portfolio weights.",
            "advantages": [
              "Captures nonlinear and time-varying dynamics ignored by single-regime models.",
              "Improves diversification by adjusting for volatility regimes.",
              "Enhances risk management during crisis periods.",
              "Provides better out-of-sample performance than constant-parameter models."
            ],
            "limitations": [
              "Parameter estimation sensitive to sample size.",
              "Subject to local maxima in likelihood optimization.",
              "Interpretation of regimes can be ambiguous.",
              "Transition probabilities may vary in unstable markets."
            ]
          },
    
          "Regime_Dependent_Asset_Allocation": {
            "name": "Dynamic Asset Allocation Based on Regime Probabilities",
            "purpose": "Adjust portfolio weights according to predicted regime, enhancing returns and lowering risk.",
            "components": {
              "expected_returns": {
                "definition": "E[R_t | state s] = μ_s",
                "role": "Expected return varies across regimes."
              },
              "expected_covariance": {
                "definition": "Cov[R_t | state s] = Σ_s",
                "role": "Captures regime-varying risk structure."
              }
            },
            "procedure": [
              "Compute probability of being in each regime at time t using filtered probabilities.",
              "Form expected mean and covariance as weighted averages over regime probabilities.",
              "Solve mean–variance optimization for dynamic weights.",
              "Rebalance as regime probabilities update."
            ],
            "output": "Time-varying asset weights that adapt to market conditions."
          },
    
          "Forward_Backward_Estimation": {
            "name": "Forward–Backward Algorithm",
            "purpose": "Infer filtered (real-time) and smoothed (historical) probabilities of hidden regimes.",
            "procedure": [
              "Forward step: compute likelihood of states given data up to time t.",
              "Backward step: refine posterior probabilities using future observations.",
              "Combine results to obtain smoothed state probabilities.",
              "Use filtered probabilities for real-time allocation decisions."
            ],
            "output": "Probabilistic regime series used for forecasting and portfolio allocation."
          }
        }
      }
    },
    
    {
      "id": 84,
      "title": "Application of support vector machines in financial time series forecasting",
      "authors": "Tay & Cao",
      "year": 2001,
      "source": "Expert Systems",
      "url": "https://www.sciencedirect.com/science/article/pii/S0957417401000841",
      "performance-metrics": [
        "Mean Absolute Error (MAE)",
        "Root Mean Squared Error (RMSE)",
        "Mean Absolute Percentage Error (MAPE)",
        "Directional Prediction Accuracy (DPA)",
        "Hit-rate for correct up/down movement forecasts",
        "Out-of-sample forecasting error metrics",
        "Training loss vs generalization loss gap",
        "Convergence performance under different kernel functions",
        "Residual diagnostics for forecast stability",
        "Comparison against ARIMA and neural networks (relative RMSE reduction)"
      ],
      "tools-used-in": [
        "Support-Vector-Regression-(SVR)"
      ],
      "paper-details": {
        "algorithms": {
          "Support_Vector_Regression": {
            "name": "Support Vector Regression (SVR)",
            "purpose": "Forecast financial time series by finding a nonlinear regression function that minimizes generalization error using structural risk minimization.",
            "components": {
              "epsilon_insensitive_loss": {
                "definition": "Lε(y, f(x)) = max(0, |y − f(x)| − ε)",
                "role": "Ignores small errors within ε margin to improve robustness.",
                "effect": "Controls model’s tolerance to noise in financial time series."
              },
              "kernel_functions": {
                "types": [
                  "Radial Basis Function (RBF)",
                  "Polynomial kernel",
                  "Sigmoid kernel"
                ],
                "role": "Map inputs into high-dimensional space enabling nonlinear forecasting relationships."
              },
              "regularization_parameter_C": {
                "purpose": "Controls trade-off between model complexity and error penalty.",
                "effect": "Higher C → higher risk of overfitting; lower C → smoother function."
              }
            },
            "procedure": [
              "Normalize financial time series to remove scale effects.",
              "Choose kernel (RBF most effective in paper).",
              "Optimize SVR parameters C, ε, and kernel width σ.",
              "Train SVR using ε-insensitive loss and quadratic programming.",
              "Evaluate forecast accuracy on out-of-sample test set.",
              "Compare results with baseline models (ARIMA, ANN)."
            ],
            "output": "Nonlinear forecast of financial returns or prices using SVR.",
            "advantages": [
              "Effective for noisy and nonlinear financial data.",
              "Outperforms ARIMA and neural networks in many experiments.",
              "Strong generalization due to structural risk minimization.",
              "Works well with high-dimensional inputs after kernelization."
            ],
            "limitations": [
              "Requires careful hyperparameter tuning.",
              "Training can be slow for large datasets due to quadratic optimization.",
              "Model performance sensitive to kernel parameters."
            ]
          },
    
          "Kernel_Selection_and_Parameter_Tuning": {
            "name": "Kernel and Hyperparameter Optimization",
            "purpose": "Identify the best combination of C, ε, and kernel parameters to improve forecast accuracy.",
            "procedure": [
              "Test multiple kernel types (RBF, polynomial, sigmoid).",
              "Perform grid search over C, ε, and σ.",
              "Evaluate candidate models using validation error metrics.",
              "Select model minimizing RMSE and maximizing directional accuracy."
            ],
            "output": "Optimally tuned SVR forecaster."
          },
    
          "Feature_Normalization": {
            "name": "Normalization of Input Features",
            "purpose": "Prevent large-scale features from dominating kernel distance computations.",
            "procedure": [
              "Apply min–max or z-score normalization to input variables.",
              "Transform training and testing sets consistently.",
              "Feed normalized data into SVR for stable learning."
            ],
            "output": "Scaled input features enabling stable SVR optimization."
          }
        }
      }
    },
    
    {
      "id": 85,
      "title": "Recurrent Reinforcement Learning for trading",
      "authors": "Moody & Saffell",
      "year": 2001,
      "source": "Neural Networks",
      "url": "https://doi.org/10.1016/S0893-6080(01)00040-1",
      "performance-metrics": [
        "Sharpe Ratio (objective function maximized by the agent)",
        "Differential Sharpe Ratio (gradient-based performance metric)",
        "Total return and cumulative profit",
        "Drawdown and maximum drawdown",
        "Risk-adjusted return (Sortino-like evaluation via downside deviation)",
        "Hit-rate of profitable trading decisions",
        "Stability of policy weights over training iterations",
        "Out-of-sample generalization performance",
        "Transaction-cost adjusted return",
        "Variance of cumulative reward"
      ],
      "tools-used-in": [
        "Reinforcement-Learning-Trading-Agent"
      ],
      "paper-details": {
        "algorithms": {
          "Recurrent_Reinforcement_Learning": {
            "name": "Recurrent Reinforcement Learning (RRL) Framework",
            "purpose": "Learn an optimal trading strategy by directly maximizing a risk-adjusted performance metric (Sharpe Ratio) using recurrent neural network dynamics.",
            "components": {
              "recurrent_network": {
                "definition": "RNN with feedback connections modeling temporal dependencies in price series.",
                "role": "Captures nonlinear, time-dependent patterns for trading signals."
              },
              "policy_output": {
                "definition": "Trading position f_t ∈ [−1, 1] determined by RNN output.",
                "role": "Maps network state to long/short position."
              },
              "reward_function": {
                "definition": "Differential Sharpe Ratio gradient approximation.",
                "role": "Enables gradient ascent on Sharpe Ratio rather than conventional reward.",
                "advantage": "Direct risk-adjusted optimization avoids ad-hoc reward shaping."
              }
            },
            "procedure": [
              "Feed financial time-series data into recurrent neural network.",
              "Generate trading positions based on RNN hidden-state output.",
              "Compute portfolio returns using r_t · f_t.",
              "Update network weights by climbing gradient of Sharpe Ratio.",
              "Iterate until convergence on optimal risk-adjusted policy.",
              "Evaluate performance on unseen market data."
            ],
            "output": "A recurrent trading policy that maximizes risk-adjusted returns.",
            "advantages": [
              "Direct optimization of Sharpe Ratio—no need for proxy rewards.",
              "Captures nonlinear and temporal patterns through recurrence.",
              "Robust to non-Gaussian, noisy financial returns.",
              "Fully online update procedure—suitable for streaming data."
            ],
            "limitations": [
              "Sensitive to hyperparameters and learning-rate selection.",
              "Can overfit if market regimes change significantly.",
              "Non-convex optimization landscape may trap model in local minima.",
              "High variance in gradients due to noisy financial data."
            ]
          },
    
          "Differential_Sharpe_Ratio": {
            "name": "Differential Sharpe Ratio Optimization",
            "purpose": "Optimize the Sharpe Ratio over time using an online gradient formulation compatible with reinforcement learning.",
            "components": {
              "sharpe_ratio": {
                "definition": "SR = E[R] / √Var(R)",
                "role": "Measures risk-adjusted profitability of trading strategy."
              },
              "differential_form": {
                "definition": "∂SR/∂θ expressed as recursive update using exponential moving averages of returns.",
                "role": "Allows differentiability and gradient-based improvement."
              }
            },
            "procedure": [
              "Track mean and variance of returns via exponential estimators.",
              "Compute gradient of SR with respect to policy parameters θ.",
              "Update policy weights using gradient ascent.",
              "Repeat online as new price data arrives."
            ],
            "output": "Updated trading policy directly maximizing Sharpe Ratio."
          },
    
          "Trading_Position_Mapping": {
            "name": "Trading Position Mapping Function",
            "purpose": "Transform RNN output into a tradeable position with bounded exposure.",
            "procedure": [
              "Feed hidden-state vector into activation function (e.g., tanh).",
              "Scale output to range [−1, 1] representing short-to-long exposure.",
              "Multiply exposure by asset return to compute trading P&L.",
              "Feed resulting reward into Sharpe objective for gradient updates."
            ],
            "output": "Position series encoding long/short trading decisions."
          }
        }
      }
    },
    
    {
      "id": 86,
      "title": "Random Forests",
      "authors": "Breiman",
      "year": 2001,
      "source": "Machine Learning",
      "url": "https://link.springer.com/article/10.1023/A:1010933404324",
      "performance-metrics": [
        "Out-of-Bag (OOB) Error Rate",
        "Classification Accuracy / Regression MSE",
        "Gini Impurity Reduction",
        "Permutation Feature Importance",
        "Proximity Scores between samples",
        "Ensemble variance reduction",
        "Generalization error estimation via OOB samples",
        "Stability of predictions across different bootstraps",
        "ROC-AUC for classification tasks",
        "RMSE for regression tasks"
      ],
      "tools-used-in": [
        "Random-Forest-Gradient-Boosted-Trees-(XGBoost)"
      ],
      "paper-details": {
        "algorithms": {
          "Random_Forest": {
            "name": "Random Forest Algorithm",
            "purpose": "Construct an ensemble of decision trees using bootstrap aggregation and random feature selection to improve predictive accuracy and reduce overfitting.",
            "components": {
              "bootstrap_sampling": {
                "definition": "Random sampling with replacement to generate training subsets.",
                "role": "Creates diverse training datasets for each decision tree.",
                "benefit": "Reduces variance through ensemble averaging."
              },
              "random_feature_selection": {
                "definition": "Select a random subset of features at each split.",
                "role": "Induces tree diversity by altering split decisions.",
                "advantage": "Improves generalization and reduces correlation among trees."
              },
              "decision_tree_base_learners": {
                "definition": "Fully grown, unpruned CART trees.",
                "role": "High-variance learners combined to create a stable ensemble."
              },
              "out_of_bag_estimation": {
                "definition": "Use samples not included in bootstrap draws to estimate generalization error.",
                "role": "Internal validation without a separate test set.",
                "importance": "Provides unbiased error estimates."
              }
            },
            "procedure": [
              "Draw bootstrap sample from training dataset.",
              "Grow an unpruned decision tree using random subset of features at each node.",
              "Repeat for B trees to form an ensemble.",
              "Aggregate predictions via majority vote (classification) or averaging (regression).",
              "Evaluate performance using out-of-bag samples."
            ],
            "output": "Ensemble prediction with improved accuracy, stability, and robustness.",
            "advantages": [
              "Handles high-dimensional data well.",
              "Robust to noise and overfitting.",
              "Automatically estimates feature importance.",
              "Provides internal generalization error (OOB)."
            ],
            "limitations": [
              "Less interpretable than single decision trees.",
              "Large ensembles may be computationally intensive.",
              "Randomization can reduce model transparency.",
              "Not ideal when very strong linear relationships dominate."
            ]
          },
    
          "Feature_Importance": {
            "name": "Permutation and Gini-Based Feature Importance",
            "purpose": "Measure relevance of each predictor by evaluating its impact on prediction quality.",
            "components": {
              "gini_importance": {
                "definition": "Sum of impurity reductions from splits using the feature.",
                "role": "Fast to compute but biased toward high-cardinality predictors."
              },
              "permutation_importance": {
                "definition": "Increase in prediction error when feature values are randomly permuted.",
                "role": "More reliable measure of true predictive power.",
                "advantage": "Not biased by scale or cardinality."
              }
            },
            "procedure": [
              "Compute baseline OOB error.",
              "For each feature, permute values in OOB samples.",
              "Recompute OOB error and measure increase.",
              "Rank features based on error increase."
            ],
            "output": "Feature importance ranking for interpretability and model refinement."
          },
    
          "Proximity_Measures": {
            "name": "Proximity Matrix Calculation",
            "purpose": "Quantify similarity between data samples based on co-occurrence in leaf nodes across trees.",
            "procedure": [
              "Track which samples fall into the same terminal node in each tree.",
              "Increase proximity score for each pair that co-occurs.",
              "Normalize results to form proximity matrix.",
              "Use matrix for anomaly detection, clustering, or visualization."
            ],
            "output": "Sample similarity structure derived from the random forest ensemble."
          }
        }
      }
    },
    
    {
      "id": 87,
      "title": "Optimization of Conditional Value-at-Risk",
      "authors": "Rockafellar & Uryasev",
      "year": 2000,
      "source": "Journal of Risk",
      "url": "https://www.jstor.org/stable/2586997",
      "performance-metrics": [
        "CVaR minimization performance under different portfolio scenarios",
        "VaR exceedance frequency (backtesting metric)",
        "Comparison of optimized CVaR vs optimized VaR",
        "Tail-loss sensitivity for extreme events",
        "Stability of optimal weights under scenario perturbations",
        "Convergence rate of linear programming formulation",
        "Scenario-based risk estimation error",
        "Out-of-sample tail-risk reduction",
        "Robustness across heavy-tailed loss distributions"
      ],
      "tools-used-in": [
        "CVaR-Optimization"
      ],
      "paper-details": {
        "algorithms": {
          "Rockafellar_Uryasev_CVaR_Optimization": {
            "name": "Rockafellar–Uryasev CVaR Optimization Framework",
            "purpose": "Transform the minimization of Conditional Value-at-Risk (CVaR) into a tractable convex optimization problem using an auxiliary function and linear programming.",
            "components": {
              "auxiliary_function_F": {
                "definition": "F_α(w, η) = η + (1 / (1 − α)) · E[(L(w) − η)⁺]",
                "role": "Convex surrogate function whose minimizer gives both VaR and CVaR.",
                "effect": "Allows replacing non-smooth VaR optimization with convex optimization."
              },
              "eta_variable": {
                "definition": "η approximates VaR at confidence level α.",
                "role": "Splits loss function into tail and non-tail regions.",
                "importance": "Makes CVaR computable through linear programming."
              },
              "exceedance_variables": {
                "definition": "ξ_i = max{0, L_i(w) − η}",
                "role": "Measures scenario-by-scenario tail losses.",
                "benefit": "Turns tail losses into linear constraints."
              }
            },
            "procedure": [
              "Represent losses L_i(w) under N scenarios.",
              "Choose α confidence level (e.g., 95%).",
              "Introduce η and exceedance variables ξ_i.",
              "Form convex objective: minimize η + (1 / ((1 − α)N)) Σ ξ_i.",
              "Solve linear program for weights w, η, ξ_i.",
              "Recover VaR from η and CVaR from optimal objective value."
            ],
            "output": "Optimal portfolio weights that minimize CVaR at level α.",
            "advantages": [
              "Transforms hard VaR optimization into convex optimization.",
              "Guarantees a global optimum due to convexity.",
              "Works for arbitrary loss distributions, including heavy-tailed ones.",
              "Applicable to large-scale portfolio and risk management problems."
            ],
            "limitations": [
              "Scenario generation must be accurate for good results.",
              "LP size grows with number of scenarios, increasing computation.",
              "Assumes losses can be linearly approximated in portfolio weights."
            ]
          },
    
          "CVaR_Estimation": {
            "name": "CVaR Estimation via Auxiliary Function Minimization",
            "purpose": "Compute CVaR by minimizing the convex auxiliary function F_α(w, η).",
            "procedure": [
              "Fix portfolio weights w or consider w as optimization variable.",
              "Minimize F_α(w, η) with respect to η.",
              "Determine η* as VaR estimate.",
              "Compute CVaR = F_α(w, η*)."
            ],
            "output": "Closed-form CVaR estimate for general loss distributions."
          },
    
          "Scenario_Based_Formulation": {
            "name": "Scenario-Based CVaR Linear Programming Formulation",
            "purpose": "Convert CVaR optimization into a finite-dimensional LP for computational tractability.",
            "procedure": [
              "Generate or sample loss scenarios L_i(w).",
              "Introduce linear constraints ξ_i ≥ L_i(w) − η.",
              "Enforce ξ_i ≥ 0.",
              "Minimize objective with respect to w, η, ξ_i.",
              "Solve LP using simplex or interior-point methods."
            ],
            "output": "Portfolio optimization framework compatible with real-world risk scenarios."
          }
        }
      }
    },
    
    {
      "id": 88,
      "title": "Finite Mixture Models",
      "authors": "McLachlan & Peel",
      "year": 2000,
      "source": "Wiley",
      "url": "https://www.wiley.com/en-us/Finite+Mixture+Models-p-9780471006268",
      "performance-metrics": [
        "Log-likelihood of mixture model fit",
        "Akaike Information Criterion (AIC)",
        "Bayesian Information Criterion (BIC)",
        "Integrated Completed Likelihood (ICL)",
        "Classification accuracy of component assignments",
        "Convergence rate of EM algorithm",
        "Stability of estimated mixture parameters across initializations",
        "Posterior probability purity of clusters",
        "Likelihood ratio test for number of components",
        "Out-of-sample density estimation error"
      ],
      "tools-used-in": [
        "Gaussian-Mixture-Model-(GMM)"
      ],
      "paper-details": {
        "algorithms": {
          "Finite_Mixture_Model": {
            "name": "Finite Mixture Model",
            "purpose": "Model heterogeneous data as arising from a mixture of multiple probability distributions, usually Gaussian components, enabling clustering, density estimation, and classification.",
            "components": {
              "mixture_distribution": {
                "definition": "p(x) = Σ_{k=1}^K π_k f_k(x | θ_k)",
                "role": "Represents overall density as weighted sum of component densities.",
                "parameters": {
                  "π_k": "Mixture weight for component k (Σ π_k = 1)",
                  "f_k": "Component density (often Gaussian)",
                  "θ_k": "Parameters of each component (mean, covariance)"
                }
              },
              "latent_variables": {
                "definition": "z_{ik} = 1 if observation i comes from component k",
                "role": "Forms basis for EM algorithm expectation step."
              }
            },
            "procedure": [
              "Choose number of mixture components K.",
              "Initialize component weights π_k and parameters θ_k.",
              "Iterate EM until convergence:",
              "  - E-step: Compute responsibilities γ_{ik} = P(z_{ik}=1 | x_i).",
              "  - M-step: Update π_k and θ_k using weighted maximum likelihood.",
              "Evaluate convergence using log-likelihood or parameter changes."
            ],
            "output": "Estimated mixture parameters and posterior probabilities for each component.",
            "advantages": [
              "Flexible modeling for multimodal and heterogeneous data.",
              "Probabilistic cluster assignments provide soft clustering.",
              "General framework extends beyond Gaussian mixtures to many distributions."
            ],
            "limitations": [
              "Sensitive to initialization; EM may converge to local optima.",
              "Choosing number of components K is nontrivial.",
              "Gaussian mixture assumption may fail for heavy-tailed or skewed data."
            ]
          },
    
          "EM_Algorithm": {
            "name": "Expectation–Maximization (EM) Algorithm for Mixture Models",
            "purpose": "Iteratively estimate component parameters and latent memberships in finite mixture models.",
            "components": {
              "e_step": {
                "definition": "Compute posterior responsibilities γ_{ik}",
                "role": "Soft assignment of each observation to components."
              },
              "m_step": {
                "definition": "Update parameters using γ_{ik} as weights",
                "updates": [
                  "π_k = (1/n) Σ_i γ_{ik}",
                  "μ_k = (Σ_i γ_{ik} x_i) / (Σ_i γ_{ik})",
                  "Σ_k = (Σ_i γ_{ik} (x_i−μ_k)(x_i−μ_k)ᵀ) / (Σ_i γ_{ik})"
                ]
              }
            },
            "procedure": [
              "Start with initial parameters θ_k and weights π_k.",
              "E-step: Compute γ_{ik}.",
              "M-step: Update π_k, μ_k, Σ_k.",
              "Check log-likelihood for monotonic increase.",
              "Repeat until convergence."
            ],
            "output": "Maximum likelihood estimates for mixture parameters."
          },
    
          "Model_Selection_for_Mixtures": {
            "name": "Model Selection for Mixture Components",
            "purpose": "Determine the optimal number of mixture components using statistical criteria.",
            "procedure": [
              "Fit mixture models for various K values.",
              "Compute AIC, BIC, and ICL for each K.",
              "Select model with lowest criterion score.",
              "Validate cluster interpretability and stability."
            ],
            "output": "Optimal number of mixture components for clustering or density modeling."
          }
        }
      }
    },
    
    {
      "id": 89,
      "title": "Quantum annealing and Boltzmann machines",
      "authors": "Kadowaki & Nishimori",
      "year": 1998,
      "source": "PRE",
      "url": "https://journals.aps.org/pre/abstract/10.1103/PhysRevE.58.5355",
      "performance-metrics": [
        "Ground-state convergence probability",
        "Energy gap scaling during annealing",
        "Annealing time vs solution accuracy",
        "Convergence rate under quantum fluctuations",
        "Residual energy after annealing schedule completion",
        "Comparison of quantum annealing vs simulated annealing performance",
        "Success probability under different annealing schedules",
        "Sensitivity to transverse field strength",
        "Robustness to thermal noise in quantum Boltzmann sampling"
      ],
      "tools-used-in": [
        "Quantum-Boltzmann-Machine-(QBM)"
      ],
      "paper-details": {
        "algorithms": {
          "Quantum_Annealing": {
            "name": "Quantum Annealing (QA)",
            "purpose": "Solve combinatorial optimization problems by gradually reducing quantum fluctuations that help escape local minima.",
            "components": {
              "problem_hamiltonian": {
                "definition": "H_P encodes the optimization objective via energy landscape.",
                "role": "Ground state of H_P corresponds to optimal solution."
              },
              "transverse_field_hamiltonian": {
                "definition": "H_T = −Γ Σ σ_x",
                "role": "Introduces quantum tunneling across energy barriers.",
                "effect": "Enables exploration of configuration space not possible via thermal jumps alone."
              },
              "annealing_schedule": {
                "definition": "H(t) = A(t) H_T + B(t) H_P",
                "role": "Gradually decreases A(t) and increases B(t).",
                "importance": "Adiabatic evolution ensures convergence to ground state if slow enough."
              }
            },
            "procedure": [
              "Define optimization problem via problem Hamiltonian H_P.",
              "Initialize system in ground state of transverse-field Hamiltonian H_T.",
              "Apply annealing schedule decreasing quantum fluctuations.",
              "Evolve system adiabatically toward H_P.",
              "Measure final state to obtain solution candidate."
            ],
            "output": "Approximate or exact solution to optimization objective encoded in quantum Hamiltonian.",
            "advantages": [
              "Capable of escaping local minima using quantum tunneling.",
              "May converge faster than simulated annealing for rugged landscapes.",
              "Forms theoretical foundation for modern quantum annealers (e.g., D-Wave)."
            ],
            "limitations": [
              "Requires adiabatically slow evolution for guaranteed optimality.",
              "Energy-gap bottlenecks can drastically increase computation time.",
              "Sensitive to noise and decoherence in physical implementations."
            ]
          },
    
          "Quantum_Boltzmann_Machine": {
            "name": "Quantum Boltzmann Machine (QBM) Framework",
            "purpose": "Generalize classical Boltzmann machines using quantum tunneling and transverse-field Hamiltonians for enhanced sampling and optimization.",
            "components": {
              "quantum_energy_function": {
                "definition": "H = H_P + H_T combining classical Ising energy with quantum term.",
                "role": "Defines probabilistic distribution via quantum partition function."
              },
              "quantum_distribution": {
                "definition": "ρ = e^{-βH} / Z",
                "role": "Generalizes classical Gibbs distribution to quantum domain.",
                "advantage": "Enables richer representational capacity and better sampling."
              }
            },
            "procedure": [
              "Encode machine parameters into H_P.",
              "Introduce transverse field for quantum enhancement.",
              "Run quantum annealing or sampling to approximate partition function.",
              "Optimize parameters using gradient-based updates with quantum statistics."
            ],
            "output": "Quantum-enhanced generative model capable of representing complex data distributions."
          },
    
          "Simulated_vs_Quantum_Annealing_Comparison": {
            "name": "Comparison of Classical and Quantum Annealing",
            "purpose": "Evaluate how quantum fluctuations outperform thermal fluctuations in escaping local minima.",
            "procedure": [
              "Construct identical energy landscapes for SA and QA.",
              "Run annealing under varied temperature (SA) and transverse field (QA).",
              "Compare convergence rates and success probabilities.",
              "Analyze role of tunneling vs thermal hopping."
            ],
            "output": "Evidence supporting potential computational advantages of quantum annealing."
          }
        }
      }
    },
    
    {
      "id": 90,
      "title": "Long Short-Term Memory",
      "authors": "Hochreiter & Schmidhuber",
      "year": 1997,
      "source": "Neural Computation",
      "url": "https://www.bioinf.jku.at/publications/older/2604.pdf",
      "performance-metrics": [
        "Sequence prediction accuracy",
        "Gradient stability (vanishing/exploding gradient mitigation)",
        "Long-horizon dependency retention ability",
        "Training loss convergence speed",
        "Test loss (generalization error)",
        "Perplexity (for language modeling tasks)",
        "Mean Squared Error (MSE) for regression sequences",
        "Classification accuracy for sequence labeling",
        "Robustness across long temporal contexts",
        "Memory retention performance measured via synthetic tasks (e.g., adding problem)"
      ],
      "tools-used-in": [
        "LSTM-(Long-Short-Term-Memory)-Neural-Network"
      ],
      "paper-details": {
        "algorithms": {
          "LSTM": {
            "name": "Long Short-Term Memory Network",
            "purpose": "Address the vanishing and exploding gradient problems in recurrent neural networks by introducing memory cells with gated control mechanisms.",
            "components": {
              "cell_state": {
                "definition": "C_t, a linear memory pathway allowing long-term information flow.",
                "role": "Preserves information over long time intervals with minimal modification.",
                "advantage": "Solves vanishing gradient problem by maintaining constant error flow."
              },
              "input_gate": {
                "definition": "i_t = σ(W_i·[h_{t−1}, x_t] + b_i)",
                "role": "Controls how much new information enters the cell state."
              },
              "forget_gate": {
                "definition": "f_t = σ(W_f·[h_{t−1}, x_t] + b_f)",
                "role": "Determines which past information should be forgotten.",
                "importance": "Allows selective memory decay and prevents irrelevant accumulation."
              },
              "output_gate": {
                "definition": "o_t = σ(W_o·[h_{t−1}, x_t] + b_o)",
                "role": "Controls how much of the cell state influences the hidden output."
              },
              "candidate_memory": {
                "definition": "C̃_t = tanh(W_C·[h_{t−1}, x_t] + b_C)",
                "role": "Proposes new memory content to be blended with existing cell state."
              }
            },
            "procedure": [
              "Compute input, forget, and output gate activations.",
              "Update cell state using: C_t = f_t ⊙ C_{t−1} + i_t ⊙ C̃_t.",
              "Compute hidden state using: h_t = o_t ⊙ tanh(C_t).",
              "Use h_t for predictions and error backpropagation.",
              "Train network via backpropagation through time (BPTT)."
            ],
            "output": "Hidden state sequence capable of learning long-term dependencies without gradient decay.",
            "advantages": [
              "Mitigates vanishing gradient problem via linear memory cell.",
              "Learns long-term dependencies more effectively than standard RNNs.",
              "Highly versatile for sequence modeling tasks (speech, language, time series).",
              "Gating mechanism provides fine-grained control of memory updates."
            ],
            "limitations": [
              "Computationally more expensive than simple RNNs.",
              "More parameters increase overfitting risk without proper regularization.",
              "Training can be slow for very long sequences."
            ]
          },
    
          "Gradient_Flow_Analysis": {
            "name": "Constant Error Carousel (CEC)",
            "purpose": "Explain how LSTM maintains stable gradients over long sequences.",
            "components": {
              "gradient_preservation": {
                "definition": "CEC ensures ∂C_t/∂C_{t−k} remains close to 1.",
                "role": "Prevents exponential decay of gradients seen in classical RNNs."
              }
            },
            "procedure": [
              "Analyze cell state derivative flow.",
              "Show that memory cell’s linear path preserves gradient magnitude.",
              "Demonstrate mitigation of both vanishing and exploding gradients."
            ],
            "output": "Theoretical justification for LSTM’s long-term memory capability."
          },
    
          "Sequence_Learning_Tasks": {
            "name": "Synthetic Long-Term Dependency Tasks",
            "purpose": "Evaluate LSTM’s ability to learn relationships over long horizons.",
            "tasks": [
              "Adding task",
              "Multiplication task",
              "Temporal order memory tasks"
            ],
            "procedure": [
              "Generate synthetic sequences requiring long-term memory.",
              "Train LSTM and compare with classical RNN and feedforward networks.",
              "Measure performance using accuracy or MSE depending on task.",
              "Analyze LSTM’s ability to maintain error flow over long sequences."
            ],
            "output": "Empirical demonstration of LSTM’s superiority in long-range dependency learning."
          }
        }
      }
    },
    
    {
      "id": 91,
      "title": "The Nature of Statistical Learning Theory",
      "authors": "Vapnik",
      "year": 1995,
      "source": "Springer",
      "url": "https://link.springer.com/book/10.1007/978-1-4757-2440-0",
      "performance-metrics": [
        "Generalization error bounds",
        "VC-dimension capacity measure",
        "Empirical risk vs structural risk evaluation",
        "Margin maximization effectiveness",
        "Training error vs test error gap",
        "Complexity regularization performance",
        "Convergence rate of empirical risk minimization (ERM)",
        "Stability of solutions under different kernels",
        "Robustness of margin-based classifiers",
        "Prediction error under various loss functions"
      ],
      "tools-used-in": [
        "Support-Vector-Regression-(SVR)"
      ],
      "paper-details": {
        "algorithms": {
          "Statistical_Learning_Theory": {
            "name": "Statistical Learning Theory (SLT)",
            "purpose": "Provide the mathematical foundations for machine learning by analyzing generalization, model complexity, and optimal learning rules.",
            "components": {
              "vc_dimension": {
                "definition": "Maximum number of points that a hypothesis class can shatter.",
                "role": "Quantifies model capacity and generalization ability.",
                "importance": "High VC dimension → risk of overfitting; low VC dimension → underfitting."
              },
              "empirical_risk_minimization": {
                "definition": "Minimize average loss on training data.",
                "role": "Forms core of classical learning but may overfit without capacity control."
              },
              "structural_risk_minimization": {
                "definition": "Minimize bound: Empirical Risk + Complexity Penalty.",
                "role": "Balances bias–variance trade-off using nested hypothesis classes.",
                "benefit": "Foundation for SVM regularization."
              }
            },
            "procedure": [
              "Define hypothesis class and compute empirical risk.",
              "Analyze VC dimension to assess complexity.",
              "Select optimal model via Structural Risk Minimization.",
              "Optimize chosen learning rule (e.g., SVM).",
              "Evaluate generalization bounds."
            ],
            "output": "Theoretical guarantees on learning performance and generalization ability.",
            "advantages": [
              "Provides rigorous mathematical framework for machine learning.",
              "Explains why models generalize despite high dimensionality.",
              "Foundation for SVM, SVR, and margin-based learning."
            ],
            "limitations": [
              "Theory can be conservative and may not always match empirical performance.",
              "Exact VC-dimension computation is difficult for many hypothesis classes.",
              "Does not directly address modern deep learning complexity."
            ]
          },
    
          "Support_Vector_Regression_Framework": {
            "name": "Support Vector Regression (SVR) — Theoretical Foundation",
            "purpose": "Apply SLT principles to regression tasks by minimizing empirical error with structural risk control via margin-based loss.",
            "components": {
              "epsilon_insensitive_loss": {
                "definition": "Lε(y, f(x)) = max(0, |y − f(x)| − ε)",
                "role": "Defines a margin of tolerance for regression errors.",
                "importance": "Creates sparse support vector representation."
              },
              "kernel_trick": {
                "definition": "Use kernel function k(x, x′) to enable nonlinear mappings.",
                "role": "Implements regression in high-dimensional feature spaces efficiently."
              },
              "regularization_term": {
                "definition": "‖w‖² penalty in SVR optimization.",
                "role": "Controls complexity of the regression function.",
                "benefit": "Ensures model adheres to Structural Risk Minimization."
              }
            },
            "procedure": [
              "Define regression objective using epsilon-insensitive loss.",
              "Map data into high-dimensional feature space using kernel.",
              "Solve convex quadratic optimization problem.",
              "Obtain sparse representation where only support vectors define decision function.",
              "Predict outputs using kernel expansion."
            ],
            "output": "Sparse, generalizable regression model consistent with learning theory.",
            "advantages": [
              "Strong generalization guarantees due to SRM.",
              "Convex optimization yields global optimum.",
              "Effective for nonlinear regression tasks with proper kernel selection."
            ]
          },
    
          "Generalization_Bounds": {
            "name": "VC-Based Generalization Bounds",
            "purpose": "Provide upper bounds on test error based on training error and model complexity.",
            "procedure": [
              "Determine VC dimension of hypothesis class.",
              "Estimate empirical risk on training data.",
              "Apply Vapnik–Chervonenkis inequality to compute bound.",
              "Use bound to guide model selection via SRM."
            ],
            "output": "Statistical guarantees on predictive performance."
          }
        }
      }
    },
    
    {
      "id": 92,
      "title": "Autoregressive conditional heteroskedasticity and changes in regime",
      "authors": "Hamilton & Susmel",
      "year": 1994,
      "source": "Journal of Econometrics",
      "url": "https://www.sciencedirect.com/science/article/pii/0304407694900208",
      "performance-metrics": [
        "Log-likelihood of regime-switching GARCH model",
        "Filtered and smoothed regime classification accuracy",
        "Volatility persistence across regimes",
        "Likelihood ratio test for regime-switching vs single-regime GARCH",
        "In-sample volatility forecast error (MAE, RMSE)",
        "Out-of-sample volatility prediction accuracy",
        "Transition probability stability across sample windows",
        "Fit improvement over standard GARCH in high-volatility periods",
        "Regime duration and expected regime lengths",
        "Comparison of unconditional vs regime-specific variance estimates"
      ],
      "tools-used-in": [
        "Regime-Switching-GARCH"
      ],
      "paper-details": {
        "algorithms": {
          "Regime_Switching_GARCH": {
            "name": "Regime-Switching GARCH (SWARCH) Model",
            "purpose": "Model volatility dynamics using GARCH structure with discrete regime changes governed by a Markov process.",
            "components": {
              "markov_regime_process": {
                "definition": "s_t ∈ {1, 2, …, K}, with transition matrix P(i→j)",
                "role": "Determines current volatility regime (e.g., calm vs turbulent).",
                "importance": "Captures sudden shifts in volatility level."
              },
              "regime_specific_volatility": {
                "definition": "h_t(s_t) = ω_{s_t} + α_{s_t} ε_{t−1}² + β_{s_t} h_{t−1}",
                "role": "Allows different GARCH dynamics depending on regime.",
                "benefit": "Provides better fit to clustering and jumps in volatility."
              },
              "conditional_mean_model": {
                "definition": "y_t = μ + ε_t, ε_t = √h_t(s_t) · z_t",
                "role": "Separates mean and volatility processes."
              }
            },
            "procedure": [
              "Specify number of regimes (often 2: high and low volatility).",
              "Initialize parameters for regime-specific GARCH equations.",
              "Estimate transition probabilities using maximum likelihood.",
              "Compute filtered probabilities via forward recursion.",
              "Compute smoothed probabilities via forward–backward algorithm.",
              "Evaluate model fit and compare with standard GARCH.",
              "Use regime-specific volatility forecasts for risk modeling."
            ],
            "output": "Volatility estimates and probabilities of latent regimes over time.",
            "advantages": [
              "Captures volatility shifts unmodeled by classical GARCH.",
              "Allows asymmetric persistence across regimes.",
              "Improves fit to financial time series with structural breaks.",
              "Enhances risk forecasting in turbulent periods."
            ],
            "limitations": [
              "Estimation is computationally intensive.",
              "Likelihood surface may have multiple local maxima.",
              "Choosing number of regimes is nontrivial.",
              "Interpretation of regimes may be ambiguous."
            ]
          },
    
          "Forward_Backward_Algorithm_for_Regimes": {
            "name": "Hidden Markov Forward–Backward Algorithm",
            "purpose": "Infer filtered and smoothed probabilities of volatility regimes.",
            "procedure": [
              "Forward step: compute state likelihoods p(s_t | y_1:t).",
              "Backward step: incorporate future information for smoothing.",
              "Combine both to obtain complete regime probability sequence.",
              "Use smoothed probabilities for parameter re-estimation or forecasting."
            ],
            "output": "Posterior probabilities of each volatility regime at every time step."
          },
    
          "Likelihood_Ratio_Testing": {
            "name": "Likelihood Ratio Test for Regime Switching",
            "purpose": "Determine whether regime-switching GARCH provides significantly better fit than classical GARCH.",
            "procedure": [
              "Estimate single-regime GARCH model.",
              "Estimate SWARCH model with K regimes.",
              "Compute LR statistic: LR = 2(ℓ_SWARCH − ℓ_GARCH).",
              "Evaluate significance using nonstandard distribution.",
              "Determine validity of structural regime changes."
            ],
            "output": "Statistical evidence supporting or rejecting regime-switching behavior."
          }
        }
      }
    },
    
    {
      "id": 93,
      "title": "Returns to Buying Winners and Selling Losers",
      "authors": "Jegadeesh & Titman",
      "year": 1993,
      "source": "Journal of Finance",
      "url": "https://www.jstor.org/stable/2328882",
      "performance-metrics": [
        "Average monthly momentum return (winner minus loser portfolio)",
        "Sharpe Ratio of momentum strategy",
        "t-statistics for momentum profitability",
        "Long-horizon return persistence (3–12 month formation)",
        "Autocorrelation of returns across formation periods",
        "Drawdown and crash sensitivity during market reversals",
        "Turnover and transaction-cost adjusted returns",
        "Cross-sectional consistency of momentum profits",
        "Volatility-adjusted return metrics",
        "Comparison against CAPM and Fama–French alpha"
      ],
      "tools-used-in": [
        "Momentum-Strategy"
      ],
      "paper-details": {
        "algorithms": {
          "Cross_Sectional_Momentum": {
            "name": "Cross-Sectional Momentum Strategy",
            "purpose": "Construct portfolios by ranking assets on past returns and exploiting short- to mid-term return continuation.",
            "components": {
              "formation_period": {
                "definition": "Lookback window of 3–12 months to compute past returns.",
                "role": "Identifies recent winners and losers.",
                "effect": "Determines strength and persistence of momentum."
              },
              "holding_period": {
                "definition": "Hold long/short positions for 3–12 months after formation.",
                "role": "Captures return continuation post-formation."
              },
              "ranking_mechanism": {
                "definition": "Sort assets into deciles based on cumulative past returns.",
                "role": "Top decile = winners (long), bottom decile = losers (short)."
              }
            },
            "procedure": [
              "Compute past returns over formation window (e.g., 12 months).",
              "Rank all stocks into deciles based on returns.",
              "Form WML (Winner–Minus–Loser) portfolio: long top decile, short bottom decile.",
              "Hold portfolio for k months while forming new overlapping portfolios each month.",
              "Evaluate performance using alphas, Sharpe Ratio, and t-stats."
            ],
            "output": "Winner–Minus–Loser (WML) momentum return series.",
            "advantages": [
              "Strong and statistically significant returns across decades.",
              "Simple, rules-based, and robust across markets.",
              "Low correlation with value and other equity factors."
            ],
            "limitations": [
              "Susceptible to momentum crashes during sharp reversals.",
              "High turnover increases transaction costs.",
              "Performance varies across market regimes."
            ]
          },
    
          "Factor_Adjusted_Performance": {
            "name": "Factor-Adjusted Alpha Computation",
            "purpose": "Evaluate whether momentum profits persist after controlling for risk factors.",
            "procedure": [
              "Regress WML returns on CAPM beta.",
              "Extend regression with Fama–French factors (MKT, SMB, HML).",
              "Obtain factor-adjusted alpha.",
              "Assess statistical significance using t-statistics."
            ],
            "output": "Risk-adjusted abnormal return confirming momentum anomaly."
          },
    
          "Return_Decomposition": {
            "name": "Decomposition of Momentum Profits",
            "purpose": "Identify sources of momentum return: continuation vs reversal components.",
            "procedure": [
              "Separate winners and losers into subgroups based on long-term past returns.",
              "Analyze return behavior beyond holding periods.",
              "Measure continuation (0–12 months) and long-term reversal (3–5 years)."
            ],
            "output": "Attribution of momentum profits to short-run continuation and long-run reversal."
          }
        }
      }
    },
    
    {
      "id": 94,
      "title": "Global Portfolio Optimization",
      "authors": "Black & Litterman",
      "year": 1992,
      "source": "Financial Analysts Journal",
      "url": "https://www.jstor.org/stable/4479577",
      "performance-metrics": [
        "Stability of posterior expected returns",
        "Out-of-sample portfolio return performance",
        "Tracking error relative to equilibrium benchmark",
        "Portfolio volatility reduction compared to Markowitz optimization",
        "Sensitivity of optimal weights to investor views",
        "Sharpe Ratio improvement from blended return estimates",
        "Robustness to errors in covariance estimates",
        "Consistency of optimal weights across varying confidence levels",
        "Turnover and transaction-cost impact",
        "Diversification improvement measured via effective number of assets"
      ],
      "tools-used-in": [
        "Black–Litterman"
      ],
      "paper-details": {
        "algorithms": {
          "Black_Litterman_Model": {
            "name": "Black–Litterman Global Portfolio Optimization Model",
            "purpose": "Blend market equilibrium returns with investor views to generate stable, intuitive, and diversified optimal portfolio weights.",
            "components": {
              "equilibrium_excess_returns": {
                "definition": "π = λ Σ w_mkt",
                "role": "Represents implied returns consistent with global CAPM.",
                "importance": "Starting point for blending with subjective views."
              },
              "investor_views": {
                "definition": "Q: expected returns for selected assets or spreads",
                "role": "Incorporate subjective or model-based expectations.",
                "types": [
                  "Absolute views: expected return for a specific asset",
                  "Relative views: expected outperformance of one asset vs another"
                ]
              },
              "confidence_matrix": {
                "definition": "Ω, covariance matrix of view errors",
                "role": "Controls strength of each view in the posterior combination."
              },
              "blending_formula": {
                "definition": "Posterior returns: μ* = [ (τΣ)^−1 π + Pᵀ Ω^−1 Q ]⁻¹ [ (τΣ)^−1 π + Pᵀ Ω^−1 Q ]",
                "role": "Combines equilibrium returns and views weighted by confidence.",
                "benefit": "Produces stable and intuitive expected returns."
              }
            },
            "procedure": [
              "Estimate equilibrium return vector using global market-cap weights.",
              "Specify investor absolute or relative views.",
              "Construct view linking matrix P and uncertainty matrix Ω.",
              "Choose scalar τ to scale prior estimate uncertainty.",
              "Compute posterior expected returns using Black–Litterman formula.",
              "Perform mean–variance optimization with updated returns μ*.",
              "Evaluate resulting portfolio for diversification and stability."
            ],
            "output": "Posterior expected returns and optimal portfolio weights consistent with both market equilibrium and investor views.",
            "advantages": [
              "Mitigates instability and unintuitive weights from mean–variance optimization.",
              "Allows flexible incorporation of subjective information.",
              "Produces well-diversified, stable portfolios.",
              "Reduces sensitivity to noisy expected return estimates.",
              "Smoothly transitions between equilibrium portfolio and view-driven portfolio."
            ],
            "limitations": [
              "Requires estimation of τ and view confidence Ω, which may be subjective.",
              "Model complexity increases for many simultaneous views.",
              "Outcome quality depends on accuracy of covariance matrix Σ.",
              "Interpretation of absolute vs relative views can be nontrivial."
            ]
          },
    
          "Equilibrium_Return_Estimation": {
            "name": "Reverse Optimization for Equilibrium Returns",
            "purpose": "Derive implied expected returns consistent with observed market-cap weights.",
            "procedure": [
              "Obtain global market weights w_mkt.",
              "Estimate risk aversion parameter λ.",
              "Compute π = λ Σ w_mkt.",
              "Use π as prior for blending with investor views."
            ],
            "output": "Equilibrium return vector consistent with CAPM."
          },
    
          "Posterior_Portfolio_Optimization": {
            "name": "Mean–Variance Optimization Using Posterior Returns",
            "purpose": "Optimize portfolio using stabilized expected returns generated by the Black–Litterman model.",
            "procedure": [
              "Input posterior expected returns μ* and covariance Σ.",
              "Solve quadratic optimization for optimal weights.",
              "Evaluate diversification and tracking error.",
              "Adjust view confidence parameters if necessary."
            ],
            "output": "Optimized global portfolio with improved stability and diversification."
          }
        }
      }
    },
    
    {
      "id": 95,
      "title": "The Markowitz Optimization Enigma: Is 'Optimized' Optimal?",
      "authors": "Michaud",
      "year": 1989,
      "source": "Financial Analysts Journal",
      "url": "https://www.jstor.org/stable/4479057",
      "performance-metrics": [
        "Out-of-sample portfolio return",
        "Out-of-sample portfolio volatility",
        "Stability of portfolio weights",
        "Turnover and transaction-cost adjusted performance",
        "Sharpe Ratio comparison (optimized vs resampled)",
        "Sensitivity of optimal weights to estimation error",
        "Frontier stability across repeated samples",
        "Diversification score (effective number of assets)",
        "Tracking error vs equilibrium or benchmark portfolio",
        "Variance of portfolio returns across bootstrap samples"
      ],
      "tools-used-in": [
        "Markowitz-Mean–Variance"
      ],
      "paper-details": {
        "algorithms": {
          "Resampled_Efficient_Frontier": {
            "name": "Resampled Efficient Frontier",
            "purpose": "Stabilize Markowitz portfolio optimization by replacing single-point parameter estimates with averaged results from repeated resampling.",
            "components": {
              "bootstrap_sampling": {
                "definition": "Generate many synthetic return datasets via resampling with replacement.",
                "role": "Represents parameter uncertainty in mean returns and covariance matrices.",
                "benefit": "Reduces sensitivity of optimal weights to sampling noise."
              },
              "mean_variance_optimization": {
                "definition": "Optimize each bootstrap sample to obtain a set of efficient portfolios.",
                "role": "Produces diverse solutions reflecting estimation variability."
              },
              "portfolio_averaging": {
                "definition": "Average optimal weights across bootstrap optimizations.",
                "role": "Creates more stable and diversified portfolios.",
                "advantage": "Improves out-of-sample performance vs classical Markowitz optimization."
              }
            },
            "procedure": [
              "Generate many bootstrap samples of historical returns.",
              "Perform mean–variance optimization for each sample.",
              "Collect optimized weights from all bootstrap runs.",
              "Average weights across runs to obtain resampled portfolio.",
              "Plot resulting resampled efficient frontier.",
              "Compare stability and performance vs traditional Markowitz frontier."
            ],
            "output": "More stable and diversified portfolio weights that generalize better out of sample.",
            "advantages": [
              "Mitigates extreme and unintuitive weights from classical Markowitz optimization.",
              "Reduces estimation error impact on portfolio construction.",
              "Produces smoother, more realistic efficient frontiers.",
              "Improves out-of-sample performance and reduces turnover."
            ],
            "limitations": [
              "Computationally expensive due to repeated optimizations.",
              "Results depend on bootstrap sampling scheme.",
              "Does not eliminate dependence on mean estimates entirely.",
              "Theoretical justification was debated in early literature."
            ]
          },
    
          "Estimation_Error_Analysis": {
            "name": "Parameter Estimation Error Diagnostic",
            "purpose": "Analyze how errors in mean return and covariance estimates distort Markowitz portfolio weights.",
            "components": {
              "mean_estimation_error": {
                "definition": "Deviation between sample mean and true expected return.",
                "effect": "Even small errors can drastically shift optimal weights."
              },
              "covariance_estimation_error": {
                "definition": "Instability in estimated covariance matrix from finite samples.",
                "effect": "Leads to extreme and highly concentrated portfolios."
              }
            },
            "procedure": [
              "Evaluate sensitivity of optimal portfolio weights to perturbations in inputs.",
              "Perform simulations with slightly altered return/covariance estimates.",
              "Compare optimized weights across simulation runs.",
              "Assess magnitude of estimation-driven instability."
            ],
            "output": "Quantitative measure of fragility in classical Markowitz optimization."
          },
    
          "Stability_Comparison": {
            "name": "Frontier Stability and Diversification Comparison",
            "purpose": "Compare classical Markowitz frontier with resampled frontier on stability and diversification metrics.",
            "procedure": [
              "Plot efficient frontier for classical and resampled methods.",
              "Compute diversification metrics such as effective number of assets.",
              "Evaluate turnover and sensitivity to sample changes.",
              "Assess smoothness and robustness of each frontier."
            ],
            "output": "Empirical evidence supporting the superiority of resampling for practical portfolio construction."
          }
        }
      }
    },
    
    {
      "id": 96,
      "title": "A New Approach to the Economic Analysis of Nonstationary Time Series",
      "authors": "Hamilton",
      "year": 1989,
      "source": "Econometrica",
      "url": "https://www.jstor.org/stable/1912559",
      "performance-metrics": [
        "Log-likelihood of Markov-switching model fit",
        "Filtered and smoothed probability accuracy for regime inference",
        "Likelihood ratio test for regime-switching vs linear model",
        "Out-of-sample forecasting accuracy under different regimes",
        "Volatility and mean-shift detection sensitivity",
        "Regime duration and expected persistence",
        "Transition probability stability across time",
        "Model identification quality for turning points in economic cycles",
        "State-dependent prediction error metrics",
        "Improvement in forecast RMSE relative to linear AR models"
      ],
      "tools-used-in": [
        "Hidden-Markov-Model-(HMM)"
      ],
      "paper-details": {
        "algorithms": {
          "Markov_Switching_Model": {
            "name": "Markov-Switching Autoregressive (MS-AR) Model",
            "purpose": "Capture structural changes in time-series dynamics through latent regimes governed by a Markov process.",
            "components": {
              "state_variable": {
                "definition": "s_t ∈ {1, 2, …, K}, indicating regime at time t.",
                "role": "Determines which autoregressive parameters apply.",
                "importance": "Allows model to capture shifts in economic conditions."
              },
              "regime_specific_parameters": {
                "definition": "y_t = μ_{s_t} + φ_{s_t} y_{t−1} + ε_t",
                "role": "Allows intercepts and slopes to vary across regimes.",
                "advantage": "Captures mean, variance, and persistence changes."
              },
              "transition_matrix": {
                "definition": "P(i → j): probability of moving from regime i to j.",
                "role": "Defines persistence and switching dynamics of regimes."
              }
            },
            "procedure": [
              "Specify number of regimes (commonly 2 for recession/expansion).",
              "Initialize Markov transition probabilities.",
              "Estimate regime-dependent parameters μ_s, φ_s using maximum likelihood.",
              "Use forward algorithm to compute filtered probabilities P(s_t | y_1:t).",
              "Use forward–backward smoothing for historical regime inference.",
              "Forecast future values conditional on predicted regime probabilities.",
              "Evaluate model performance using likelihood-based tests."
            ],
            "output": "State-dependent autoregressive model capturing regime-driven shifts in economic time series.",
            "advantages": [
              "Effectively models nonstationary time series with abrupt structural changes.",
              "Identifies business cycle turning points.",
              "Improves forecasts relative to linear AR models.",
              "Provides interpretable regime probabilities."
            ],
            "limitations": [
              "Likelihood optimization is complex and sensitive to initialization.",
              "Regime labeling ambiguity (switching symmetry).",
              "Selecting number of regimes can be subjective.",
              "Computational intensity increases with additional regimes."
            ]
          },
    
          "Hamilton_Filter": {
            "name": "Hamilton Filter (Forward Algorithm for Regime Filtering)",
            "purpose": "Compute real-time probabilities of latent regimes in a Markov-switching time series model.",
            "procedure": [
              "Start with initial state distribution π_0.",
              "For each time step, compute predicted probabilities P(s_t | y_1:t−1).",
              "Update with observation likelihood P(y_t | s_t).",
              "Normalize to obtain filtered probabilities P(s_t | y_1:t).",
              "Repeat recursively across time steps."
            ],
            "output": "Filtered regime probabilities used for inference and forecasting."
          },
    
          "Smoothing_Algorithm": {
            "name": "Forward–Backward Smoothing Algorithm",
            "purpose": "Compute the full-sample posterior probability P(s_t | y_1:T).",
            "procedure": [
              "Run Hamilton filter forward to compute filtered probabilities.",
              "Run backward recursion using predicted and filtered probabilities.",
              "Combine to obtain smoothed regime probabilities.",
              "Use smoothed states for economic cycle dating and model validation."
            ],
            "output": "Smoothed historical regime path showing latent structural shifts."
          },
    
          "Likelihood_Ratio_Analysis": {
            "name": "Likelihood Ratio Test for Regime Switching",
            "purpose": "Determine whether adding regime-switching significantly improves model fit.",
            "procedure": [
              "Estimate linear AR model as baseline.",
              "Estimate MS-AR model with K regimes.",
              "Compute LR statistic: LR = 2(ℓ_MS − ℓ_linear).",
              "Assess significance using nonstandard distributions.",
              "Evaluate whether structural breaks are statistically justified."
            ],
            "output": "Evidence supporting or rejecting presence of regime-dependent dynamics."
          }
        }
      }
    },
    
    {
      "id": 97,
      "title": "Time Series Analysis: Forecasting and Control",
      "authors": "Box & Jenkins",
      "year": 1976,
      "source": "Wiley",
      "url": "https://www.wiley.com/en-us/Time+Series+Analysis%3A+Forecasting+and+Control%2C+5th+Edition-p-9781118675021",
      "performance-metrics": [
        "Mean Absolute Error (MAE)",
        "Root Mean Squared Error (RMSE)",
        "Mean Absolute Percentage Error (MAPE)",
        "Akaike Information Criterion (AIC)",
        "Bayesian Information Criterion (BIC)",
        "Residual autocorrelation diagnostics (Ljung–Box Q-statistic)",
        "Forecast error variance",
        "Out-of-sample forecast accuracy",
        "Parameter stability across rolling windows",
        "Goodness-of-fit via analysis of residual whiteness"
      ],
      "tools-used-in": [
        "ARIMA-(AutoRegressive-Integrated-Moving-Average)"
      ],
      "paper-details": {
        "algorithms": {
          "Box_Jenkins_ARIMA": {
            "name": "Box–Jenkins ARIMA Modeling Framework",
            "purpose": "Develop a systematic methodology for identifying, estimating, and forecasting time series using ARIMA, SARIMA, and intervention models.",
            "components": {
              "identification": {
                "definition": "Use ACF and PACF plots to determine AR(p), I(d), MA(q) orders.",
                "role": "Model structure selection before estimation.",
                "tools": [
                  "Autocorrelation Function (ACF)",
                  "Partial Autocorrelation Function (PACF)"
                ]
              },
              "estimation": {
                "definition": "Estimate ARIMA parameters using maximum likelihood or nonlinear least squares.",
                "role": "Obtain stable and statistically significant coefficients."
              },
              "diagnostic_checking": {
                "definition": "Evaluate model adequacy through residual analysis.",
                "criteria": [
                  "Residuals should be uncorrelated (white noise)",
                  "Residuals should be normally distributed",
                  "No remaining structure in ACF/PACF"
                ]
              },
              "forecasting": {
                "definition": "Generate future predictions using fitted ARIMA equations.",
                "role": "Produce optimal linear forecasts under model assumptions."
              }
            },
            "procedure": [
              "Plot data and check for stationarity.",
              "Difference the series (I(d)) until stationarity is achieved.",
              "Use ACF/PACF to choose AR(p) and MA(q) terms.",
              "Estimate parameters using MLE.",
              "Perform diagnostic checks using residual tests.",
              "Refine model if needed and select the best via AIC/BIC.",
              "Generate forecasts and evaluate forecast accuracy."
            ],
            "output": "A validated ARIMA model with optimized parameters and forecast output.",
            "advantages": [
              "Highly effective for univariate time series forecasting.",
              "Clear, structured methodology ensures repeatable results.",
              "Captures autocorrelation and moving-average dynamics.",
              "Extensible to SARIMA, ARIMAX, and intervention models."
            ],
            "limitations": [
              "Assumes linearity; may fail to capture nonlinear dynamics.",
              "Differencing can add noise if misapplied.",
              "Identification using ACF/PACF can be subjective.",
              "Parameter estimation becomes difficult for high orders."
            ]
          },
    
          "Seasonal_ARIMA": {
            "name": "Seasonal ARIMA (SARIMA) Extension",
            "purpose": "Model periodic seasonal patterns along with non-seasonal ARIMA dynamics.",
            "components": {
              "seasonal_terms": {
                "definition": "(P, D, Q)_s seasonal autoregressive, differencing, and moving-average terms.",
                "role": "Capture repeating seasonal cycles in data."
              }
            },
            "procedure": [
              "Identify seasonal patterns using ACF peaks at seasonal lags.",
              "Difference seasonally if required.",
              "Add seasonal AR, MA terms matching ACF/PACF.",
              "Estimate all parameters jointly.",
              "Assess residuals for seasonal structure."
            ],
            "output": "SARIMA model capturing both short-term and seasonal dependencies."
          },
    
          "Intervention_Analysis": {
            "name": "Intervention (Impact) Analysis",
            "purpose": "Analyze the effects of external shocks (policy changes, events) on time-series behavior.",
            "procedure": [
              "Identify time of intervention.",
              "Specify impulse or step functions.",
              "Estimate ARIMA model with intervention term.",
              "Assess significance of intervention parameters.",
              "Evaluate post-intervention forecasting performance."
            ],
            "output": "Quantification of structural effects due to external interventions."
          },
    
          "Forecast_Evaluation": {
            "name": "Forecast Accuracy Evaluation",
            "purpose": "Assess how well ARIMA forecasts match actual data.",
            "procedure": [
              "Split data into training and test sets.",
              "Fit ARIMA on training data.",
              "Generate multi-step-ahead forecasts.",
              "Compute forecast accuracy measures (MAE, RMSE, MAPE).",
              "Perform error diagnostics and refine model if necessary."
            ],
            "output": "Reliable forecast performance metrics and validated model."
          }
        }
      }
    },
    
    {
      "id": 98,
      "title": "Portfolio Selection",
      "authors": "Markowitz",
      "year": 1952,
      "source": "Journal of Finance",
      "url": "https://www.jstor.org/stable/2327556",
      "performance-metrics": [
        "Expected portfolio return",
        "Portfolio variance (risk)",
        "Efficient frontier curvature and smoothness",
        "Sharpe Ratio of optimal portfolios",
        "Diversification level (effective number of assets)",
        "Sensitivity of optimal weights to input changes",
        "Stability of efficient portfolios across samples",
        "Comparison of minimum-variance vs tangency portfolio performance",
        "Correlation structure impact on diversification",
        "Risk-return trade-off metrics across efficient frontier"
      ],
      "tools-used-in": [
        "Markowitz-Mean–Variance"
      ],
      "paper-details": {
        "algorithms": {
          "Mean_Variance_Optimization": {
            "name": "Markowitz Mean–Variance Optimization",
            "purpose": "Construct optimal portfolios by balancing expected return and risk, formalizing modern portfolio theory.",
            "components": {
              "expected_return_vector": {
                "definition": "μ = E[R]",
                "role": "Represents expected returns for all assets."
              },
              "covariance_matrix": {
                "definition": "Σ = Cov(R)",
                "role": "Captures co-movement and diversification effects."
              },
              "risk_measure": {
                "definition": "Portfolio variance σ_p² = wᵀ Σ w",
                "role": "Measures total portfolio risk."
              }
            },
            "procedure": [
              "Estimate expected returns μ and covariance matrix Σ.",
              "Formulate optimization problem to minimize variance for a target return or maximize return for a target risk.",
              "Solve quadratic programming problem for optimal weights.",
              "Repeat for various target returns to construct the efficient frontier.",
              "Identify minimum-variance and tangency (maximum Sharpe Ratio) portfolios."
            ],
            "output": "Set of efficient portfolios and weights balancing risk and return.",
            "advantages": [
              "First rigorous mathematical formulation of diversification.",
              "Provides clear trade-off between risk and return.",
              "Forms basis of all subsequent quantitative portfolio theories.",
              "Encourages use of covariance to reduce risk."
            ],
            "limitations": [
              "Highly sensitive to estimation error in μ and Σ.",
              "Often produces unstable, extreme weights.",
              "Assumes normally distributed returns and quadratic utility.",
              "Real-world constraints may violate model assumptions."
            ]
          },
    
          "Efficient_Frontier": {
            "name": "Efficient Frontier Construction",
            "purpose": "Identify the set of portfolios that minimize risk for each level of expected return.",
            "procedure": [
              "Compute optimal portfolios for a range of return targets.",
              "Plot risk (variance) vs return.",
              "Identify frontier as the upper envelope of feasible portfolios.",
              "Analyze risk-return trade-off across the curve."
            ],
            "output": "Curve of optimal portfolios forming the efficient frontier."
          },
    
          "Diversification_Principle": {
            "name": "Formalization of Diversification",
            "purpose": "Show mathematically how low or negative correlations reduce overall risk.",
            "procedure": [
              "Decompose portfolio risk into individual variances and covariances.",
              "Examine marginal contribution of each asset to portfolio variance.",
              "Show how adding assets with low correlation lowers total risk.",
              "Evaluate diversification benefits quantitatively."
            ],
            "output": "Mathematical basis for diversification effects in portfolio construction."
          }
        }
      }
    },
    
    {
      "id": 99,
      "title": "Value at Risk, GARCH Modelling and the Forecasting",
      "authors": "Füss",
      "year": null,
      "source": "SpringerLink",
      "url": "https://link.springer.com/chapter/10.1057/9781137554178_5",
      "performance-metrics": [
        "Volatility forecasting accuracy (RMSE, MAE)",
        "Value-at-Risk (VaR) backtesting violation rates",
        "Kupiec likelihood ratio test for VaR accuracy",
        "Christoffersen independence test for VaR clustering",
        "Expected shortfall (ES) forecasting error",
        "In-sample vs out-of-sample volatility persistence",
        "GARCH parameter stability (α + β)",
        "Tail risk sensitivity under heavy-tailed distributions",
        "Log-likelihood of fitted GARCH models",
        "Predictive performance of conditional variance equation"
      ],
      "tools-used-in": [
        "GARCH-1-1-Volatility-Forecasting"
      ],
      "paper-details": {
        "algorithms": {
          "GARCH_1_1_Model": {
            "name": "GARCH(1,1) Volatility Model",
            "purpose": "Model and forecast conditional volatility in financial returns for VaR estimation and risk management applications.",
            "components": {
              "variance_equation": {
                "definition": "σ_t² = ω + α ε_{t−1}² + β σ_{t−1}²",
                "role": "Captures volatility clustering and persistence seen in financial markets.",
                "interpretation": {
                  "ω": "Long-run average variance",
                  "α": "Reaction to recent shocks",
                  "β": "Persistence of past volatility"
                }
              },
              "conditional_mean": {
                "definition": "r_t = μ + ε_t, ε_t = σ_t z_t",
                "role": "Separates mean and variance dynamics for clean volatility estimation."
              },
              "distributional_assumptions": {
                "options": [
                  "Gaussian innovations",
                  "Student-t innovations",
                  "Generalized Error Distribution (GED)"
                ],
                "role": "Improve tail modeling for VaR forecasts."
              }
            },
            "procedure": [
              "Specify return model with conditional variance driven by GARCH(1,1).",
              "Estimate parameters using maximum likelihood.",
              "Validate model using residual diagnostics and autocorrelation checks.",
              "Forecast future volatility using recursive variance updates.",
              "Feed volatility forecasts into VaR estimation framework.",
              "Backtest VaR performance using violation tests."
            ],
            "output": "Conditional variance forecasts for use in VaR and ES risk measures.",
            "advantages": [
              "Simple yet highly effective volatility model.",
              "Captures clustering and persistence in financial markets.",
              "Works well with heavy-tailed distributions when using t or GED errors.",
              "Widely used due to interpretability and robustness."
            ],
            "limitations": [
              "Assumes symmetric response to shocks; cannot capture leverage effect.",
              "α + β close to 1 may imply near-nonstationarity.",
              "Misses asymmetric and long-memory effects found in some markets."
            ]
          },
    
          "Value_at_Risk_Forecasting": {
            "name": "VaR Forecasting Using GARCH(1,1)",
            "purpose": "Estimate future portfolio losses at a specified confidence level using GARCH-driven volatility forecasts.",
            "components": {
              "quantile_estimation": {
                "definition": "VaR_α = μ_t + σ_t · q_α",
                "role": "Computes risk threshold using predicted volatility and quantile q_α."
              },
              "backtesting_framework": {
                "tests": [
                  "Kupiec unconditional coverage test",
                  "Christoffersen independence and conditional coverage test"
                ],
                "role": "Validate VaR forecasts empirically."
              }
            },
            "procedure": [
              "Forecast conditional volatility using GARCH(1,1).",
              "Compute VaR using distribution-specific quantile estimates.",
              "Compare realized losses to predicted VaR threshold.",
              "Perform Kupiec and Christoffersen tests.",
              "Adjust model parameters or innovation distribution if violations are excessive."
            ],
            "output": "Backward-tested VaR series demonstrating forecast reliability."
          },
    
          "Model_Comparison_and_Tail_Risk": {
            "name": "Comparison of GARCH Models for Tail-Risk Forecasting",
            "purpose": "Benchmark GARCH(1,1) against alternative volatility models.",
            "procedure": [
              "Estimate models under different distributional assumptions.",
              "Generate out-of-sample forecasts for volatility and VaR.",
              "Compare performance by violation counts and log-likelihood.",
              "Examine tail sensitivity and extreme-loss prediction accuracy."
            ],
            "output": "Assessment of GARCH suitability for tail-risk forecasting."
          }
        }
      }
    },
    
    {
      "id": 100,
      "title": "Realized Peaks Over Threshold (Realized POT)",
      "authors": "Bee et al.",
      "year": null,
      "source": null,
      "url": "https://academic.oup.com/jfec/article/17/2/254/5369812",
      "performance-metrics": [
        "Tail index estimation accuracy",
        "Violation ratio (observed vs. expected exceedances)",
        "Backtesting performance of POT-based VaR",
        "Expected Shortfall (ES) prediction error",
        "Goodness-of-fit for Generalized Pareto Distribution (GPD)",
        "Threshold stability diagnostics",
        "Extreme quantile forecast accuracy",
        "Bias and variance of tail parameter estimates",
        "Out-of-sample tail risk forecasting performance",
        "Likelihood-based comparison of GPD parameterizations"
      ],
      "tools-used-in": [
        "Extreme-Value-Theory-(EVT)–Peaks-Over-Threshold"
      ],
      "paper-details": {
        "algorithms": {
          "Realized_POT_Model": {
            "name": "Realized Peaks Over Threshold (Realized POT) Framework",
            "purpose": "Enhance traditional Peaks-Over-Threshold (POT) extreme value modeling by incorporating realized volatility measures for adaptive thresholding and improved tail risk estimation.",
            "components": {
              "realized_volatility_input": {
                "definition": "Use high-frequency realized volatility (RV) or realized range to adapt threshold selection.",
                "role": "Aligns threshold with market microstructure volatility.",
                "benefit": "Improves stability of exceedance process."
              },
              "threshold_selection": {
                "definition": "Dynamic, volatility-adjusted threshold u_t.",
                "role": "Determines observations classified as extremes.",
                "importance": "Avoids over-/under-selection of tail events."
              },
              "generalized_pareto_distribution": {
                "definition": "GPD(x | ξ, β) for exceedances x > u_t",
                "role": "Models excess distribution beyond threshold.",
                "parameters": {
                  "ξ": "Tail index (shape parameter)",
                  "β": "Scale parameter"
                }
              }
            },
            "procedure": [
              "Compute realized volatility measures from high-frequency data.",
              "Determine volatility-adjusted threshold u_t to identify exceedances.",
              "Model excess returns using the Generalized Pareto Distribution.",
              "Estimate GPD parameters via maximum likelihood.",
              "Generate extreme quantile forecasts for VaR and ES.",
              "Validate results using violation tests and tail diagnostics."
            ],
            "output": "Improved tail-risk estimates through dynamic POT modeling with realized volatility adjustments.",
            "advantages": [
              "Addresses weaknesses of static thresholds in POT models.",
              "Enhances responsiveness to intraday volatility spikes.",
              "Improves VaR and ES forecasting accuracy.",
              "Reduces bias in extreme quantile estimation."
            ],
            "limitations": [
              "Requires high-frequency data, which may not be available for all assets.",
              "Dynamic thresholding adds complexity and calibration challenges.",
              "Realized volatility measures can be noisy under microstructure effects."
            ]
          },
    
          "Generalized_Pareto_Modeling": {
            "name": "Generalized Pareto Distribution (GPD) for Excesses",
            "purpose": "Model tail behavior of returns exceeding a threshold.",
            "components": {
              "exceedances": {
                "definition": "Y_i = X_i − u | X_i > u",
                "role": "Transform data into threshold exceedances for EVT modeling."
              },
              "gpd_parameters": {
                "shape": "ξ (tail thickness)",
                "scale": "β (dispersion of exceedances)"
              }
            },
            "procedure": [
              "Select threshold u using realized-volatility-driven method.",
              "Compute exceedances Y_i.",
              "Estimate ξ and β via maximum likelihood.",
              "Check threshold stability using parameter plots.",
              "Use GPD fit to compute extreme quantiles."
            ],
            "output": "Tail distribution estimates and extreme return quantiles."
          },
    
          "Tail_Risk_Forecasting": {
            "name": "Extreme Tail Risk Forecasting with Realized POT",
            "purpose": "Forecast risk measures such as VaR and ES using dynamically modeled tail distributions.",
            "procedure": [
              "Use fitted GPD to compute VaR_α and ES_α.",
              "Evaluate predictive accuracy with backtesting metrics.",
              "Compare static POT vs Realized POT for tail-risk responsiveness.",
              "Assess model robustness across volatility regimes."
            ],
            "output": "Adaptive, volatility-informed extreme risk forecasts."
          }
        }
      }
    },
    
    {
      "id": 101,
      "title": "Parametric Method in Value at Risk (VaR): Definition and Examples",
      "authors": null,
      "year": "2024",
      "source": "Investopedia",
      "url": "https://www.investopedia.com/ask/answers/041715/what-variancecovariance-matrix-or-parametric-method-value-risk-var.asp",
      "performance-metrics": [
        "VaR estimation error under normality assumptions",
        "Backtesting violation frequency vs expected exceedances",
        "Mean squared error of predicted risk vs realized losses",
        "Sensitivity of VaR estimates to changes in volatility",
        "Accuracy of portfolio variance–covariance estimation",
        "Impact of correlation mis-specification on VaR accuracy",
        "Stability of VaR estimates across rolling windows",
        "Tail loss prediction accuracy under Gaussian assumptions",
        "Coverage ratios for different confidence levels",
        "Comparative VaR performance vs historical and Monte Carlo methods"
      ],
      "tools-used-in": [
        "Variance-Covariance-Value-at-Risk"
      ],
      "paper-details": {
        "algorithms": {
          "Parametric_VaR_Method": {
            "name": "Parametric (Variance–Covariance) VaR Method",
            "purpose": "Compute Value-at-Risk using the assumption that returns follow a known parametric distribution, typically normal, enabling fast and analytical VaR estimation.",
            "components": {
              "expected_return": {
                "definition": "μ_p = wᵀ μ",
                "role": "Represents portfolio mean return under normality assumption."
              },
              "portfolio_volatility": {
                "definition": "σ_p = √(wᵀ Σ w)",
                "role": "Captures aggregate portfolio risk via covariance matrix Σ."
              },
              "confidence_quantile": {
                "definition": "z_α (e.g., 1.645 for 95%, 2.326 for 99%)",
                "role": "Determines magnitude of extreme loss threshold under normality."
              }
            },
            "procedure": [
              "Estimate expected asset returns μ and covariance matrix Σ.",
              "Compute portfolio mean μ_p and volatility σ_p from portfolio weights w.",
              "Select confidence level α (e.g., 95% or 99%).",
              "Obtain corresponding quantile z_α from standard normal distribution.",
              "Compute VaR analytically using: VaR_α = μ_p − z_α σ_p.",
              "Validate estimated VaR via backtesting violation counts."
            ],
            "output": "Analytical VaR estimate assuming normal distribution of returns.",
            "advantages": [
              "Computationally efficient and easy to implement.",
              "Provides closed-form VaR expression.",
              "Useful for large portfolios with Gaussian-like returns.",
              "Enables fast scenario and sensitivity analysis."
            ],
            "limitations": [
              "Assumes normally distributed returns, which underestimates tail risk.",
              "Relies heavily on covariance matrix accuracy.",
              "Not robust under skewness, kurtosis, or regime changes.",
              "Fails during high-volatility crises due to Gaussian assumption."
            ]
          },
    
          "Portfolio_Variance_Covariance_Calculation": {
            "name": "Portfolio Variance–Covariance Computation",
            "purpose": "Compute overall portfolio volatility using individual asset variances and correlations.",
            "components": {
              "covariance_matrix": {
                "definition": "Σ where Σ_ij = Cov(R_i, R_j)",
                "role": "Captures co-movement among assets, shaping portfolio risk."
              }
            },
            "procedure": [
              "Estimate daily or periodic returns for all assets.",
              "Compute sample covariance matrix Σ.",
              "Use weights w to compute σ_p = √(wᵀ Σ w).",
              "Feed σ_p into parametric VaR formula."
            ],
            "output": "Portfolio volatility estimate enabling VaR calculation."
          },
    
          "VaR_Backtesting": {
            "name": "Backtesting of Parametric VaR",
            "purpose": "Evaluate accuracy and reliability of VaR forecasts under normality assumptions.",
            "procedure": [
              "Compare realized losses with predicted VaR levels.",
              "Count exceedances (loss > VaR).",
              "Perform Kupiec unconditional coverage test.",
              "Perform Christoffersen independence test.",
              "Assess whether violation rate matches expected frequency."
            ],
            "output": "Statistical validation of parametric VaR performance."
          }
        }
      }
    },
    
    {
      "id": 102,
      "title": "Forecasting hedge funds volatility: a Markov regime-switching GARCH",
      "authors": null,
      "year": null,
      "source": "SSRN",
      "url": "https://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID1768864_code639897.pdf",
      "performance-metrics": [
        "In-sample log-likelihood of MS-GARCH model",
        "Out-of-sample volatility forecasting accuracy (RMSE, MAE)",
        "Regime classification accuracy (filtered vs smoothed)",
        "Hit ratio for volatility regime prediction",
        "Likelihood ratio test for regime-switching vs standard GARCH",
        "Volatility persistence estimates across regimes",
        "VaR backtesting violation rate using MS-GARCH volatility",
        "Parameter stability across regimes",
        "Forecast error variance decomposition",
        "Comparative forecasting performance: MS-GARCH vs GARCH(1,1)"
      ],
      "tools-used-in": [
        "GARCH-1-1-Volatility-Forecasting"
      ],
      "paper-details": {
        "algorithms": {
          "Markov_Regime_Switching_GARCH": {
            "name": "Markov Regime-Switching GARCH (MS-GARCH)",
            "purpose": "Capture volatility dynamics of hedge funds that alternate between distinct market regimes, improving forecasting accuracy over standard GARCH models.",
            "components": {
              "regime_variable": {
                "definition": "s_t ∈ {1, 2, …, K} controlling which GARCH parameters apply.",
                "role": "Distinguishes calm vs turbulent volatility regimes in hedge fund returns."
              },
              "regime_specific_garch": {
                "definition": "σ_t²(s_t) = ω_{s_t} + α_{s_t} ε_{t−1}² + β_{s_t} σ_{t−1}²",
                "role": "Allows each regime to have unique persistence and shock response.",
                "advantage": "Models nonlinear volatility clustering across different market environments."
              },
              "transition_matrix": {
                "definition": "P(i → j) giving probability of switching from regime i to j.",
                "role": "Captures duration and persistence of hedge fund volatility cycles."
              }
            },
            "procedure": [
              "Specify number of regimes (commonly 2: low-volatility and high-volatility).",
              "Estimate regime-dependent parameters using maximum likelihood and Hamilton filtering.",
              "Compute filtered regime probabilities for real-time inference.",
              "Compute smoothed probabilities for historical regime identification.",
              "Forecast volatility using regime-weighted expectations of conditional variances.",
              "Compare forecasts to standard GARCH(1,1).",
              "Use predicted volatility for VaR and ES risk forecasting."
            ],
            "output": "State-dependent volatility model providing more accurate volatility forecasts across market regimes.",
            "advantages": [
              "Captures nonlinear volatility patterns missed by standard GARCH.",
              "Improves forecasting during crisis periods (high-volatility regimes).",
              "Allows asymmetric persistence across regimes.",
              "Better fits hedge fund return characteristics, which are often regime-dependent."
            ],
            "limitations": [
              "Estimation is computationally intensive and sensitive to initialization.",
              "May suffer from identification issues when regimes are similar.",
              "Transition probabilities may vary over time, complicating interpretation."
            ]
          },
    
          "Hamilton_Filtering": {
            "name": "Hamilton Forward Filtering",
            "purpose": "Compute real-time (filtered) probabilities of each volatility regime.",
            "procedure": [
              "Start with initial state probabilities π_0.",
              "Predict regime probabilities using transition matrix.",
              "Update probabilities using return likelihood under each regime.",
              "Normalize to obtain filtered probabilities P(s_t | y_1:t).",
              "Repeat through entire time series."
            ],
            "output": "Filtered regime probabilities used for forecasting."
          },
    
          "Backward_Smoothing": {
            "name": "Forward–Backward Smoothing",
            "purpose": "Compute full-sample posterior probabilities of regime states for historical inference.",
            "procedure": [
              "Run forward Hamilton filter to get filtered probabilities.",
              "Run backward recursion using predicted state probabilities.",
              "Combine forward and backward results to compute smoothed probabilities.",
              "Use smoothed state series to validate model fit and identify regime switches."
            ],
            "output": "Smoothed probabilities revealing full regime history."
          },
    
          "Volatility_Forecasting": {
            "name": "Regime-Specific Volatility Forecasting",
            "purpose": "Generate more accurate volatility forecasts by accounting for distinct volatility behavior in each regime.",
            "procedure": [
              "Forecast future regime probabilities using transition matrix.",
              "Compute expected future variance using regime-weighted GARCH equations.",
              "Compare volatility forecasts with realized hedge fund volatility.",
              "Use forecasts to compute VaR and ES for hedge fund portfolios."
            ],
            "output": "Improved volatility forecasts across calm and turbulent market states."
          }
        }
      }
    },
    
    {
      "id": 103,
      "title": "Portfolio Optimization - Qiskit Finance",
      "authors": null,
      "year": null,
      "source": "Qiskit Community GitHub",
      "url": "https://qiskit-community.github.io/qiskit-finance/tutorials/01_portfolio_optimization.html",
      "performance-metrics": [
        "Portfolio Expected Return",
        "Portfolio Variance (Risk)",
        "Sharpe Ratio",
        "Energy Value of Portfolio Hamiltonian",
        "Probability of Optimal Portfolio State",
        "Sampling Variance of Objective",
        "Execution Time (Quantum vs Classical)",
        "Convergence Rate of QAOA Parameters",
        "Feasible State Probability (Constraint Satisfaction)",
        "Approximation Quality Compared to Classical Optimum"
      ],
      "tools-used-in": [
        "Quantum-Approximate-Optimization-Algorithm-(QAOA)-for-CVaR-based-Portfolio-Optimization"
      ],
      "paper-details": {
        "algorithms": {
          "Portfolio_Optimization_Hamiltonian": {
            "name": "Quadratic Unconstrained Binary Optimization (QUBO) Hamiltonian for Portfolios",
            "purpose": "Represent mean-variance portfolio optimization as a Hamiltonian suitable for quantum algorithms.",
            "components": {
              "risk_term": {
                "definition": "xᵀ Σ x",
                "role": "Penalizes allocations that increase covariance-based portfolio risk.",
                "encoding": "Implemented as quadratic Z_i Z_j interactions in the Hamiltonian."
              },
              "return_term": {
                "definition": "-μᵀ x",
                "role": "Rewards selecting assets with higher expected returns.",
                "encoding": "Linear Z_i terms representing contribution of each included asset."
              },
              "budget_constraint": {
                "definition": "(Σ x_i - k)²",
                "role": "Enforces fixed number of selected assets (cardinality constraint).",
                "encoding": "Penalty Hamiltonian added to ensure feasible solutions."
              }
            },
            "procedure": [
              "Load asset price data and compute expected returns μ and covariance matrix Σ.",
              "Translate classical portfolio optimization problem into QUBO form.",
              "Create cost Hamiltonian combining risk, return, and constraints.",
              "Use quantum algorithm (e.g., QAOA) to minimize the Hamiltonian energy.",
              "Interpret lowest-energy bitstring as optimal portfolio allocation."
            ],
            "output": "A binary decision vector x indicating selected assets.",
            "advantages": [
              "Provides a consistent quantum representation for portfolio problems.",
              "Compatible with QAOA, VQE, and other variational algorithms.",
              "Supports flexible constraints through penalty Hamiltonians."
            ]
          },
    
          "QAOA_For_Portfolio_Optimization": {
            "name": "QAOA for Portfolio Optimization",
            "purpose": "Use QAOA to minimize the portfolio Hamiltonian and produce an optimal or near-optimal portfolio.",
            "operator_layers": {
              "cost_layer": "U_C(γ) = e^{-iγH_cost}",
              "mixer_layer": "U_M(β) = e^{-iβH_mix}"
            },
            "procedure": [
              "Initialize superposition across all binary portfolios.",
              "Apply alternating cost and mixer layers for p QAOA steps.",
              "Sample bitstrings from resulting quantum state.",
              "Evaluate the Hamiltonian for each sampled bitstring.",
              "Select bitstring(s) with lowest energy as portfolio solution."
            ],
            "advantages": [
              "Obtains high-quality portfolio solutions using few quantum operations.",
              "Balances exploration and exploitation via mixer-cost alternation.",
              "Naturally handles binary decision problems like asset selection."
            ],
            "limitations": [
              "Performance sensitive to parameter initialization.",
              "Sampling noise affects energy estimation.",
              "Constraint satisfaction may require strong penalty tuning."
            ]
          },
    
          "Classical_Portfolio_Computation": {
            "name": "Classical Preprocessing for Quantum Portfolio Optimization",
            "purpose": "Prepare financial data inputs used to construct the quantum Hamiltonian.",
            "steps": [
              "Import historical asset price data.",
              "Compute mean return vector μ.",
              "Compute covariance matrix Σ.",
              "Normalize or scale data to fit Hamiltonian coefficient requirements."
            ],
            "properties": [
              "Ensures the quantum Hamiltonian reflects real financial behavior.",
              "Classical preprocessing is required for any quantum portfolio algorithm."
            ]
          },
    
          "Sampling_Based_Solution_Extraction": {
            "name": "Bitstring Sampling & Energy Evaluation",
            "purpose": "Extract candidate portfolios from quantum measurements.",
            "procedure": [
              "Execute QAOA circuit many times to obtain bitstrings.",
              "Map each bitstring to a portfolio weight vector.",
              "Compute portfolio objective (risk-return) for each sample.",
              "Identify bitstrings with minimum Hamiltonian energy."
            ],
            "advantages": [
              "Allows probabilistic exploration of high-quality portfolios.",
              "Supports post-processing filtering to enforce constraints."
            ]
          },
    
          "Classical_Optimizers": {
            "purpose": "Optimize variational parameters (γ, β) used in QAOA.",
            "types": {
              "COBYLA": "Gradient-free local optimizer used in Qiskit examples.",
              "SPSA": "Noise-tolerant optimizer effective for QAOA parameter landscapes."
            },
            "role_in_paper": "Refines QAOA parameters to minimize expected Hamiltonian energy."
          }
        }
      }
    },
    
    {
      "id": 104,
      "title": "Improving Variational Quantum Optimization using CVaR - Qiskit Tutorial",
      "authors": null,
      "year": null,
      "source": "Qiskit Optimization",
      "url": "https://qiskit-community.github.io/qiskit-optimization/tutorials/08_cvar_optimization.html",
      "performance-metrics": [
        "CVaR Objective Value",
        "Expected Energy (Cost Function Value)",
        "Probability of Optimal (Best) Solutions",
        "Convergence Rate of Variational Parameters",
        "Sampling Efficiency",
        "Distribution Concentration on Low-Energy States",
        "Success Rate (Global Minimum Frequency)",
        "Classical Optimizer Stability (SPSA/COBYLA)",
        "Energy Variance Across Samples"
      ],
      "tools-used-in": [
        "Quantum-Approximate-Optimization-Algorithm-(QAOA)-for-CVaR-based-Portfolio-Optimization"
      ],
      "paper-details": {
        "algorithms": {
          "CVaR_Optimization": {
            "name": "Conditional Value-at-Risk (CVaR) Optimization for Variational Algorithms",
            "purpose": "Improve variational quantum optimization by minimizing the expected value of the lowest α-fraction of the energy distribution, emphasizing high-quality solutions.",
            "components": {
              "cvar_objective": {
                "definition": "CVaRα = (1/α) * Σ lowest(α·N samples) of energy outcomes",
                "role": "Focuses optimization on best-performing solutions instead of averaging all samples.",
                "effect": "Steers the variational circuit toward low-energy regions even in noisy settings."
              },
              "energy_sampling": {
                "method": "Sample measurement outcomes from parameterized quantum circuit.",
                "role": "Generate an empirical distribution of solution energies.",
                "importance": "CVaR relies on sorted energy statistics instead of global averages."
              },
              "gradient_updates": {
                "method": "Use SPSA or COBYLA to update variational parameters.",
                "role": "Optimize parameters to minimize CVaR instead of mean energy.",
                "effect": "Produces faster convergence toward optimal solution distribution."
              }
            },
            "procedure": [
              "Construct a cost Hamiltonian for the combinatorial optimization problem.",
              "Initialize variational parameters θ.",
              "Run the PQC (e.g., QAOA circuit) to sample bitstrings.",
              "Compute energy for each sample using the cost Hamiltonian.",
              "Sort energies and compute CVaRα using lowest α-fraction.",
              "Update θ with classical optimizer to minimize CVaR.",
              "Iterate until CVaR convergence threshold is reached.",
              "Output bitstring(s) with highest probability among low-energy solutions."
            ],
            "output": "A distribution highly concentrated on near-optimal and optimal solutions.",
            "advantages": [
              "Improves robustness against noise and sampling errors.",
              "Faster convergence than mean-energy-based optimization.",
              "Higher likelihood of reaching global minima.",
              "Provides adjustable trade-off between exploration (α close to 1) and exploitation (α small)."
            ]
          },
    
          "QAOA_With_CVaR": {
            "name": "QAOA Enhanced with CVaR Objective",
            "purpose": "Integrate CVaR loss into QAOA to bias optimization toward more optimal energy states.",
            "operator_layers": {
              "cost_layer": "U_C(γ) = exp(-iγH_cost)",
              "mixer_layer": "U_M(β) = exp(-iβH_mix)"
            },
            "procedure": [
              "Prepare initial equal superposition of all states.",
              "Apply alternating mixer and cost layers for p QAOA steps.",
              "Sample bitstrings from final state distribution.",
              "Compute energies and CVaRα.",
              "Optimize (γ, β) parameters using classical optimizer.",
              "Repeat until CVaR minimizes."
            ],
            "advantages": [
              "Concentrates probability on optimal solutions more effectively than vanilla QAOA.",
              "Reduces noise sensitivity in near-term hardware.",
              "Provides a controlled way to refine solution quality."
            ],
            "limitations": [
              "Smaller α values require more samples to estimate CVaR accurately.",
              "Sorting overhead increases with number of samples."
            ]
          },
    
          "Sampling_Based_Energy_Estimation": {
            "name": "Energy Estimation via Measurement Sampling",
            "purpose": "Provide classical–quantum loop with accurate energy distribution for CVaR.",
            "steps": [
              "Execute quantum circuit multiple times to collect bitstrings.",
              "Evaluate cost Hamiltonian for each result.",
              "Produce histogram of energies.",
              "Use histogram to compute CVaR and expected value.",
              "Feed back into classical optimizer."
            ],
            "properties": [
              "Measurement noise impacts variance of energy estimates.",
              "CVaR reduces influence of high-energy (undesirable) samples."
            ]
          },
    
          "Classical_Optimizers": {
            "purpose": "Optimize QAOA or variational circuit parameters under CVaR objective.",
            "types": {
              "SPSA": "Efficient for stochastic CVaR estimation, uses two function evaluations per iteration.",
              "COBYLA": "Gradient-free, robust for small problem sizes."
            },
            "role_in_paper": "Used to minimize CVaRα derived from quantum samples."
          }
        }
      }
    },
    
    {
      "id": 105,
      "title": "Improved Quantum Approximate Optimization Algorithm based on Conditional Value-at-Risk for Portfolio Optimization",
      "authors": "Zhang et al.",
      "year": 2024,
      "source": "ResearchGate",
      "url": "https://www.researchgate.net/publication/388016183",
      "performance-metrics": [
        "Portfolio Expected Return",
        "Portfolio Variance (Risk)",
        "Conditional Value-at-Risk (CVaR)",
        "Sharpe Ratio",
        "Approximation Ratio (QAOA optimality gap)",
        "Convergence Speed (iterations to tolerance)",
        "Optimizer Stability",
        "Sampling Cost",
        "Energy Expectation Value",
        "Probability of Target Portfolio State",
        "Resource Scalability (qubit count vs. assets)"
      ],
      "tools-used-in": [
        "Quantum-Approximate-Optimization-Algorithm-(QAOA)-for-CVaR-based-Portfolio-Optimization"
      ],
      "paper-details": {
        "algorithms": {
          "CVaR_Improved_QAOA": {
            "name": "CVaR-Enhanced Quantum Approximate Optimization Algorithm",
            "purpose": "Improve portfolio allocation by integrating Conditional Value-at-Risk (CVaR) directly into the QAOA objective, steering the quantum search toward low-risk, high-return portfolios.",
            "components": {
              "cost_hamiltonian": {
                "definition": "H_cost = xᵀΣx - λ(μᵀx) + CVaR_penalty",
                "role": "Encodes the optimization objective combining risk, return, and tail-loss into the QAOA energy landscape.",
                "CVaR_penalty": "Adds heavier penalties for portfolio states producing losses above the VaR threshold."
              },
              "mixer_hamiltonian": {
                "definition": "H_mix = Σ X_i",
                "role": "Generates transitions between feasible portfolio bitstrings.",
                "effect": "Encourages global exploration and avoids premature convergence."
              },
              "CVaR_layer": {
                "method": "Quantum sampling followed by classical CVaR estimation",
                "definition": "CVaRα = E[L | L ≥ VaRα]",
                "effect": "Biases parameter updates to minimize worst-case losses, improving risk sensitivity."
              }
            },
            "procedure": [
              "Prepare initial uniform superposition over portfolio states.",
              "Apply parameterized cost and mixer Hamiltonians.",
              "Sample the quantum state to obtain portfolio outcomes.",
              "Compute empirical loss distribution and estimate CVaR.",
              "Update variational parameters to minimize CVaR-adjusted energy.",
              "Repeat until convergence and output the optimal portfolio bitstring."
            ],
            "output": "Portfolio allocation minimizing CVaR while preserving return targets.",
            "advantages": [
              "Superior performance under downside risk scenarios.",
              "Faster parameter convergence versus baseline QAOA.",
              "Stable under noisy sampling due to CVaR-driven reweighting.",
              "Better approximation of optimal financial portfolio solutions."
            ]
          },
    
          "Standard_QAOA": {
            "name": "Vanilla QAOA for Portfolio Optimization",
            "purpose": "Serve as a baseline by solving mean–variance optimization without tail-risk adjustments.",
            "kernel": {
              "cost_layer": "U_C(γ) = exp(-iγH_cost)",
              "mixer_layer": "U_M(β) = exp(-iβH_mix)"
            },
            "procedure": [
              "Encode portfolio risk-return objective in H_cost.",
              "Alternate cost and mixer unitaries for p layers.",
              "Optimize parameters (γ, β) to minimize expected cost.",
              "Measure final distribution to extract portfolio selection."
            ],
            "limitations": [
              "Does not incorporate CVaR or VaR.",
              "Sensitive to high-risk asset distributions.",
              "Performs poorly when losses have fat-tailed behavior."
            ]
          },
    
          "CVaR_Estimator": {
            "name": "Quantum-Assisted CVaR Estimation",
            "purpose": "Evaluate tail-risk from quantum-generated portfolio samples.",
            "procedure": [
              "Collect N samples of portfolio bitstrings from QAOA.",
              "Compute corresponding loss L(x) for each bitstring.",
              "Sort losses to identify the α-quantile (VaRα).",
              "Average losses above VaR to compute CVaR.",
              "Feed CVaR into classical optimizer for gradient-free updates."
            ],
            "properties": [
              "Captures downside-risk more effectively than variance.",
              "Allows QAOA to avoid risky local minima."
            ]
          },
    
          "Portfolio_Hamiltonian_Model": {
            "name": "Hamiltonian Encoding of Financial Portfolio",
            "purpose": "Translate classical portfolio optimization into a quantum Hamiltonian framework.",
            "structure": {
              "risk_term": "xᵀΣx encoded as quadratic Z_i Z_j interactions.",
              "return_term": "μᵀx encoded as linear Z_i fields.",
              "constraint_term": "Optional penalty to enforce budget or cardinality constraints."
            },
            "role_in_paper": "Forms the backbone of the cost Hamiltonian used inside improved QAOA."
          },
    
          "Classical_Optimizers": {
            "purpose": "Optimize variational parameters involved in QAOA.",
            "types": {
              "COBYLA": "Effective for noisy landscapes with limited sample precision.",
              "SPSA": "Efficient stochastic optimizer requiring only two function evaluations per iteration."
            },
            "role_in_paper": "Used to minimize CVaR-adjusted loss based on quantum measurements."
          }
        }
      }
    }
    
  ]
}